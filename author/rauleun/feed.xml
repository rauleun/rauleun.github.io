<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/author/rauleun/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2021-02-09T14:06:23+09:00</updated>
  <id>http://localhost:4000/author/rauleun/feed.xml</id>

  
  
  

  
    <title type="html">RE Tech Archive | </title>
  

  
    <subtitle>machine learning research notes</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">KPConv - Flexible and Deformable Convolution for Point Clouds 리뷰</title>
      <link href="http://localhost:4000/KPConv" rel="alternate" type="text/html" title="KPConv - Flexible and Deformable Convolution for Point Clouds 리뷰" />
      <published>2021-02-07T09:00:00+09:00</published>
      <updated>2021-02-07T09:00:00+09:00</updated>
      <id>http://localhost:4000/KPConv</id>
      <content type="html" xml:base="http://localhost:4000/KPConv">&lt;p&gt;원문 : &lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/html/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.html&quot;&gt;Thomas, Hugues, et al. “Kpconv: Flexible and deformable convolution for point clouds.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 2019년 ICCV에서 소개된 &lt;strong&gt;&lt;em&gt;Kpconv: Flexible and deformable convolution for point clouds&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;Kernel point convolution(KPConv)는 3D point cloud 형태의 데이터를 처리하기 위한 여러가지 방법들 중 graph나 3D voxel 등의 형태로 변환하지 않고 point cloud에 직접 convolution을 적용하는 류의 방법입니다. 하나의 convolution kernel은 여러 개의 kernel point들로 구성이 되어있고 각 kernel point마다 연속적인 값을 가지는 kernel weight을 배치하여 주변 점들에 대한 convolution 연산을 수행했습니다. 이 때 kernel point의 개수와 위치를 유동적으로 설정해줌으로써 network capacity를 조절할 수 있고, kernel의 형태를 point cloud의 기하학적 형태에 최적화하여 설정해줄 수 있습니다. KPConv는 3D point cloud classification이나 segmentation 등의 task에서 SOTA의 성능을 기록하였습니다. 그럼 지금부터 KPConv를 파헤쳐보겠습니다!&lt;/p&gt;

&lt;h2 id=&quot;kernel-point-convolution&quot;&gt;Kernel point convolution&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/1-kpconv.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한 점에 대한 kernel point convolution 연산은 일정한 반지름 &lt;em&gt;r&lt;/em&gt; 내부에 있는 주변 점들을 대상으로 합니다. 일반적으로 이웃한 점을 정의할 때 이용하는 kNN과 달리, 반지름을 정의하여 내부에 있는 모든 점들을 이웃한 점으로 정의하게 되면, 점 밀도의 변화에 대해 robust하며 해당 점들에 대한 일정 크기의 kernel을 정의하기가 훨씬 더 수월해집니다. 이렇게 구 형태의 kernel 유효 범위를 정의하게 되면, 범위 내에 특정 개수의 kernel point들을 배치하여 kernel point 별로 kernel weight을 할당합니다. Kernel weight은 각 kernel point에 대해 correlation function과 상수 (&lt;em&gt;W&lt;/em&gt;) 의 곱으로 정의하며, correlation function은 kernel point와 점의 거리가 가까울수록 커지는 linear function을 이용합니다. 예를 들면, 위의 그림에서 각 kernel point에 대해 kernel point와의 거리가 멀어질수록 filter value의 절대값이 작아지는 것을 볼 수 있습니다. Filter value의 부호나 크기는 학습에 의해 정해지는 kernel의 상수값 &lt;em&gt;W&lt;/em&gt;에 따라 달라지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/2-convolution.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 여러 개의 kernel point들로 구성된 각 kernel들을 point cloud의 모든 점에 대입하여 convolution 연산을 해줍니다. Kernel의 개수에 따라서 output feature의 차원이 결정됩니다. 또한 모든 kernel 내 filter value들은 학습에 의해 정해집니다. 이는 위의 그림처럼 image에 적용하는 2D convolution과 정확히 같은 형태입니다. 두 경우 모두 각 Kernel에 대해 convolution 연산(elementwise 곱의 합)을 적용하고, 이 결과를 kernel 별로 쌓아 새로운 feature vector를 형성합니다.&lt;/p&gt;

&lt;p&gt;논문에서는 사전에 정의되어 고정된 kernel point 위치를 사용하는 rigid KPConv와 주변 점들의 분포에 따라 유동적으로 변화하는 kernel point 위치를 사용하는 deformable KPConv의 두 가지 연산을 정의하는데, rigid KPConv의 경우 classification이나 part segmentation 등 간단한 task에서, deformable KPConv의 경우 semantic segmentation과 같은 어려운 task에서 좋은 성능을 보여주었다고 합니다. 그럼 각각에 대해서 설명해보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;rigid-kernel-point-convolution&quot;&gt;Rigid kernel point convolution&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/12-rigid-kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rigid KPConv는 정해진 몇 개의 kernel point에 대해 kernel weight을 배치하여 convolution 연산을 수행합니다. 가장 효율적으로 convolution 연산을 수행하기 위해 repulsive potential과 attractive potential의 합을 최소화하는 최적화 방정식을 정의하였습니다. 이 때 효율적인 convolution 연산이라는 것은, 각 kernel point들의 correlation range가 kernel의 유효 범위를 모두 포함하며 kernel point 간에 겹치는 범위는 최소화하는 것을 의미합니다. Kernel point의 개수를 &lt;em&gt;K&lt;/em&gt; 라고 할 때, 최적화 방정식의 해는 &lt;em&gt;K&lt;/em&gt; 값에 따른 가장 효율적인 kernel point들의 배치입니다. 아래 그림에서는 몇 개의 &lt;em&gt;K&lt;/em&gt; 값에 대한 kernel point의 배치를 보여주고 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;deformable-kernel-point-convolution&quot;&gt;Deformable kernel point convolution&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rigid kernel point convolution을 정의하면 효율적으로 point cloud가 존재하는 공간의 정보를 모을 수 있습니다. 여기에서 좀 더 나아가서 kernel point들의 위치를 학습을 통해 결정할 수 있다면, 주변 점들의 분포를 가장 이상적으로 표현하도록 kernel point들이 배치될 것이므로, 고정되어 있을 때보다 convolution 연산이 가지는 정보의 capacity가 훨씬 커질것입니다. 또한 3D point cloud 형식의 특성상 점들의 분포가 규칙적이지 않으므로, deformable convolution을 적용하게 되면 점들이 존재하지 않는 공간에 대한 무의미한 kernel point 배치를 최소화할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/3-deformable.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 위해 &lt;em&gt;deformable version&lt;/em&gt; 의 network에서는, rigid KPConv 연산을 통해 K개 점에 대한 shift vector를 추출해주는 layer를 추가하였습니다. (Shift vector는 xyz 좌표를 포함하므로 총 3K개의 좌표를 추출합니다.) 이렇게 얻어진 벡터를 local shift로 정의하고 이에 따라 kernel point의 위치를 옮겨줍니다. Network 학습 과정에서는 local shift를 생성하는 rigid KPConv layer와 이를 통해 output feature를 생성하는 deformable KPConv layer를 동시에 학습하였습니다.&lt;/p&gt;

&lt;p&gt;하지만 이렇게 학습을 진행하게 되면 input point와 접점이 없는 kernel point들은 back propagation 과정에서 gradient 값을 잃게 되고, 결국 network는 kernel point를 잃어버리게 됩니다. 이는 point 들의 분포가 일정하지 않은 3D point cloud의 특성 때문인데, 이를 해결하기 위해 가장 가까운 input neighboring point와의 거리를 제한하는 regularization loss와 kernel point 간의 범위 중복을 최소화하기 위한 repulsive loss를 추가하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/13-deformable-kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 loss들을 통해 network는 위의 그림처럼 input point cloud의 geometry와 맞는 형태의 local shift를 추출하였습니다. Regularization loss가 없을 때에는 kernel point들이 input point cloud와 멀리 위치하는 경우들이 존재하였습니다. 이러한 경우에는 convolution 연산이 주변 점들의 정보를 잘 통합하지 못하여 특징 벡터의 표현력이 떨어질 수 밖에 없게됩니다. 하지만 regularization loss를 통해 input point들이 존재하는 영역들에 kernel point를 배치한다면, convolution을 통해서 얻은 특징 벡터가 주변 구조를 더 잘 표현하게 됩니다.&lt;/p&gt;

&lt;p&gt;위에서 소개한 두 가지 convolution 연산은 block 형태로 통합되어 전체 network를 구성하는 데에 사용하였습니다. 두 convolution block은 모두 skip-copnnection, batch normalization, leaky ReLU 등을 이용하여 학습의 안정성과 성능을 도모하였습니다. 아래에 구조를 담은 그림을 참고하면, deformable block에서는 KPConv 연산을 통해 3K 크기의 local shift vector를 추출하는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/10-network-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;kernel-point-network&quot;&gt;Kernel Point Network&lt;/h2&gt;

&lt;p&gt;논문에서는 KPConv 연산을 활용하여 network를 구축하였습니다. Network는 U-Net과 유사하게 pooling layer과 upsampling layer를 이용하여 multi-scale의 feature vector를 추출하였습니다. 이 때 pooling을 하면서 점들을 sub-sampling 하는 과정이 필요한데, input point의 density에 independent한 grid subsampling 방법을 이용하였습니다. 이는 점들을 voxel 형태의 grid에 놓은 뒤, 각 grid의 무게중심에 해당하는 점들만을 sampling하는 방법입니다. Pooling 과정은 앞서 설정한 grid의 cell size를 2배씩 키워서 output point의 개수를 줄여나간 뒤에, KPConv를 통해 각 cell 내부의 점들에 대한 feature vector를 통합해주었습니다. 이를 strided KP-Conv라고 부르기도 하였습니다.&lt;/p&gt;

&lt;p&gt;Network parameter는 cross validation을 통해서 결정했습니다. Kernel point의 개수는 15, convolution radius와 kernel point radius는 각각 단위길이의 2.5, 1.5배를 이용하였습니다. Subsampling cell size는 pooling layer마다 2배씩 증가하게끔 설정해주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/11-network-architecture-fig.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Network는 classification 및 segmentation에 대한 각각의 task 별로 하나씩 구성하였습니다. 위의 그림에서 Classification을 위한 network는 KP-CNN, segmentation을 위한 network는 KP-FCNN이라고 부릅니다. KP-FCNN은 KP-CNN의 encoder 부분을 공유하지만, segmentation task의 특징 상 점들 별로 하나 씩의 class output을 도출해야 하기 때문에, nearest upsampling 과정을 통해 점들의 개수를 맞춰준 후에 point-wise feature를 추출했습니다. 반면에 KP-CNN은 encoder를 통해 얻은 feature vector로부터 fully-connected layer를 활용하여 곧바로 class 정보를 추출하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;3D Shape Classification and Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/4-modelnet-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모델은 가장 일반적인 3D classification dataset인 ModelNet40과 part segmentation dataset인 Shapenet을 이용하여 성능을 평가하였습니다. Grid subsampling을 이용했기에 점들의 개수는 data마다 달랐지만, KPConv 과정에서 variable batch size에 대한 normalization 처리를 해주었기 때문에 아무런 문제가 되지 않았습니다. 또한 point cloud의 크기를 바꾸거나, 좌우 반전 및 점들을 일부 제거하는 등의 augmentation 과정을 통해 dataset의 개수를 늘려주었습니다. KPConv를 이용한 두 모델은 각 dataset에 대해 SOTA의 성능을 달성하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3D Scene Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/5-segment-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3D scene segmentation task에서는 indoor, outdoor scene에 대한 Scannet, S3DIS, Semantic3D 등의 dataset을 이용하여 성능을 평가하였습니다. 보통 3D scene dataset의 크기가 굉장히 크기 때문에, 부분적으로 구 형태의 subcloud를 분리하고 그에 대해서만 segmentation 작업을 수행했습니다. Network에 input으로 들어가는 구는 2m 또는 4m의 반지름을 가지게끔 설정해주었습니다. Rigid convolution과 deformable convolution을 이용한 network는 각 task에서 모두 뛰어난 성공을 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/6-segment-result-fig.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;특히 deformable convolution을 활용한 network는 더 큰 capacity를 가지고 있기 때문에 dataset이 더 크고 다양한 data로 구성되어있을 때 더 좋은 성능을 보여주었습니다. 아래의 그림처럼 kernel point의 개수가 줄어들어도, deformable convolution이 충분한 표현력을 가지고 있기 때문에 성능이 크게 줄어들지 않는 것을 확인할 수 있었습니다. 또한 deformable convolution은 class의 종류가 훨씬 다양한 indoor segmentation에서 좋은 성능을 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/7-kernel-point-iou.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Kernel point convolution은 point cloud에 직접 convolution을 적용하는 방법으로 classification 및 segmentation task에 대한 SOTA의 성능을 보여주었습니다. 논문에서는 kernel point의 위치가 고정된 rigid KPConv와 객체의 형태에 따라 변화하는 deformable KPConv를 제안하여 dataset의 크기나 다양성에 맞게 convolution block을 선택할 수 있게 제안하였습니다. 또한 연산 방법의 성능을 검증하기 위해 다양한 실험을 진행하였고, github에 코드도 잘 정리되어 있으니 좋은 성능의 3D classification 및 segmentation model이 필요하다면 꼭 알아야 할 연구라고 생각됩니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf&quot;&gt;https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/HuguesTHOMAS/KPConv&quot;&gt;https://github.com/HuguesTHOMAS/KPConv&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3D" />
      

      
        <category term="3D" />
      

      
        <summary type="html">원문 : Thomas, Hugues, et al. “Kpconv: Flexible and deformable convolution for point clouds.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs 리뷰</title>
      <link href="http://localhost:4000/Superpoint-Graphs" rel="alternate" type="text/html" title="Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs 리뷰" />
      <published>2021-01-29T09:00:00+09:00</published>
      <updated>2021-01-29T09:00:00+09:00</updated>
      <id>http://localhost:4000/Superpoint-Graphs</id>
      <content type="html" xml:base="http://localhost:4000/Superpoint-Graphs">&lt;p&gt;원문 : &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/html/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.html&quot;&gt;Landrieu, Loic, and Martin Simonovsky. “Large-scale point cloud semantic segmentation with superpoint graphs.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 2018년 CVPR에서 소개된 &lt;strong&gt;&lt;em&gt;Large-scale point cloud semantic segmentation with superpoint graphs&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/1-spg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존에 진행된 많은 AI 기반의 point cloud 연구는 좋은 성능을 보여주었지만, input size의 한계 때문에 적은 수의 점들로 구성된 point cloud에 대해서만 적용할 수 있었습니다. Neural network는 몇 백만개 이상의 점들로 구성된 LiDAR scan을 직접 다루기 어려웠고, down-sampling 하는 등의 후처리를 거쳐서 network의 input으로 이용했습니다. 하지만 이는 point cloud의 장점인 물체에 대한 정교한 표현력을 떨어트릴 수밖에 없었습니다. 오늘 소개드릴 연구는 기존의 연구와는 달리 몇 백만개 단위의 점들로 구성된 point cloud를 대상으로 semantic segmentation을 수행하였습니다. 논문에서는 유사한 구조의 점들을 superpoint라는 하나의 점으로 모아서 새로운 그래프(superpoint graph)를 구성했습니다. Superpoint graph(SPG)는 많은 점들로 구성되어 물체 간의 의미론적 관계에 대한 풍부한 정보를 담고 있기 때문에 semantic segmentation 성능을 크게 끌어올릴 수 있었습니다.&lt;/p&gt;

&lt;p&gt;수학적인 내용이 많아 어려웠지만, 최대한 잘 정리해보겠습니다 :)&lt;/p&gt;

&lt;h2 id=&quot;superpoint-graph-spg&quot;&gt;Superpoint Graph (SPG)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/2-spg-process.png&quot; alt=&quot;&quot; /&gt;
SPG는 point cloud로부터 크게 세 단계를 거쳐서 생성됩니다. 우선, 전체 point cloud에서 기하학적으로 비슷한 구조를 가진 점들을 묶어서 작은 여러 개의 point cloud로 분리합니다. 이후, 분리된 각각의 point cloud를 하나의 점으로 변환하는데, 이를 논문에서는 superpoint라고 정의합니다. 이 때 분리된 point cloud 내에 속한 점들의 feature vector를 통합해서 각 superpoint의 embedding vector를 추출합니다. 마지막으로 superpoint 간의 연결 관계를 파악해서 edge를 연결하고, 이를 graph convolution network에 넣어서 segmentation task를 수행합니다. 그럼 각각의 부분에 대해서 자세히 설명하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Geometric Partitioning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Superpoint graph를 구성하기 위해서는 우선 많은 수의 점들로 구성된 point cloud를 작은 점들의 단위로 분리해주어야 합니다. 이 때 단순하고 비슷한 기하학적 모양을 가진 점들을 묶어줌으로써, 점들이 전반적으로 균일한 의미론적 특징을 가지고 실제로도 같은 class에 해당하게끔 설정해주었습니다. 또한 많은 수의 점들이 partitioning 과정을 거쳐야 하기 때문에 계산량 측면에서 효율적인 방법을 이용해주어야 합니다. 논문에서는 global energy model을 통해서 점들을 분리해주었습니다.&lt;/p&gt;

&lt;p&gt;Global enerygy model은 각 점마다 주변 점들의 모양에 대한 특징을 담은 geometric vector를 계산하고, geometric vector가 비슷한 점들 끼리 연결해주는 partitioning 방법입니다. 논문에서는 각 점과 주변 점들에 대한 선형성(linearity), 평면성(planarity), 분산성(scattering), 수직성(verticality)와 높이를 geometric vector로 이용했습니다. 이후 점들의 geometric vector들에 대한 최적화 문제를 푸는 방식으로 connected components를 찾아주었습니다. Point cloud의 크기가 커질 경우에 geometric vector가 nonconvex 하거나 noncontinuous 할 수 있기 때문에, 이러한 경우 graph cut 알고리즘의 하나인 persuit cut을 이용해서 graph를 여러 개로 나눠 크기를 줄여주었습니다. (Graph cut 알고리즘은 분리된 점들 간의 edge distance가 가장 멀게끔 graph를 두 개로 나누는 방법입니다.)&lt;/p&gt;

&lt;p&gt;이렇게 partitioning을 통해서 connected components &lt;em&gt;S = {S1, S2, …, Sk}&lt;/em&gt; 를 얻을 수 있는데, 각 component들은 기하학적으로 간단한 구조를 가지며 component 내의 점들은 같은 구조를 가지게 됩니다. 앞으로는 이를 superpoint라고 부르고, 이를 통해 graph를 형성해서 segmentation을 수행할 것입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Superpoint Graph Construction&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이제 각 superpoint를 node로 하는 하나의 graph를 형성할 것입니다. 일단 superpoint를 형성하기 전, 전체 input point cloud에 대해 인접한 점들을 연결하는 Voronoi adjacency graph를 형성합니다. Superpoint 간의 연결관계는 superpoint 내에 속한 점들이 Voronoi adjacency graph 상에서 연결되었는지에 따라 결정합니다. 예를 들면, &lt;em&gt;S&lt;/em&gt; 와 &lt;em&gt;T&lt;/em&gt; 라는 superpoint가 있고 내부에 각각 &lt;em&gt;s1&lt;/em&gt; 과 &lt;em&gt;t1&lt;/em&gt; 이라는 점이 소속되어 있다고 하면, &lt;em&gt;s1&lt;/em&gt; 과 &lt;em&gt;t1&lt;/em&gt; 이 연결되었다면 &lt;em&gt;S&lt;/em&gt; 와 &lt;em&gt;T&lt;/em&gt; 도 인접한 것으로 간주합니다. 반대로 만약 &lt;em&gt;S&lt;/em&gt; 와 &lt;em&gt;T&lt;/em&gt; 내의 모든 점들 간에 연결 관계가 없다면, &lt;em&gt;S&lt;/em&gt; 와 &lt;em&gt;T&lt;/em&gt; 는 인접하지 않은 것으로 간주합니다. Superpoint 간의 연결된 edge는 Superedge 라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/3-feature.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 superedge feature를 정의해야 합니다. Superedge feature은 superpoint 내 점들 간의 모양이나 크기에 따라 정의합니다. 논문에서는 위의 7개 특징에 대한 13차원의 superedge feature를 통해 superpoint의 특징 및 주변 superpoint 간의 연결 관계를 표현했습니다. 이 때 &lt;em&gt;length, surface, volume&lt;/em&gt; 등의 feature은 x,y,z 좌표값의 covariance matrix에 대한 eigenvalue를 통해 principle component에 대한 크기를 계산하고, 이 eigenvalue의 곱으로(각각 &lt;em&gt;e1&lt;/em&gt; , &lt;em&gt;e1 x e2&lt;/em&gt; , &lt;em&gt;e1 x e2 x e3&lt;/em&gt;) 표현했습니다. 또한 centroid offset 이나 length ratio 등은 graph의 방향에 따라 값이 달라지기 때문에, superpoint graph는 directed graph의 형태를 띄게 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Superpoint Embedding&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Superpoint 내의 점들은 같은 기하학적 특성을 공유하므로, 각 superpoint 별로 embedding vector를 추출할 수 있습니다. 논문에서는 PointNet을 통해서 superpoint의 contextual information을 추출합니다. 이 때 같은 superpoint 내의 점들은 기하학적으로 간단하고 서로 유사하기 때문에, GPU efficiency를 위해 몇 개의 점들만 골라서 embedding을 해도 reliably represent 할 수 있습니다. 따라서 저자는 128개의 점들을 sampling해서 이에 대해서만 embedding vector를 추출했고, 만약 한 superpoint에 128개 이하의 점이 있다면 그대로 사용했습니다. (PointNet은 max-pooling을 통해서 embedding vector를 추출했기 때문에 점의 개수가 바뀌어도 같은 크기의 embedding vector를 얻을 수 있었습니다.) 다만 점의 개수가 40개 이하인 경우에는 embedding을 0으로 두었는데, superpoint의 대표성이 떨어져서 성능 저하를 유발했기 때문입니다. 이렇게 superpoint 내에서 sub-sampling을 하게 되면 메모리 측면에서 효율적일 뿐만아니라 augmentation을 해주는 효과도 있었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Contextual Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Superpoint, superedge를 통해 superpoint graph를 정의했다면, 이제 이를 graph convolution을 통해 segmentation 해야 합니다. 이는 Gated Graph Neural Networks와 Edge-Conditioned Convolution을 이용해서 진행되었습니다. 간단히 설명드리자면 Gated Recurrent Unit(GRU)를 통해 각 superpoint에 대한 embedding을 update했는데, 이 때 이용되는 incoming message vector를 주변 superpoint와의 graph convolution을 통해서 얻었습니다. Graph convolution은 앞서 말한 edge-conditioned convolution 기반의 방법으로 진행되었는데, multi-layer perceptron을 통해 구현된 filter generating network가 edge feature vector로부터 attention weight을 계산하면, 이 weight를 기반으로 주변 점들의 feature vector를 dynamic하게 더해주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Geometric partitioning 단계는 unsupervised하게 학습되었고, superpoint embedding과 contextual segmentation 과정은 supervised하게 동시에 학습되었습니다. Superpoint 내의 점들은 같은 label을 가진 점들로 가정하였기 때문에, 점들의 label들 중 대다수에 해당하는 것으로 지정했습니다. Superpoint graph의 크기가 커서 GPU limit을 뛰어넘는 경우에는, SPG로부터 몇개의 superpoint(512개)만을 sub-sampling해서 크기가 작은 SPG를 형성하고 기존 SPG와 같은 연결 관계(superedge)를 형성한 후에 작은 SPG에 대해서 학습을 진행했습니다. 이렇게 하면 메모리 이슈도 해결할 수 있고, 동시에 data augmentation의 효과도 가져갈 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/5-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 논문들에서와 마찬가지로 가장 일반적인 3D semantic point cloud segmentation 평가 지표인 Semantic3D와 S3DIS dataset을 이용한 성능 평가가 진행되었습니다. 성능은 IOU와 overall accuracy로 측정하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic3D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/4-semantic3d-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Semantic3D는 30억개 이상의 점들로 구성된 가장 큰 LiDAR dataset입니다. SPG는 기존의 SOTA 모델보다 12mIOU points나 높은 SOTA의 성능을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;S3DIS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/6-s3dis-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S3DIS는 실내 환경에 대한 3D RGB point cloud dataset입니다. Area 5를 제외한 나머지 영역에 대해 학습을 진행했고, area 5를 이용해서 평가하였습니다. 전반적인 성능에 대해 SOTA를 기록했지만, white board와 같은 경우 partitioning 과정에서 wall과 제대로 구별이 되지 않으며 평균보다 낮은 IOU를 기록하기도 했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/7-voxelization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 많은 점들에 대한 inference 시간을 줄이기 위해, voxelization 형태의 전처리를 진행하고 결과를 보여주었습니다. 위의 표에서 알 수 있듯이 적절한 크기의 voxel 단위로 점들의 개수를 줄여준 경우에, inference 속도도 빨라지고 정확도도 증가하는 것을 볼 수 있었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ablations&quot;&gt;Ablations&lt;/h2&gt;
&lt;p&gt;SPG 모델에 활용된 여러가지 모듈들의 성능을 검증하기 위해 ablation study가 진행되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/8-ablation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우선 superpoint graph에 대해 graph convolution을 통해서 contextual information을 추출하는 것이 성능 향상에 얼마나 많은 영향을 주었는지를 확인했습니다. 기존 모델을 Perfect model이라고 할 때, superpoint graph를 GRU가 아닌 일반적인 PointNet을 활용해 처리하는 모델을 Unary model로 하여 두 성능을 비교했습니다. 결과는 22mIOU points에 가까운 성능 하락이 발생하여 graph convolution 기반의 모델을 통해 contextual information을 파악하는 것이 얼마나 중요한지를 보여주었습니다.&lt;/p&gt;

&lt;p&gt;그 외에 GRU + ECC를 이용하는 대신 CRF-RNN을 이용해보는 등 CRF를 후처리로 하는 모델들과의 비교, GRU 기반의 구조를 수정한 모델에 대한 성능 비교 등을 진행했습니다. 결과는 SPG Perfect 모델의 압승이었습니다. 중요하진 않은 것 같아서 넘어가겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Superpoint Graph는 많은 점들을 partitioning 과정을 통해 효율적으로 graph convolution 연산에 적용하는 방법을 제시했습니다. 기존과 달리 많은 수의 점들로 구성된 dataset으로 학습을 진행할 수 있었고, 디테일하고 많은 정보량 덕분인지 월등히 좋은 성능을 보여주었습니다. 또한 graph congolution 연산에 적용해 contextual information도 알맞게 추출하여 성능 향상에 기여하였습니다. 많은 점들에 대한 학습을 가능하게 한 실험적인 좋은 논문이라고 생각됩니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.09869&quot;&gt;https://arxiv.org/abs/1711.09869&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v51/landrieu16.html&quot;&gt;http://proceedings.mlr.press/v51/landrieu16.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3D" />
      

      
        <category term="3D" />
      

      
        <summary type="html">원문 : Landrieu, Loic, and Martin Simonovsky. “Large-scale point cloud semantic segmentation with superpoint graphs.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Graph Attention Convolution for Point Cloud Semantic Segmentation 리뷰</title>
      <link href="http://localhost:4000/GACnet" rel="alternate" type="text/html" title="Graph Attention Convolution for Point Cloud Semantic Segmentation 리뷰" />
      <published>2021-01-27T09:00:00+09:00</published>
      <updated>2021-01-27T09:00:00+09:00</updated>
      <id>http://localhost:4000/GACnet</id>
      <content type="html" xml:base="http://localhost:4000/GACnet">&lt;p&gt;원문 : &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf&quot;&gt;Wang, Lei, et al. “Graph attention convolution for point cloud semantic segmentation.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 Stanford에서 2019년 CVPR에서 발표한 &lt;strong&gt;&lt;em&gt;Graph attention convolution for point cloud semantic segmentation&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 graph attention convolution 연산을 제안하였는데, 이는 graph convolution kernel에 이웃한 점들과의 연관성에 따라 attention을 가하여 연관성이 높은 점들에 focus된 feature vector를 추출해줄 수 있도록 설계했습니다. 이를 통해 feature contamination을 방지하고 구조적 feature을 잘 추출하여 높은 segmentation 성능을 달성했습니다.
&lt;img src=&quot;assets/images/210127-GACnet/1-example.png&quot; alt=&quot;&quot; /&gt;
3D Point cloud의 semantic segmentation 분야는 PointNet 논문을 기점으로 활발하게 연구되고 있습니다. 최근에는 point cloud를 graph 형태로 표현하여 CNN 기반의 네트워크를 통해 segmentation 하려는 시도들이 많이 있습니다. 하지만 일반적인 graph convolution 연산에 이용하는 convolution kernel은 일정한 값으로 고정되어 있고, 이는 보통 주변 점들의 feature vector를 homogeneous하게 더하여 새로운 feature로 변환해줍니다. 예를 들어, 위의 그림에서 1번 점에 대한 graph convolution output은 1~5번 점들의 feature를 정해진 비율로 더해서 얻게 되고, 이는 1번 점의 클래스와 주변 점들의 클래스(table 또는 chair)를 고려하지 않고 얻은 것이기 때문에 좋은 output feature vector가 아닙니다. 이는 이웃한 점들과의 구조적인 연결 관계를 파악하게 어렵게 만들기 때문에 결과적으로 명확하지 않은 경계선을 만들거나 부분적으로 잘못된 segmentation 영역을 생성하는 등의 문제를 야기합니다.&lt;/p&gt;

&lt;p&gt;이러한 standard convolution kernel의 단점을 해결하기 위해 제안된 것이 graph attention convolution입니다. Graph attention convolution은 convolution 연산이 가해지는 점과 그 주변 점들의 공간적/특징적 정보들을 이용하여 attention weight을 계산하는데, 이는 연관성이 떨어지는 점들이 output feature vector에 관여하는 것을 막아줍니다. 이를 통해 point cloud에 적용하는 convolution kernel의 실질적인 receptive field는 해당하는 점과 주변 점들의 특징에 의해 동적으로 변화하게 됩니다.&lt;/p&gt;

&lt;p&gt;논문에서는 앞서 제안한 graph attention convolution과 graph coarsening 및 graph interpolation 기법을 통해서 graph pyramid network 구조를 구현하였습니다. 그렇다면 graph attention convolution(GAC)부터 GACnet까지 차근차근 살펴보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;graph-attention-convolution&quot;&gt;Graph Attention Convolution&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/2-gac.png&quot; alt=&quot;&quot; /&gt;
Point cloud는 각 점들을 vertex로 하고 인접한 점들을 연결한 선은 edge로 하는 graph 형태의 구조로 변환할 수 있습니다. 이 때 각 점은 &lt;em&gt;f&lt;/em&gt; 차원의 feature vector를 가질 수 있습니다. Graph attention convolution 연산은 우선 기존 &lt;em&gt;f&lt;/em&gt; 차원의 input feature vector를 &lt;em&gt;k&lt;/em&gt; 차원의 output feature vector로 변환하는 것으로 시작합니다. 이 변환은 미분 가능한 형태의 어떠한 함수로도 진행할 수 있는데, 논문에서는 multi-layer perceptron을 이용했습니다.&lt;/p&gt;

&lt;p&gt;이제 각 output feature vector에 attention weight을 elementwise하게 곱해서 convolution 연산을 수행할 것입니다. Attention weight은 output feature vector과 같은 &lt;em&gt;k&lt;/em&gt; 차원의 벡터여야 합니다. GAC에서는 점들 간의 좌표 차이와 feature vector 차이를 이용해서 attention weight을 추출하였습니다. 따라서 attention weight을 계산하는 함수는 &lt;em&gt;xyz&lt;/em&gt; 좌표의 3차원에 input feature vector의 &lt;em&gt;k&lt;/em&gt; 차원을 더한 &lt;em&gt;(3+k)&lt;/em&gt; 차원의 input을 받아서 output feature vector의 &lt;em&gt;k&lt;/em&gt; 차원의 output을 도출합니다. Sharing attention mechanism이라고 부르는 위 변환도 마찬가지로 미분가능한 어떠한 함수로도 진행할 수 있으며 논문에서는 multi-layer perceptron을 이용했습니다.&lt;/p&gt;

&lt;p&gt;Attention weight은 output feature 값의 안정성을 위해 exponential normalization 과정을 거치게 됩니다. 이제 위에서 얻은 &lt;em&gt;k&lt;/em&gt; 차원의 output feature vector에 attention weight을 element-wise 곱한 후 학습가능한 bias 값을 더하여서 GAC의 최종 output vector를 얻습니다.&lt;/p&gt;

&lt;p&gt;GAC 연산은 일반적인 graph attention mechanism과 다르게 attention weight을 얻을 때 점들의 위치 정보와 특징 정보의 차이를 동시에 이용한다는 특징이 있습니다. 이를 통해 점들 사이의 위치적 관계를 고려한 attention 값을 얻을 수 있고, 비슷한 특징을 가진 점들에 더 큰 attention을 가할 수도 있습니다. 또한 feature의 channel별로 attention을 가했기 때문에, 이상적으로 channel-wise independent한 feature vector의 특징을 더 잘 살릴 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;gacnet&quot;&gt;GACnet&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/3-gacnet.png&quot; alt=&quot;&quot; /&gt;
Feature pyramid network(FPN) 기반의 구조는 image segmentation 또는 object detection 분야에서 흔하게 이용되는 유용한 구조입니다. 논문에서는 이 구조를 차용한 graph pyramid network 형태의 구조를 위의 그림과 같이 구현하였습니다. FPN에서 feature extraction를 위해 활용되었던 convolution layer는 graph attention convolution layer가 수행합니다. 앞서 설명드린 것처럼 GAC layer는 점들의 위치 및 특징 정보를 활용하여 graph의 local feature vector를 잘 추출해낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게 얻은 local feature vector는 graph coarsening 및 pooling 과정을 통해 하나의 feature vector로 통합되었습니다. Graph coarsening은 PointNet++에서 소개된 방법을 그대로 이용했는데, furthest point sampling algorithm을 적용하여 여러 개의 중심점들을 선정하고, 각 중심점 주위의 점들을 grouping해서 중심점 단위의 group으로 모아주었습니다. 이후 각 중심점 group마다 max 또는 mean pooling을 적용하여 하나의 feature vector를 도출하였습니다.&lt;/p&gt;

&lt;p&gt;Graph pooling을 거치면 graph의 resolution이 작아지게 되는데, segmentation 등의 task에서는 원래 점들과 같은 개수의 feature map을 output으로 얻어야 하기 때문에 작아진 graph를 interpolation을 통해 up-sampling해주는 과정이 필수적입니다. 이를 feature interpolation layer이라고 부르는데, 마찬가지로 PointNet++에서 제안된 spatial distance 기반의 interpolation 방법을 적용하였습니다. (PointNet++ 리뷰를 참고하시면 도움이 됩니다!) GAC 및 graph pooling layer를 통해 학습된 feature vector는 feature interpolation layer를 통해 서서히 finest scale로 복원됩니다. 논문에서는 더 풍부한 semantic feature를 추출하기 위해 interpolation의 각 단계에서 skip-connection을 통해 down-sampling 되기 전의 feature vector와 합쳐주었습니다. 또한 feature refinement를 위해 최종적으로 학습된 feature vector를 GAC layer에 통과해주었습니다.&lt;/p&gt;

&lt;p&gt;Initial feature vector는 점의 높이, RGB, geo-feature로 구성했습니다. Geo-feature란 finest scale graph에서 이웃한 점들에 대한 covariance matrix의 eigenvalue 값으로 정의하였습니다. 각 구성 특징들의 역할은 후에 ablation study에서 다시 설명드리겠습니다. 초기 그래프는 각 점에 대해 특정 반지름 내에 존재하는 점들 중 random하게 &lt;em&gt;K&lt;/em&gt; 개의 점을 sampling하여 연결하여 생성하였습니다. 이 때 kNN이 아니라 random sampling을 통해서 edge를 구성한 이유는 point cloud의 density가 달라지더라도 이와 무관하게 일정한 범위의 점들에 대해 sampling 할 수 있기 때문입니다. 이렇게 되면 이웃한 두 점이 항상 쌍방으로 연결되지는 않게 되고, graph는 방향성이 있는 edge를 가진 directed graph의 형태가 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;gac-and-conditional-random-fieldcrf&quot;&gt;GAC and Conditional Random Field(CRF)&lt;/h2&gt;
&lt;p&gt;Image segmentation 분야의 유명한 네트워크 모델인 DeepLab에서는 명확한 경계선 구분을 위해 conditional random field(CRF) algoritm을 이용한 후처리 방법을 제안하였습니다. 이는 IOU 및 결과물에 대한 artifact reduction에 많은 공헌을 했고 이후에 이를 기준삼아 많은 segmentation 연구에서 CNN의 output에 CRF를 적용했습니다. CRF는 좌표 및 RGB 값이 비슷한 점들을 같은 label로 일치시켜 segmentation 경계선을 깔끔하고 detail하게 만들어주었습니다.&lt;/p&gt;

&lt;p&gt;앞서 설명드린 GAC도 CRF와 마찬가지로 위치 및 특징 정보를 활용하여 feature vector를 추출해주는 역할을 합니다. 특히 input feature vector 간의 차이를 이용하여 attention weight을 계산하기 때문에 비슷한 input feature를 가지는 점들에 대해 일관적인 output feature를 도출합니다. 이는 CRF와 정확히 같은 특징을 공유하기 때문에 GACnet에서는 더 이상 CRF 과정을 거치지 않아도 됩니다. 실제로 CRF model을 RNN 형식으로 구현하여 후처리하는 방법은 CNN과 독립적으로 진행되기 때문에 end-to-end로 결과를 얻을 수 없게 되는 등의 번거로움이 발생합니다. 또한 input feature의 유사도를 기반으로 layer를 거칠 때마다 더 semantic한 feature vector를 추출할 수 있기 때문에 segmentation 성능도 훨씬 더 잘나오게 되는 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;논문에서는 GAC 및 GACnet을 검증하기 위해 S3DIS와 Semantic3D라는 두 가지 3D semantic segmentation benchmark를 이용했습니다. GACnet의 성능은 IOU 및 overall accuracy를 이용하여 평가했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;S3DIS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/7-gacnet-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S3DIS는 6개의 실내 공간에 대한 3D RGB point cloud dataset입니다. 각 점은 13개 카테고리로 labeling 되어 있습니다. 논문에서는 6개의 공간 중 5번 공간을 testing set으로, 나머지 공간을 training set으로 하여 네트워크를 학습 및 평가하였습니다. 이렇게 이전에 전혀 보지 못한 공간에 대해 test하게 되면 task가 어려워지기 때문에 초기에는 성능이 잘 안나올 수 있지만, 네트워크 모델이 잘 일반화 되었는지를 평가하기에 좋습니다. 각 공간을 방 별로 먼저 분리하고, 각 방을 &lt;em&gt;1.2m x 1.2m&lt;/em&gt; 의 블럭으로 분리하여, 각 블럭 별로 random sampling된 4096 개의 점을 하나의 dataset으로 이용하였습니다. 방은 분리하는 과정에서 가장자리 &lt;em&gt;0.1m&lt;/em&gt; 씩은 물체의 일부밖에 보이지 않을 수 있기 때문에 buffer area로 설정하여 학습 및 loss 계산에 포함하지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/4-s3dis-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과는 위와 같습니다. GACnet은 대부분의 클래스에서 SOTA의 성능을 보여주었습니다. 특히나 GACnet은 S3DIS dataset에서 검출하기 어려운 벽 위의 창문이나 보드와 같은 클래스들도 잘 검출하였는데, 이는 GAC가 공간 정보 뿐만 아니라 RGB 값을 포함한 특징 정보도 활용하기 때문에 공간적 구조가 명확하지 않더라도 구별을 잘 해낸 것으로 생각합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic3D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Semantic3D는 LiDAR에서 얻은 약 40억개 이상의 점들로 구성된 3D point cloud dataset입니다. 점들에 대한 feature는 RGB와 intensity 값으로 구성되어 있고, 각 점은 8개의 카테고리 중 하나로 labeling 되어있습니다. 또한 Semantic3D는 외부 풍경에 대한 dataset이기 때문에 일반적으로 물체의 크기가 큽니다. 따라서 논문에서는 공간을 &lt;em&gt;4m x 4m&lt;/em&gt;의 블럭으로 구분하였고, 블럭 별로 4096 개의 점을 sampling하여 하나의 dataset으로 이용하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/5-semantic-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GACnet은 Semantic3D dataset에서도 SOTA의 성능을 보여주었습니다. 특히 Semantic3D에서는 차나 건물 등의 물체들이 가려져 있는 경우가 많은데, GACnet은 구조적인 feature learning 덕분인지 가려져 있는 물체에 대해서도 좋은 성능을 보여주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h2&gt;
&lt;p&gt;GAC 및 GACnet을 자세히 분석/검증하기 위해 논문에서는 몇 가지 항목에 대한 ablation study를 진행했습니다.
&lt;img src=&quot;assets/images/210127-GACnet/6-ablation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GAC 연산의 효율성&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우선 GAC 연산의 효율성을 검증하기 위해 저자는 GACnet의 GAC 부분을 PointNet에서 활용한 max pooling layer로 치환해서 성능을 측정하고 비교했습니다. Max-pooling 함수는 특징을 통합하여 분류하는 object classification 성능은 좋았지만, 정교한 경계선 설정이 중요한 segmentation task에서는 local한 정보들을 많이 소실하여 비교적 성능이 좋지 못했습니다. 반면에 GAC는 두 task 모두 뛰어난 성능을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위치 및 특징 정보들의 성능 향상 기여&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GAC에서 attention weight은 위치 좌표 값의 차이와 feature vector 간의 차이를 input으로 계산하였습니다. 저자는 각각을 하나씩 제거하여 얻은 attention weight을 이용하여 성능을 측정해보았는데, 두 경우 모두 성능 하락이 발생했습니다. 특히 점의 높이, RGB, geo-feature 등으로 구성된 feature vector를 제거하였을 때, 명확한 경계선 설정에 어려움을 겪었다고 이야기합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CRF-RNN 과의 비교&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GAC 모듈을 CRF-RNN으로 치환했을 때의 성능도 비교하였습니다. 두 경우 모두 유사한 성능을 보여주었는데, CRF와 GAC가 특징 벡터의 유사도를 이용하여 label을 도출하는 근본적으로 같은 성질을 공유하기 때문으로 보입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;초기 feature vector 구성에 따른 효과&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GAC의 input으로 활용하는 초기 feature vector는 점의 높이, RGB, geo-feature로 구성하였습니다. 각각 성능에 어떤 영향을 주는지를 분석하기 위해, 특정 component를 제외하고 성능을 측정하였습니다. 결과는 전반적인 성능 하락이 발생하여 각 component가 성능에 주요한 역할을 하는 것을 할 수 있었습니다. 
&lt;img src=&quot;assets/images/210127-GACnet/8-geo-feature.png&quot; alt=&quot;&quot; /&gt;
이 중 흥미로운 결과가 있었는데, geo-feature가 training accuracy에는 별다른 영향을 주지 않았는데 test accuracy에는 비교적 큰 성능 향상에 기여했습니다. 이는 covariance matrix의 eigenvalue로부터 추출한 geo-feature 값이 전반적인 low-level feature에 대한 정보를 포함하고 있고, 결과적으로 네트워크의 일반화에 기여한 것으로 보입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;데이터 소실 및 잡음에 대한 강도 실험
&lt;img src=&quot;assets/images/210127-GACnet/9-robustness-test.png&quot; alt=&quot;&quot; /&gt;
마지막으로 데이터의 일부가 소실되거나 Gaussian noise가 추가되었을 때 classification 성능이 어떻게 변화하는지를 실험하였습니다. GAC 모듈을 max-pooling 함수로 치환하여 비교하였는데, 두 경우 모두 GACnet이 훨씬 더 robust한 결과를 보여주었습니다. 아마 max-pooling 함수를 이용하게 되면, noise 값이 크게 튀는 경우에 이를 대표적인 feature 값으로 인식되어 classification 정확도를 낮추지 않았을까 생각됩니다. 반면에 GAC는 상대적 위치정보 뿐만아니라 여러 성분들(RGB 등)로 구성된 특징 정보를 활용하기 때문에 (정보량이 많기 때문에), 방해 요소들이 있어도 robust한 결과를 보여주는 것 같습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Graph attention convolution은 주변 점들에 따라 attention weight을 다르게 가하는 방식을 통해 dynamic한 receptive field를 가지는 convolution kernel를 생성할 수 있었고, 이를 통해 feature vector를 추출했습니다. Edge convolution 등의 방법과 유사하면서도, 점들의 특징 벡터의 차이를 이용하여 비슷한 label을 가진 점들을 확실하게 일치시켜 주는 (CRF가 기존에 해주던 방식의) 연산 방식이 SOTA의 성능을 만들 수 있지 않았나 생각이 듭니다. 긴 글 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf&quot;&gt;https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yanx27/GACNet&quot;&gt;https://github.com/yanx27/GACNet&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3D" />
      

      
        <category term="3D" />
      

      
        <summary type="html">원문 : Wang, Lei, et al. “Graph attention convolution for point cloud semantic segmentation.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PointNet++ - Deep Hierarchical Feature Learning on Point Sets in a Metric Space 리뷰</title>
      <link href="http://localhost:4000/PointNet++" rel="alternate" type="text/html" title="PointNet++ - Deep Hierarchical Feature Learning on Point Sets in a Metric Space 리뷰" />
      <published>2021-01-21T09:00:00+09:00</published>
      <updated>2021-01-21T09:00:00+09:00</updated>
      <id>http://localhost:4000/PointNet++</id>
      <content type="html" xml:base="http://localhost:4000/PointNet++">&lt;p&gt;원문 : &lt;a href=&quot;https://papers.nips.cc/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf&quot;&gt;Qi, Charles Ruizhongtai, et al. “Pointnet++: Deep hierarchical feature learning on point sets in a metric space.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 Stanford에서 2017년 NIPS에 발표한 &lt;strong&gt;&lt;em&gt;Pointnet++: Deep hierarchical feature learning on point sets in a metric space&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Point cloud 형식의 데이터를 Deep learning 분야에 적용시킨 선구적인 논문인 PointNet의 후속편으로, local한 특징을 잡아내지 못하는 기존의 PointNet을 보완하여 classification 및 segmentation 성능을 크게 끌어올렸습니다.&lt;/p&gt;

&lt;p&gt;그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;pointnet&quot;&gt;PointNet&lt;/h2&gt;
&lt;p&gt;PointNet++에 대하여 설명드리기 전에, 전편인 PointNet에 관해서 간단히 짚고 넘어가겠습니다. (자세한 설명은 PointNet 논문 리뷰를 참고해주세요.) Point cloud는 3차원 공간 내의 물체를 표현하기 위한 데이터의 한 형식으로 각 점들에 대한 좌표값으로 구성되어 있습니다. Point cloud는 sparse한 점들의 집합이기 때문에 계산량 및 메모리 차원에서 효율적이지만, 순서가 없는 집합 형태의 데이터이기 때문에 neural network가 이를 처리하려면 permutation-invariant한 특성을 가진 layer를 포함하고 있어야 했습니다. PointNet에서는 symmetric function 중의 하나인 max-pooling 함수를 활용하여 point cloud 데이터에 대한 global한 feature vector를 추출하였습니다. 하지만 max-pooling 함수의 특성상 최대값을 제외한 local한 정보들을 소실될 수밖에 없고 이는 정교한 경계선 설정이 중요한 segmentation task 등에서 성능 저하의 요인이 되었습니다. PointNet++에서는 몇 가지 구조를 제안하여 local한 feature vector를 추출하였습니다.&lt;/p&gt;

&lt;p&gt;Local한 특징을 추출하는 것은 중요합니다. CNN에서도 다양한 receptive field를 가지는 convolution kernel을 활용함으로써 local한 특징을 추출하였고, 이는 2D image들에 대한 폭발적인 성능 향상을 이뤄냈습니다. PointNet++는 계층적 구조의 neural network를 활용하여 다양한 scale의 local feature를 추출하였고, 이를 통합하여 SOTA의 classification 및 segmentation 성능을 기록할 수 있었습니다. 그럼 PointNet++에 대해 좀 더 자세히 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;pointnet-1&quot;&gt;PointNet++&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210121-pointnet++/1-pointnet++.png&quot; alt=&quot;&quot; /&gt;
PointNet++는 &lt;strong&gt;Set abstraction layer&lt;/strong&gt;과 &lt;strong&gt;Density adaptive layer&lt;/strong&gt;로 구성되어 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;set-abstraction&quot;&gt;Set Abstraction&lt;/h2&gt;
&lt;p&gt;PointNet++에서는 set abstraction이라고 부르는 과정을 통해 point cloud의 local feature를 추출하였습니다. Set abstraction을 거치면, point 들의 집합은 이전보다 적은 수의 점들로 구성된 point cloud로 추상화되며, semantic한 정보를 담게 됩니다.&lt;/p&gt;

&lt;p&gt;Set abstraction은 세 단계로 구분할 수 있습니다. 우선 sampling 단계에서는 point cloud의 local한 부분집합의 중심에 해당하는 중요한 몇 개의 점들을 선택합니다. 이후 grouping 단계에서는 sampling 단계에서 찾은 중요한 몇 개의 점들과 이웃한 점들을 찾고 하나의 집합으로 구성합니다. 마지막으로 pointnet 단계에서는 local한 점들의 집합에 대한 패턴을 encoding하여 feature vector를 추출합니다.&lt;/p&gt;

&lt;p&gt;예를 들어 Point cloud가 &lt;em&gt;N&lt;/em&gt; 개의 점들로 구성되어 있고 각 점은 &lt;em&gt;d&lt;/em&gt; 차원의 좌표를 가지며 &lt;em&gt;C&lt;/em&gt; 차원의 feature vector를 가진다고 가정하겠습니다. Set abstraction 과정을 거치면 &lt;em&gt;N&lt;/em&gt; x &lt;em&gt;(d+C)&lt;/em&gt; 크기의 input 행렬이 &lt;em&gt;N’&lt;/em&gt; x &lt;em&gt;(d+C’)&lt;/em&gt; 크기의 행렬로 변환되어 출력됩니다. 이 때 &lt;em&gt;N’&lt;/em&gt; 는 subsampling된 점들의 개수이고, &lt;em&gt;C’&lt;/em&gt; 는 중심점 행렬의 feature vector의 차원입니다.&lt;/p&gt;

&lt;p&gt;Sampling 단계에서는 &lt;em&gt;N&lt;/em&gt; 개의 점들 중 &lt;em&gt;N’&lt;/em&gt; 개의 점을 중심점으로 선택합니다. 이 &lt;em&gt;N’&lt;/em&gt; 개의 점들은 전체 점들 중 나머지 점들과의 거리가 가장 먼 점들로 구성됩니다. 이때 이 거리라는 개념은 Uclidian distance 일 수도 있고 그 외 다른 거리 관련 metric일 수도 있습니다. 논문에서는 이 거리가 가장 먼 점들의 집합을 Farthest point sampling (FPS) 알고리즘을 반복적으로 적용해 얻었다고 설명하였습니다. 또한, 이렇게 서로 최대한 떨어져 있는 점들을 선택하게 되면 random sampling을 통해서 점들을 선택하는 것보다 더 일반적이고 전체적인 범위의 point들을 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;Grouping 단계에서는 sampling 단계에서 추출한 &lt;em&gt;N’&lt;/em&gt; 개 중심점들의 주변 점들을 선택하여 local한 영역으로 묶어줍니다. 각 중심점과 묶이는 주변 점들의 개수는 영역마다 다를 수 있는데, 후에 설명드릴 pointnet 단계에서 점들의 개수와 상관 없이 같은 크기의 feature vector로 변환해주기 때문입니다. 또한 이웃한 점들을 선택하는 방식도 두 가지로 나뉠 수 있습니다. 우선, ball query 방식은 반지름 &lt;em&gt;r&lt;/em&gt; 을 정해 이 반지름 내에 있는 점들을 모두 이웃한 점으로 선택하는 방식입니다. 또한, kNN 방식은 정해진 &lt;em&gt;K&lt;/em&gt; 라는 개수의 가장 가까운 점들을 이웃한 점으로 선택하는 방식입니다. 논문에서는 ball query 방식을 선택했는데, kNN과 비교했을 때 정해진 크기의 지역을 확실하게 표현할 수 있기 때문입니다. 이는 일정하게 sampling되지 않은 point cloud 데이터에서 영역 별로 범위가 달라지는 문제를 방지할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;마지막으로 pointnet 단계에서는 각 local한 영역에 대해 특징 정보를 encoding한 하나의 feature vector를 추출할 수 있습니다. 각 중심점(총 &lt;em&gt;N’&lt;/em&gt; 개)별로 생성된 &lt;em&gt;K&lt;/em&gt; 개의 이웃한 점들에 대한 &lt;em&gt;(d+C)&lt;/em&gt; 크기의 feature vector를 행렬로 표현하면 &lt;em&gt;N’&lt;/em&gt; x &lt;em&gt;K&lt;/em&gt; x &lt;em&gt;(d+C’)&lt;/em&gt; 크기의 행렬이 되는데, 이 행렬이 pointnet 단계를 거치면 &lt;em&gt;N’&lt;/em&gt;x&lt;em&gt;(d+C’)&lt;/em&gt; 크기의 feature vector로 변환됩니다. 이 때, 점들의 좌표값은 중심점과의 상대적인 거리의 차이에 대한 값이 들어가게 되는데, 이를 통해 점들 사이의 위치적 관계에 대한 정보를 잡아낼 수 있기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;density-adaptive-feature-learning&quot;&gt;Density Adaptive Feature Learning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210121-pointnet++/2-nonuniform-pointcloud.png&quot; alt=&quot;&quot; /&gt;
Point cloud 데이터는 위의 그림처럼 non-uniform한 밀도 분포를 가지는 것이 대부분입니다. 이렇게 일정하지 않은 점들의 분포는 point cloud의 특징에 대한 학습을 어렵게 만드는데, density가 높은 point cloud에 대해 학습한 네트워크는 sparse한 점들의 point cloud를 만났을 때 정보가 부족하다고 느끼고 특징을 제대로 표현하지 못하기 때문입니다. (그 반대의 경우도 마찬가지입니다.) 논문에서는 이를 해결하기 위해 point cloud의 sampling density를 변화시키며 학습하였습니다. 또한, 다양한 scale의 point cloud에서 feature vector를 추출하는 multi-scale feature extraction 구조를 활용하였는데, 저자는 이를 &lt;em&gt;density adaptive layer&lt;/em&gt; 이라고 표현했습니다. 다양한 크기의 density를 가지는 point cloud를 학습하는 두 가지 방법에 대해 소개하겠습니다.
&lt;img src=&quot;assets/images/210121-pointnet++/3-grouping.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-scale grouping (MSG)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우선 grouping 단계를 다양한 scale로 여러 번 적용하여 하나의 중심점에 대해 여러 scale의 point group을 얻는 방법이 있습니다. 각 point group에서 각각 추출한 feature vector를 이어붙이면(concatenate), multi-scale feature vector를 얻을 수 있게 됩니다. 이 때, 각 point group은 임의의 dropout ratio를 선택하여 그 비율에 맞게 random하게 down-sampling(dropout) 하여 각 point group마다 서로 다른 scale로 균일하지 않은 density를 가지게끔 변환해주었습니다. 이러한 과정을 거치면 다양한 sparsity와 서로 다른 uniformity를 가지는 점들을 얻을 수 있습니다. 하지만 MSG는 각 중심점들과 그 이웃한 점들이 모두 pointnet을 거쳐야 하므로 게산량 차원에서 아주 비효율적이고 time-consuming하다는 단점이 있습니다. 논문에서는 이를 보완하기 위해 multi-resolution grouping을 제안하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-resolution grouping (MRG)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MRG는 MSG의 단점을 보완한 grouping 방식입니다. MRG는 서로 다른 scale로 얻은 두 feature vector를 이어붙여서(concatenate) multi-scale feature vector를 얻습니다. 이 때, 첫 번째 vector는 local group에 해당하는 점들 전체에 대해 pointnet 단계를 거쳐서 얻고, 두 번째 vector는 local group에 대해 그보다 한 단계 아래의 sub-region에서 얻은 feature를 종합하여 얻습니다. 저자는, 만약 input으로 들어오는 point cloud의 density가 낮다면, 첫 번째 vector에 의해 전반적인 특징에 대한 정보를 추출할 수 있고, density가 높다면, sub-region에 대한 feature가 높은 resolution 더 디테일한 특징 정보를 제공할 수 있다고 이야기합니다. 따라서 두 vector를 모두 이용하게 되면 여러 density의 point cloud에 대해서 모두 대응할 수 있으며, 계산량 측면에서도 효율적이라고 말합니다.&lt;/p&gt;

&lt;h2 id=&quot;point-feature-propagation&quot;&gt;Point Feature Propagation&lt;/h2&gt;
&lt;p&gt;Set abstracion layer를 거치게 되면, sampling 단계에 의해 point cloud의 크기가 줄어들게 됩니다. 이렇게 얻은 feature vector를 segmentation task에 활용하려면 다시 원래의 크기로 복원해주어야 합니다. 복원해주지 않고 set abstraction을 하기 위해 모든 점들을 중심점으로 지정해서 feature aggregation을 해주는 방법도 있지만, 이는 computation cost가 너무 많이 들기 때문에 논문에서는 point cloud를 down-sampling하고 다시 interpolation 기반의 방법을 통해 up-sampling하는 방식을 제안하고 있습니다.&lt;/p&gt;

&lt;p&gt;구체적으로, 이전 점들에 대한 feature vector로부터 (&lt;em&gt;1/거리값)&lt;/em&gt; 으로 weighting을 가해서 interpolation하는 방법을 이용하였습니다. 또한 down-sampling 되기 전의 feature vector를 skip-connection을 통해 concatenate하여 부족할 수도 있는 정보량을 보충해주었습니다. Interpolation 과정은 원래 point의 개수로 맞춰질 때까지 반복해주었고, 결과로 얻은 feature vector를 통해서 segmentation task를 수행해주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;PointNet++는 MNIST(2D Object), ModelNet40(3D Object), ScanNet(3D Scene) 등의 다양한 데이터셋에 대한 evaluation 과정을 통해 classifiaction 및 segmentation task에서 성능을 증명하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST
&lt;img src=&quot;assets/images/210121-pointnet++/4-mnist-result.png&quot; alt=&quot;&quot; /&gt;
MNIST 데이터셋은 손글씨 숫자에 대한 60,000개 이상의 image입니다. PointNet++는 2D image의 좌표를 2D point cloud 형태로 변환하여 input으로 사용했는데, 기존 PointNet과 비교했을 때 error rate이 30% 이상 감소하는 등의 성능 향상을 보여주었습니다. 또한 기존 CNN 기반의 모델들과 비교했을 때도 더 좋은 성능을 보이기도 했습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ModelNet40
&lt;img src=&quot;assets/images/210121-pointnet++/5-modelnet-result.png&quot; alt=&quot;&quot; /&gt;
ModelNet40 데이터셋은 40개의 클래스에 대한 CAD 모델입니다. CAD 모델은 3D Mesh 형태를 띄고 있기 때문에, 논문에서는 mesh의 표면을 sampling하여 3D point cloud 형태로 변환한 후에 PointNet++의 input으로 사용하였습니다. 또한 모델 구조는 3단계의 계층 단계에 3개의 Fully connected layer를 이어붙여서 구성했으며, 모든 point들의 좌표는 반지름 1의 구 안에 들어오게끔 하여 normalization을 해주었습니다. PointNet++는 3D classification task에서도 기존의 SOTA 모델로 알려진 MVCNN의 성능을 크게 뛰어넘었습니다.
&lt;img src=&quot;assets/images/210121-pointnet++/6-grouping-result.png&quot; alt=&quot;&quot; /&gt;
또한 ModelNet 데이터셋을 활용하여 Density adaptive layer의 성능을 측정하는 ablation study를 진행했습니다. 위의 그림을 보면, 1024개의 점에 대한 point cloud로부터 random하게 점을 지워서 512, 256, 128개로 down-sampling하였습니다. 점의 개수가 다른 point cloud를 PointNet 및 PointNet++의 input으로 넣어주었을 때, 앞서 설명드린 density aadaptive layer (MSG 또는 MRG)를 적용시켜 multi-scale로 학습한 모델들은 점의 개수가 달라져도 robust한 결과를 보여주었습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ScanNet&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ScanNet 데이터셋은 실내 환경에 대한 1500개 이상의 3D point cloud 데이터로 구성되어 있습니다. 각각의 점들에는 해당 점이 어떤 물체에 속해 있는지에 대한 segmentation label이 달려 있습니다. PointNet++는 ScanNet에 대한 segmentation task에서도 아주 뛰어난 결과를 보여주었습니다. 계층적 구조를 통한 local한 feature 학습이 다양한 scale의 scene을 이해하는데 중요하다고 해석할 수 있을 것 같습니다. 
&lt;img src=&quot;assets/images/210121-pointnet++/8-scannet-result.png&quot; alt=&quot;&quot; /&gt;
ScanNet에서도 sampling density가 달라졌을 때 robust한 결과를 도출할 수 있는지 실험하였습니다. ScanNet 데이터를 non-uniform한 sampling density로 줄여서 학습한 뒤에 결과를 확인해보았습니다. 앞선 ModelNet40을 이용한 실험 결과와 마찬가지로, MRG 네트워크가 Single-scale grouping을 통해 학습한 네트워크보다 다양한 density의 데이터들에 대해 훨씬 더 좋은 결과를 보여주었습니다. 
&lt;img src=&quot;assets/images/210121-pointnet++/7-grouping-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;PointNet과 더불어 PointNet++는 이전에 classification이나 detection 분야에서 활용되었던 multi-scale feature learning 기법을 point cloud 데이터에 적용시켜 급격한 성능 향상을 만들었습니다. 또한 Graph의 크기를 줄이는 graph coarsening이나 다시 graph의 크기를 키우는 point feature propagation 등 다양한 개념이 제시된 논문이라 의미가 크다고 생각합니다. 다음 번에는 이를 기반으로 attention 메커니즘을 적용시킨 논문을 한번 리뷰해보겠습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://stanford.edu/~rqi/pointnet2/&quot;&gt;http://stanford.edu/~rqi/pointnet2/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/charlesq34/pointnet2&quot;&gt;https://github.com/charlesq34/pointnet2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3D" />
      

      
        <category term="3D" />
      

      
        <summary type="html">원문 : Qi, Charles Ruizhongtai, et al. “Pointnet++: Deep hierarchical feature learning on point sets in a metric space.” Advances in neural information processing systems. 2017.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs 리뷰</title>
      <link href="http://localhost:4000/Edge-Conditioned-Convolution" rel="alternate" type="text/html" title="Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs 리뷰" />
      <published>2021-01-20T09:00:00+09:00</published>
      <updated>2021-01-20T09:00:00+09:00</updated>
      <id>http://localhost:4000/Edge-Conditioned-Convolution</id>
      <content type="html" xml:base="http://localhost:4000/Edge-Conditioned-Convolution">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/abs/1704.02901&quot;&gt;Simonovsky, Martin, and Nikos Komodakis. “Dynamic edge-conditioned filters in convolutional neural networks on graphs.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 ENPC에서 2017년 CVPR에 발표한 &lt;strong&gt;&lt;em&gt;Dynamic edge-conditioned filters in convolutional neural networks on graphs&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Point cloud 등의 graph 구조로 표현 가능한 데이터에서 convolution 형태의 연산을 통해 feature vector를 추출하는 하나의 방법에 대해 제시하고 있습니다.&lt;/p&gt;

&lt;p&gt;그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;graph-convolution&quot;&gt;Graph Convolution&lt;/h2&gt;
&lt;p&gt;Convolution(합성곱) 연산은 2D image에 대한 내재된 feature 벡터를 잘 추출해줌으로써 classification, segmentation 등의 여러 task들에 대한 성능을 폭발적으로 끌어올렸습니다. 학계에서는 2D image 등의 Uclidian 공간에 대한 데이터 뿐만 아니라 graph 등 non-Uclidian 공간 상의 데이터도 convolution 연산을 통해 처리할 수 있지 않을까에 대해 많이 고민했습니다.&lt;/p&gt;

&lt;p&gt;이러한 고민의 결과로 인접한 node 간의 특징 벡터를 모아주는 graph convolution 연산을 발견하였고, 이는 graph 형태로 표현 가능한 3D modeling, biological molecule, social network 등의 데이터에 활용되었습니다. 하지만 기존의 방법대로는 한 점과 인접한 점들의 feature 벡터를 균일하게(homogeneous) 더해줄 수 밖에 없었고, 이는 점들 간의 연관성의 정도를 전혀 고려하지 않은 연산이었습니다. (일반적인 discrete convolution으로 생각해보면, uniform function만을 filter로 사용하는 형태였습니다.)&lt;/p&gt;

&lt;p&gt;이를 보완하기 위해 논문에서는 점과 점을 연결한 edge에 점들 간의 관계를 나타내는 label을 부여해서 weight parameter처럼 활용하였습니다. 이렇게 edge label을 이용하여 graph convolution을 하게 되면, 단순히 점들을 homogeneous하게 더하는 것이 아니라 점들 간의 연관성을 고려하여 더해주기 때문에 주변 점들과의 관계가 반영된 feature 벡터를 추출할 수 있다고 합니다. 또한 이 feature vector를 이용해서 graph 또는 vertex 단위로 classification을 했을 때 점들의 관계 및 분포에 대한 정보가 녹아있어 정확도가 향상되었다고 합니다. 본문에서는 이렇게 edge label을 이용한 convolution 연산을 edge-conditioned convolution(ECC)라고 부릅니다.&lt;/p&gt;

&lt;h2 id=&quot;point-cloud&quot;&gt;Point cloud&lt;/h2&gt;
&lt;p&gt;Point cloud는 3D 물체를 표현하기 위한 형식 중의 하나입니다. 원래 point cloud는 일정한 형식이 없는 집합 형태의 데이터이기 때문에 neural network의 input으로 활용하기 어려웠습니다. 하지만 특정 점에 대해 정의된 거리(r) 내에 있는 점들 중 K-nearest neighbors 알고리즘을 적용해서 인접한 점들을 찾은 뒤, 서로 연결해서 graph 형태로 변환하게 되면 graph convolution을 적용할 수 있습니다. 따라서 논문에서는 여러 종류의 graph 데이터들 중 주로 point cloud를 graph로 변환한 데이터를 활용했습니다. Point cloud는 물체나 풍경을 3D로 표현했다는 특성상 graph 또는 vertex 단위로 label이 존재하기 때문에 graph convolution의 성능을 측정하기에 적절하기도 합니다. 또한 최근에는 RGB-D 카메라나 LiDaR의 출력 형식으로 사용되기 때문에 범용적이기도 합니다.&lt;/p&gt;

&lt;h2 id=&quot;edge-conditioned-convolution-ecc&quot;&gt;Edge-conditioned Convolution (ECC)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/edge-convolution/1-edge-convolution.png&quot; alt=&quot;&quot; /&gt;
Edge-conditioned convolution 연산 과정은 그리 어렵지 않습니다. Point cloud 데이터를 graph 형태로 변환하여 &lt;em&gt;n&lt;/em&gt;개의 node와 &lt;em&gt;m&lt;/em&gt;개의 edge를 얻었다고 가정하겠습니다. 이 때 node의 feature vector가 &lt;em&gt;l&lt;/em&gt; 차원, edge의 feature vector가 &lt;em&gt;s&lt;/em&gt; 차원이라고 하면 우리는 edge의 feature vector로부터 node feature vector의 transformation matrix (&lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt; 차원)를 얻는 미분 가능한 함수를 정의하고자 합니다. 논문에서는 가장 간단한 multi-layer perceptron을 활용하였는데, input이 &lt;em&gt;s&lt;/em&gt;차원 &amp;amp; output이 &lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt; 차원인 linear layer입니다. 이렇게 각 node마다 &lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt; 차원의 “edge-specific weights”를 얻을 수 있고, 이는 edge feature vector에서 추출한 행렬이기 때문에 점들 사이의 관계를 담고 있습니다. 이렇게 얻은 weight을 이웃한 각 node feature vector에 곱해주고 normalize하면 edge-conditioned convolution을 거친 해당 점의 feature vector를 얻을 수 있습니다.
&lt;img src=&quot;assets/images/edge-convolution/2-one-hot-edge.png&quot; alt=&quot;&quot; /&gt;
각 edge feature vector를 순서에 맞는 one-hot vector로 설정하면 edge-conditioned convolution 연산은 uniform function에 대한 convolution과 같아집니다. 위의 그림에서 ECC는 filter size가 3인 1차원의 discrete convolution과 정확히 같아지는 것을 확인할 수 있습니다.
ECC의 특징 중의 하나가 multi-hop convolution이 불가능하다는 점입니다. 다른 graph convolution 연산의 경우에는 인접 행렬을 여러번 곱해서 multi-hop의 범위를 가지는 node에 대해서도 feature aggregation을 할 수가 있지만, ECC는 인접 행렬을 이용하지 않기 때문에 multi-hop 연산이 불가능합니다. 하지만 저자는 일반적인 2D convolution에서도 크기가 큰 filter를 이용하는 것보다 3x3 크기의 filter를 여러개 이어붙이는 것이 더 좋은 feature를 추출하기 때문에 ECC layer도 여러번 반복하면 multi-hop convolution보다 좋은 성능을 낼 것이라고 이야기합니다.
ECC layer의 끝에는 다른 convolution layer와 마찬가지로 batch normalization layer를 붙여주어 학습을 효율적으로 진행할 수 있게 하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;deep-network-with-ecc&quot;&gt;Deep network with ECC&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/edge-convolution/3-network-architecture.png&quot; alt=&quot;&quot; /&gt;
ECC 연산을 통해 2D image data에 대한 neural network와 비슷한 구조를 graph 구조에 대해서도 구현할 수 있습니다. 위 그림은 ECC를 이용한 convolution layer와 graph-pooling 및 coarsening 과정을 이용한 pooling layer를 이어붙여 deep neural network를 구현한 그림입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Graph pooling &amp;amp; coarsening
ECC는 nearest neighborhood를 통해 정의된 그래프의 구조를 그대로 유지한 결과를 출력합니다. 하지만 classification task의 출력 형식인 C(number of class)차원의 출력 벡터를 얻기 위해서는 graph의 크기를 줄여서 node feature을 합쳐주어야 합니다. Graph의 크기를 줄이는 과정을 graph coarsening, node feature를 합치는 과정을 graph pooling이라고 부릅니다. Coarsening을 반복하게 되면 결국에는 서로 연결되지 않은 몇 개의 node들만 남게 됩니다. 이 때 각 node들은 self-loop을 가지고 있기 때문에 여전히 graph의 형태를 띈다고 할 수 있으며 global max pooling 등의 과정을 통해 마지막 남은 feature vector를 합쳐줄 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Graph coarsening에는 다양한 알고리즘들이 활용될 수 있는데, 논문에서는 특정한 resolution을 가지는 3차원의 grid를 생성해서 해당 grid 안의 점들을 하나로 모아주는 VoxelGrid 알고리즘을 이용했습니다. 합쳐진 점들에 대해서는 다시 nearest neighbor을 통해 graph로 변환해줍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Edge label
Edge label은 해당 edge와 연결된 두 vertex 좌표값의 차이로 설정했습니다. Cartesian 및 Spherical coordinate으로 변환하여 총 6차원의 edge feature vector를 활용하였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Augmentation
학습할 때에는 dataset의 규모가 크지 않기 때문에 여러 방법으로 augmentation을 하여 overfitting을 방지하였습니다. 논문에서는 point cloud를 임의로 회전시키거나, 크기를 변화시키거나, 특정 축으로 대칭시키거나, 몇개의 점을 지우는 등의 방법을 통해 데이터 다양성을 확보했다고 주장합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;ECC는 다양한 형태의 point cloud 및 graph classification 실험을 통해 성능을 보여주었습니다. 오늘은 대표적으로 ModelNet 데이터셋과 MNIST 데이터셋을 활용한 실험을 소개하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ModelNet
&lt;img src=&quot;assets/images/edge-convolution/4-modelnet-result.png&quot; alt=&quot;&quot; /&gt;
ModelNet 데이터셋은 물체에 대한 3D Mesh 형태의 데이터입니다. 카테고리 개수에 따라 ModelNet10과 ModelNet40으로 구분되며 각각 4000개, 10000개 정도의 학습 데이터가 존재합니다. 논문에서는 Mesh 형태의 데이터의 표면에서 uniform하게 1000개씩의 point를 sampling하여 point cloud 형태로 변환해주었습니다. 신경망 구조는 &lt;strong&gt;&lt;em&gt;ECC(16)-ECC(32)-MP(2.5/32,7.5/32)-ECC(32)-ECC(32)-MP(7.5/32,22.5/32)-ECC(64)-GMP-FC(64)-Dropout(0.2)-FC(10 or 40)&lt;/em&gt;&lt;/strong&gt; 의 형태를 이용하였고 100 epoch의 학습을 진행하였습니다. Classification에 대한 전반적인 accuracy는 SOTA만큼은 아니지만 경쟁력 있는 수준을 보여주었습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST
논문에서는 숫자에 대한 28x28 크기의 이미지 데이터셋인 MNIST 데이터셋을 이용한 성능도 측정하였습니다. 우선 ECC를 적용할 수 있게 2D image를 &lt;em&gt;(x, y, 0)&lt;/em&gt;의 좌표를 가지는 point cloud 형태로 변환하고 nearest neighbor 알고리즘을 통해 다시 graph 형태로 변환하였습니다. 여기서는 &lt;strong&gt;&lt;em&gt;C(16)-MP(2,3.4)-C(32)-MP(4,6.8)-C(64)-MP(8,30)-C(128)-D(0.5)-FC(10)&lt;/em&gt;&lt;/strong&gt; 형태의 네트워크 구조를 활용하여 20 epoch만큼 학습을 진행하였습니다.
&lt;img src=&quot;assets/images/edge-convolution/5-mnist-result.png&quot; alt=&quot;&quot; /&gt;
학습 결과는 위와 같습니다. 기존의 baseline 모델들과 비교해도 괜찮은 결과를 도출하였습니다. 또한 2D image에서 어두운 부분을 제외하고 graph 형태로 변환하여 학습을 시켜도 비슷한 결과를 도출하였습니다. 이는 graph의 형태가 바뀌어도 neural network가 학습 및 추론을 안정적으로 할 수 있다는 것을 의미합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;논문에서는 edge label을 처음으로 사용한 Edge-conditioned Convolution 연산을 제시하였고, 이를 활용하여 graph 형태의 데이터를 위한 feed-forward network를 구성하였습니다. 추후에 진행되는 attention 기반의 graph convolution 등의 연구의 기반이 되는 중요한 연구라고 생각합니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.html&quot;&gt;https://openaccess.thecvf.com/content_cvpr_2017/html/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="NLP" />
      

      
        <category term="3D" />
      

      
        <summary type="html">원문 : Simonovsky, Martin, and Nikos Komodakis. “Dynamic edge-conditioned filters in convolutional neural networks on graphs.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">BERT - Pretraining of Deep Bidirectional Transformers for Language Understanding 리뷰</title>
      <link href="http://localhost:4000/BERT" rel="alternate" type="text/html" title="BERT - Pretraining of Deep Bidirectional Transformers for Language Understanding 리뷰" />
      <published>2020-11-03T09:00:00+09:00</published>
      <updated>2020-11-03T09:00:00+09:00</updated>
      <id>http://localhost:4000/BERT</id>
      <content type="html" xml:base="http://localhost:4000/BERT">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” arXiv preprint arXiv:1810.04805 (2018).
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/bert/bert.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 Google AI Language에서 2019년 arxiv에 발표한 &lt;strong&gt;&lt;em&gt;Bert: Pre-training of deep bidirectional transformers for language understanding&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Transformer Encoder 구조를 활용하여 자연어에 대한 representation을 pre-training 하였습니다. Unsupervised learning 방식으로 학습을 진행하기 위해 Masked Language Model, Nest Sequence Prediction의 두 가지 학습 방식을 이용하였으며, 11개의 자연어 학습 능력 평가 지표에서 SOTA를 달성했다고 합니다.&lt;/p&gt;

&lt;p&gt;그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;feature-based-and-fine-tuning&quot;&gt;Feature-based and Fine-tuning&lt;/h2&gt;
&lt;p&gt;언어 모델을 pre-training 방식으로 학습하여 여러 가지 task에 적용하는 것은 효율성과 성능의 측면에서 우수했기 때문에 자연어 분야의 학습 전략으로 널리 활용되어 왔습니다. 모델을 pre-training 한 뒤에, task에 알맞게 이용하는 방식에는 크게 두 가지가 있습니다.&lt;/p&gt;

&lt;p&gt;우선 Feature-based 방식은 pre-training 과정에서 각 단어에 대한 representation이 완벽하게 학습되었다고 가정하기 때문에, task-specific한 재학습 과정에서 pre-trained network의 parameter들을 바꾸지 않습니다. 대신에 pre-trainied network를 통해 얻은 Embedding vector를 task에 맞는 network에 통과시켜서 그 network의 parameter를 학습시키는 방향으로 task를 수행합니다.
반면에 Fine-tuning 방식은 downstream task를 학습할 때 pre-trained network의 parameter를 전부 조정해줍니다. Feature-based 방식에 비해 Network capacity가 커지기 때문에 일반적으로 높은 정확도를 보이지만, 계산량도 마찬가지로 늘어난다는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;BERT는 Fine-tuning 방식을 기반으로 한 모델입니다. BERT를 각 downstream task 별로 어떻게 fine-tuning 했는지는 아래에서 자세히 다루겠습니다.&lt;/p&gt;
&lt;h2 id=&quot;bert-model-architecture&quot;&gt;BERT Model Architecture&lt;/h2&gt;
&lt;p&gt;BERT는 Vaswani의 논문에서 발표된 Transformer의 Encoder 구조를 그대로 차용하였습니다. 뒤에서 자세히 설명드리겠지만 encoder의 self-attention 메커니즘은 병렬적으로 output을 생성하기 때문에, 두 방향(bidirectional)에 대한 추론을 통해 학습할 수 있습니다. 이는 BERT가 기존의 단방향 모델(OpenAI GPT) 또는 제약이 있는 양방향 모델(Bi-LSTM을 활용한 ELMo 등)보다 좋은 성과를 내는 데에 큰 기여를 하게 됩니다.&lt;/p&gt;

&lt;p&gt;논문에서는 두 개의 모델을 공개하였는데, transformer layer 개수(12/24), hidden size(768/1024), self-attention head의 개수(12/16)에 따라 BERT base 모델과 BERT large 모델로 구분하였습니다. 각각의 parameter 개수는 110M/340M으로 BERT large가 약 세 배 정도의 capacity를 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;BERT는 단일 문장에 대한 downstream task 뿐만 아니라 두 개 이상의 문장에 대해서도 잘 작동할 수 있도록 input의 형태를 변형하였습니다. BERT의 input인 sequence는 한 개 또는 두 개 이상의 문장으로 구성되었습니다. 각 sequence은 항상 [CLS] token으로 시작되는데, 이 [CLS] token에 대한 output은 classification이 필요한 task의 경우 그 probability value로 이용하였습니다. 만약 한 sequence가 두 개 이상의 문장으로 구성된 형태라면 첫 번째 문장과 두 번째 문장 사이에 [SEP] token을 넣어서 구분해주었습니다. 또한, 두 문장을 구분하기 위한 추가적인 장치로 첫 번째 문장과 두 번째 문장에 서로 다른 sentence embedding을 더해주었습니다.&lt;/p&gt;

&lt;p&gt;문장 구성에 대한 이해를 위해 downstream task 몇 가지를 예를 들어 설명하겠습니다. 만약 한 문장 안에 하나의 빈칸을 뚫어 그 단어를 예측하는 형태의 task는, input으로 하나의 문장이 들어가고 output으로는 한개의 단어가 나오면 충분할 것입니다. 하지만 어떤 문단과 그에 대한 질문의 답을 찾는 복잡한 언어 추론 형태의 task에서는, 질문이 input의 첫 번째 문장으로 들어가고 문단이 input의 두 번째 문장으로 들어가며 두 문장에 대한 명확한 구분이 필요할 것입니다. 또한 output으로는 질문에 대한 답에 해당되는 문단의 위치를 start/end token으로 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/bert/input-representation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과적으로 BERT의 input representation은 token embedding, positional embedding, segment embedding의 합으로 구성됩니다. 우선 token embedding은 original input이 WordPiece Tokenization을 통해서 변환된 결과입니다. 또한 Transformer 논문에서 input에 순서 정보를 반영하기 위해서 활용한 positional embedding을 BERT에서도 마찬가지로 더해주었습니다. 마지막으로 두 개 이상의 문장이 input으로 들어올 수 있기 때문에 이를 구분하기 위한 segment embedding을 더해주었습니다.&lt;/p&gt;
&lt;h2 id=&quot;pre-training-strategy&quot;&gt;Pre-training Strategy&lt;/h2&gt;
&lt;p&gt;BERT의 pre-training에는 중요한 두 가지의 unsupervised task가 활용되었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Masked Language Model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;자연어를 잘 이해하고 있는 모델을 얻기 위해서는, 양방향 학습을 하는것이 아주 중요하다고 알려져 있습니다. 하지만 이전까지 대부분의 모델들은 LTR(Left to Right) 또는 RTL(Right to Left)이라고 불리는 단방향 학습이나 Bi-LSTM의 output을 이어붙이는 불완전한 양방향 학습을 통해 얻어지곤 했었습니다. BERT는 Transformer 구조와 Masked LM을 활용하여 multi-layer의 완벽한 양방향 학습을 구현하였습니다.&lt;/p&gt;

&lt;p&gt;Masked Language Model은 문장의 일부분을 masking token으로 치환해서 input으로 넣어주고, 해당 mask token을 원래 단어로 올바르게 복원하도록 학습하는 전략입니다. BERT에서는 문장의 모든 토큰 중 15%를 임의로 선택해 치환하였습니다.&lt;/p&gt;

&lt;p&gt;하지만 이 과정에서 발생할 수 있는 문제가 있습니다. 학습 과정에서는 문장의 일부가 Mask 토큰으로 바뀌어 있는 input을 거의 대부분 보게 되지만, 실제 downstream task에 적용할 때는 Mask 토큰이 없는 문장을 주로 보게 됩니다. 따라서 training과 test 환경의 mismatch가 발생하여 성능이 저하됩니다.&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해서 논문에서는 15%의 확률로 선택된 토큰들에 대해서, (1) 80%는 [Mask] 토큰으로 변경하고, (2) 10%는 다른 임의의 토큰으로 변경하고, (3) 10%는 원래 토큰으로 그대로 두는 전략을 이용하였습니다. 이렇게 전혀 다른 토큰을 추가함으로써 모델의 학습이 방해될 수 있다는 지적에 대해, 저자는 전체 토큰의 1.5%의 만에 해당되기 때문에 큰 영향이 없을 것이라고 답했으며 실제 실험을 통해 증명하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/bert/ablation-mask.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MLM에 대한 예시를 몇개 보여드리면, 기존의 “My dog is hairy”라는 문장에 대해 4번째 토큰이 선택되었다면, 해당 문장은
(1) 80%의 확률로 “My dog is [Mask]”
(2) 10%의 확률로 “My dog is apple”
(3) 10%의 확률로 “My dog is hairy”
로 변환될 수 있습니다.&lt;/p&gt;

&lt;p&gt;BERT와 비교되는 유명한 pre-trained language representation 모델들은 단방향 학습이나 불완전한 양방향 학습을 통해 생성되었습니다. 아래는 BERT와 OpenAI GPT, ELMo의 모델 구조를 비교한 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/bert/elmo-gpt.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 보이듯 완벽한 Bidirectional 구조를 가진 BERT와는 달리, OpenAI GPT는 Transformer decoder 구조를 차용한듯한 왼쪽에서 오른쪽을 향하는 단방향의 구조를 가지고 있습니다. 또한 ELMo 같은 경우에는, 왼쪽에서 오른쪽을 향하는 LSTM과 오른쪽에서 왼쪽을 향하는 LSTM의 output을 concatenate하여 양방향의 특성을 모두 가지는 representation을 생성하기 위한 구조를 보여주었습니다. 다만 두 LSTM의 결과를 이어붙인 것에 지나지 않기 때문에 완전한 양방향 학습이라고 보기는 어려우며, 계산량도 매우 많고 느리다는 단점이 있습니다. 따라서 Layer를 이어붙여서 깊게 쌓을 수 없고, 높은 성능을 보여주기 어렵게 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Next Sentence Prediction&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Question and Answering(QA) 또는 Natural Language Inference(NLI) 등 몇 개의 downstream task는 두 문장 사이의 관계를 이해해야만 해결할 수 있습니다. 언어 모델이 문장 사이의 관계를 학습할 수 있게 하기 위해서 BERT는 NSP라는 학습 전략을 활용하였습니다. NSP는 corpus에서 A와 B라는 두 개의 문장을 고르되, 50%는 A와 B가 내용적으로 연결되게 선택하고 나머지 50%는 A와 B가 내용적으로 전혀 관련이 없게 선택합니다. 각각은 IsNext와 NotNext라는 binary output으로 labeling한 뒤 이를 예측하게끔 언어 모델을 학습시킵니다.&lt;/p&gt;

&lt;p&gt;BERT의 NSP 정확도는 최종적으로 97~98%이었으며, 꽤나 높은 문장 간의 이해도를 보이게 됩니다. 이처럼 쉽고 간단한 NSP 학습 전략은 QA와 NLI 등의 downstream task 성능도 크게 향상시켜주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pre-training data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BERT는 pre-training corpus로 800M 개의 단어로 구성된 BooksCorpus와 2,500M개의 단어로 구성된 English Wikipedia dataset을 활용했습니다. (list, table, header 등은 학습에서 제외하였습니다.)&lt;/p&gt;
&lt;h2 id=&quot;fine-tuning-bert&quot;&gt;Fine-tuning BERT&lt;/h2&gt;
&lt;p&gt;Pre-training된 BERT 모델은 fine-tuning을 통해 downstream task에 적용하였습니다. Fine-tuning은 BERT 모델에 task-specific한 input-output 데이터를 넣고 end-to-end로 모든 parameter를 학습하였습니다. Pre-training 과정에 비해 훨씬 더 가볍게 진행되었으며, Cloud TPU 1개로 1시간 정도 학습하였습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 BERT의 성능을 검증하기 위해 11개의 downstream task에 대해 fine-tuning을 진행했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GLUE
&lt;img src=&quot;assets/images/bert/result-glue.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;8개의 task로 구성된 The General Language Understanding Evaluation(GLUE) task에 대해서 SOTA의 성능을 보여주었다. 모든 GLUE task에 대해 32의 batch size로 3 epoch만큼 fine-tuning 해주었으며, [CLS] 토큰의 output을 마지막 classification layer의 input으로 활용하여 K개의 class에 대해 분류했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SQuAD
&lt;img src=&quot;assets/images/bert/result-squad.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SQuAD는 The Stanford Question Answering Dataset의 약자로 100k개 이상의 질문-정답 쌍으로 구성되어 있습니다. 질문과 정답을 포함한 Wikipedia 문단을 input으로 넣으면, 문단 내의 정답 text 위치를 찾아내어야 합니다. BERT는 위에서 설명한 NSP 전략을 활용해 문장 사이의 관계를 이해하도록 학습되었기 때문에, SQuAD에 대해서도 SOTA의 성능을 보여줄 수 있었습니다.&lt;/p&gt;

&lt;p&gt;SQuAD 2.0은 정답이 없을 확률에 대해서도 label이 되었는데, 이 또한 마찬가지로 BERT는 SOTA의 성능을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SWAG
&lt;img src=&quot;assets/images/bert/result-swag.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SWAG는 The Situations With Adversarial Generations의 약자로 113k개의 문장 쌍으로 구성되어 있습니다. 한 문장이 주어질 때, 뒤에 나오기에 적절한 문장을 4개의 보기 중에 선택하는 형태로 구성되었습니다. [CLS] 토큰을 활용하여 4개의 선택지에 대한 score를 얻어서 평가하였습니다. BERT는 SWAG 데이터셋에 대해서도 마찬가지로 SOTA의 성능을 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/bert/fine-tuning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;ablation-studies&quot;&gt;Ablation Studies&lt;/h2&gt;

&lt;p&gt;BERT의 성능은 다양한 downstream task들에서 SOTA의 수준으로 확인되었습니다. 그렇다면 앞에서 제안한 BERT의 어떤 특징들이 우수한 성능을 발휘할 수 있게 해준걸까요? 논문에서는 Ablation study를 통해 다양한 장치들의 역할을 분석하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pre-training Strategies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BERT는 MLM과 NSP라는 두 개의 Task로 Pre-training 되었습니다. 논문에서는 각각의 task가 얼마나 중요한지를 보여주기 위해 NSP를 없애고 학습시켜보기도 하고, NSP를 없애고 단방향으로의(LTR) 학습을 진행해보기도 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/bert/ablation-pretraining.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NSP를 없애자 GLUE와 SQuAD의 성능이 동시에 크게 저하되었습니다. 문장 간의 이해도를 높이는 데 NSP가 중요한 역할을 하고 있으며, 일반적인 Language understanding에도 영향을 미치는 것으로 보입니다.&lt;/p&gt;

&lt;p&gt;또한 MLM을 통한 bidirectional 학습에서 LTR의 단방향 학습으로 변경했을 때에는 SQuAD의 성능이 급격히 저하되었습니다. SQuAD는 단방향 토큰 추론에 대한 task가 아니기 때문에 방향성이 제한됨에 따라 성능이 저하되는 것으로 추측할 수 있을 것 같습니다. 저자는 이를 보완하기 위해서 Bi-LSTM 구조를 모델 위에 얹어서 학습했는데, SQuAD의 성능은 향상시켜주었지만 일반적인 GLUE 결과를 저하시켰습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model Size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BERT 모델의 크기와 fine-tuning task의 정확도 간의 관계를 확인하기 위한 ablation study도 진행되었습니다. 다양한 layer 개수, hidden unit의 크기, attention head의 개수의 BERT 모델들이 같은 조건 하에서 학습되었습니다. 결과는 널리 알려진 바와 같이 모델의 크기가 커질수록 모델 capacity가 커지며 성능이 향상되었습니다. 또한 큰 capacity의 모델을 pre-training 한 경우에는 아주 작은 scale의 task에 대해서도 좋은 성능을 보여주었습니다. 이는 task의 scale이 아주 작은 경우에 BERT의 효용성이 높다는 의미인데, task-specific한 모델을 설계해서 scratch부터 학습하는 것보다 충분한 pre-training을 통해 얻은 general representation을 initial parameter로 fine-tuning을 하는 것이 더 좋은 성능을 보여주기 때문입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature-based BERT&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;기존의 BERT는 downstream task에 대해 fine-tuning 방식으로 학습하였습니다. 하지만 fine-tuning 방식은 모델의 구조를 크게 바꾸지 않기 때문에 모든 경우의 downstream task를 represent하기 어렵고, 모든 parameter를 update하므로 계산량도 많다는 단점이 있습니다. 때문에 ELMo와 같은 feature-based approach로 BERT를 활용했을 때 결과가 어떨지에 대해 ablation study가 진행되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/bert/ablation-feature.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각 단어의 Entity를 찾는 NER task에 대해 다양한 방식의 feature-based 학습을 진행했는데, SOTA와 비슷한 성능을 보여주었습니다. 이로써 BERT는 fine-tuning과 feature-based approach에 모두 효과적이라는 것이 확인되었습니다.&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;BERT는 다양한 NLP task에 대해 SOTA를 달성하면서, 발표 직후부터 지금까지 NLP 분야에 많은 영향력을 끼치고 있는 모델입니다. Bidirectional training을 위해 기존에 활용되지 않던 Masked Model Language를 도입했고, Next Sentence Prediction task를 통해 문장 사이의 관계도 효과적으로 학습하였습니다. 최근까지도 BERT를 기반으로 하거나 BERT를 뛰어넘기 위해 도전하는 논문들이 계속해서 출간되고 있습니다.&lt;/p&gt;

&lt;p&gt;BERT를 읽다 보니 NLP 관련 연구들에 관심이 더 생기는 것 같습니다. 시간이 되면 NLP 관련 논문들을 더 리뷰해보겠습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.04805ㅠ&quot;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/google-research/bert&quot;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="NLP" />
      

      
        <category term="NLP" />
      

      
        <summary type="html">원문 : Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” arXiv preprint arXiv:1810.04805 (2018).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">EfficientDet - Scalable and Efficient Object Detection</title>
      <link href="http://localhost:4000/EfficientDet" rel="alternate" type="text/html" title="EfficientDet - Scalable and Efficient Object Detection" />
      <published>2020-09-26T09:00:00+09:00</published>
      <updated>2020-09-26T09:00:00+09:00</updated>
      <id>http://localhost:4000/EfficientDet</id>
      <content type="html" xml:base="http://localhost:4000/EfficientDet">&lt;p&gt;원문 : &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf&quot;&gt;Tan, Mingxing, Ruoming Pang, and Quoc V. Le. “Efficientdet: Scalable and efficient object detection.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/main-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 Google Research에서 2020년 CVPR에 발표한 &lt;strong&gt;&lt;em&gt;Efficientdet: Scalable and efficient object detection&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 주어진 제약 조건(메모리 및 레이턴시) 아래에서 가장 효율적인 모델 구조를 찾아주는 &lt;a href=&quot;https://arxiv.org/pdf/1905.11946.pdf&quot;&gt;EfficientNet&lt;/a&gt;을 object detection 분야에 접목시킨 연구를 다루고 있으며, 기존의 여러 모델과 비교하였을 떄 성능 및 효율성 측면에서 아주 뛰어난 결과를 보여주었습니다.&lt;/p&gt;

&lt;p&gt;EfficientNet 구조를 backbone으로 하며, 그 논문에서 제안한 compound scaling을 활용해 여러 capacity의 모델을 제안했기 때문에 EfficientNet 논문을 읽고서 보시는 것을 추천드립니다. 그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;efficientdet-architecture&quot;&gt;EfficientDet Architecture&lt;/h2&gt;
&lt;p&gt;전통적으로 object detection을 수행하기 위한 모델 구조는 region-of-interest(ROI)를 제안하는 부분의 유무에 따라 one-stage와 two-stage로 구분합니다. 일반적으로 one-stage가 학습 및 추론 속도가 빠르고 간단한 구조를 가진다는 장점이 있다면, two-stage는 복잡한 구조를 가지지만 높은 정확성 및 AP 값을 가진다는 장점이 있습니다. 본문에서 소개한 EfficientDet은 ROI proposal 단계가 없으면서도 모델의 구조 최적화를 통해 높은 성능을 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/efficientdet-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;EfficientDet은 크게 CNN backbone, BiFPN, Class/Bbox prediction의 세 부분으로 구성되어 있습니다. 하나씩 살펴보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN backbone (EfficientNet)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CNN backbone으로는 SOTA의 이미지 분류 성능을 보여준 EfficientNet이 사용되었습니다. EfficientNet은 convolution 연산 기반 모델의 성능을 높이기 위해 모델의 width(channel 개수), depth(layer 개수), resolution(input 크기)를 동시에 늘리는 compound scaling을 적용하였습니다. 또한 세 가지 hyperparameter를 일정 비율로 키워 memory, latency 등의 자원 제약에 따라 모델의 capacity를 증가하였습니다. 결과적으로 EfficientNet은 같은 자원을 활용한 ResNet이나 Inception, DenseNet보다 훨씬 높은 성능을 보여주었습니다. EfficientDet은 이러한 EfficientNet을 backbone으로 하여 객체의 특징을 잘 나타내주는 여러 scale의 feature map을 추출하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Weighted Bidirectional FPN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;전통적인 CNN 기반의 one-stage 모델들은 하나의 feature map에서 object detection을 수행했기 때문에 다양한 크기의 객체를 검출하지 못하는 문제가 있었습니다. Feature Pyramid Network(FPN) 구조는 이를 해결하기 위해 등장하였습니다. FPN은 여러 layer에서 다양한 scale의 feature map을 추출하고, 각각으로부터 object detection을 수행하기 때문에 다양한 크기의 객체 검출이 가능했습니다. 또한 여러 resolution의 feature map을 더해주는 multi-scale feature fusion을 통해 성능을 높여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/fpn-architectures.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multi-scale feature fusion 구조는 FPN에서 처음 제안된 이후에 더 높은 성능을 실현하기 위해 다양하게 변화하였습니다. FPN이 top-down 방식으로 서로 다른 두 scale의 feature map을 더해주었다면, PANet에서는 기존 FPN 구조에 bottom-up path aggregation 구조를 추가하여 information flow를 다양화해주었습니다. 또한 NAS-FPN에서는 강화학습을 이용한 수 천 시간의 학습을 통해 성능과 효율성이 동시에 뛰어난 cross-scale connection 구조를 제안하였습니다. 
&lt;img src=&quot;assets/images/efficientdet/bifpn-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
EfficientDet은 multi-scale feature fusion을 위해 기존에 제안된 FPN류 구조들 중 높은 성능을 내는 PANet을 몇 가지 부분에서 수정하였습니다. 우선 feature-fusion 없이 하나의 input만을 받는 top-down의 맨 위, 맨 아래 node를 제거하였습니다. 또한 같은 level에 있는 input feature를 output에 더해주는 skip-connection을 추가하여, 적은 수의 계산량으로 feature fusion을 수행할 수 있도록 설계하였습니다. 마지막으로 구성한 bi-directional(top-down &amp;amp; bottom-up) 구조를 하나의 블럭으로 두고 여러 번 반복해 high-level feature fusion을 가능하게 만들었습니다. BiFPN 블럭의 반복 횟수는 compound scaling을 통해 최적화하였습니다.&lt;/p&gt;

&lt;p&gt;EfficientDet은 feature map를 더하는 과정도 개선하였습니다. 기존의 multi-scale feature fusion 과정에서는 서로 다른 여러 scale의 feature map을 단순하게 같은 비율로 더해주었습니다. 하지만 EfficientDet에서는 서로 다른 resolution을 가진 input으로부터 얻어진 feature map들이기 때문에 더해지는 과정에서 output에 기여하는 정도가 다를 수 있다고 판단하여, 이를 반영하여 weight를 곱해서 더해주는 방법을 제안하였습니다. 각 feature map의 weight는 다른 변수와 마찬가지로 학습을 통해서 최적화하였고, 계산의 효율성을 위해 feature 단위의 스칼라 값으로 정의하였습니다. 또한 feature fusion을 하는 과정에서 weight의 총합을 1로 normalize 해주었는데, latency cost를 줄이기 위해 softmax layer 대신 단순히 weight들을 더한 값에서 각 weight의 비율을 계산하는 fast-normalization fusion 방식을 활용하였습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 parameter 개수를 줄이기 위해 feature fusion 과정에서 발생하는 convolution 연산들을 모두 depthwise-convolution으로 대신하였고, batch normalization과 activation layer를 convolution 뒤에 추가하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Class/Bbox prediction layer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;각각 여러 겹의 Convolution layer을 이용해 class 정보와 bounding box 좌표를 예측하였습니다. 이 때 convolution layer의 개수는 compound scaling을 통해 최적화하였습니다.&lt;/p&gt;
&lt;h2 id=&quot;compound-scaling&quot;&gt;Compound Scaling&lt;/h2&gt;
&lt;p&gt;이 논문에서 여러 번 반복해서 강조하는 EfficientDet의 설계 목표는 자원의 제약(resource constraint)이 주어질 때 가장 효율적인 모델 구조를 찾는 것입니다. EfficientDet에서는 다양한 resource constraint 상황을 대비하기 위해, baseline 구조를 잡고, 구조를 구성하는 각 요소의 크기를 동시에 늘리는 compound scaling을 이용하여 모델의 크기를 키워나갔습니다. EfficientNet에서는 이러한 compound scaling 과정을 grid search를 통해서 수행하여 최적의 compound coefficient를 찾았지만, EfficientDet은 훨씬 더 큰 차원의 모델이기에 grid search 대신에 heuristic 기반의 scaling approach를 활용하였습니다. 결과적으로 얻은 8개의 EfficientDet 모델에 대한 scaling configuration은 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/scale-parameters.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;EfficientDet은 COCO2017의 object detection 데이터셋을 활용하여 각각 약 118,000장과 5,000장의 이미지를 이용해 학습 및 검증되었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loss function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;대부분의 object detection 모델과 마찬가지로, EfficientDet도 여러가지 loss function을 활용하였습니다. Class predictor에 대한 classification loss와 bbox predictor에 대한 L1 및 IOU loss의 합을 loss function으로 정의하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;그 외 details&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stochastic gradient descent(SGD) optimizer를 이용하였습니다. 또한 activation function으로는 ReLU를 대체하기 위해 Google에서 고안한 swish activation을 이용하였습니다. Sigmoid 함수에 일차함수를 곱한 형태입니다.&lt;/p&gt;

&lt;p&gt;각 모델은 32개의 TPU v3 코어를 활용하여 코어 당 4개의 이미지를 학습했고, 결과적으로 128의 batch size를 가집니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;EfficientDet에 약간의 구조를 추가하여 semantic segmentation을 학습해보았는데, ResNet 또는 Xception 기반의 DeepLabV3보다 성능이 뛰어났다고 합니다. 모델 구조는 D4 EfficientDet에 backbone의 level-2 feature를 받아 pixelwise classification하는 layer를 추가하였습니다.&lt;/p&gt;
&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/result-params.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 표는 다른 object detection 모델들과 비교한 EfficientDet의 성능을 나타내는데, 압도적인 결과를 보여주고 있습니다. EfficientDet은 다른 모델들과 비슷한 Parameter 개수나 CPU/GPU latency의 조건에서 뛰어난 AP성능을 보여주었습니다. 실험을 할 때, 다른 네트워크에 대해서는 공정한 비교를 위해 convolution 연산을 모두 depthwise로 바꿔주었다고 합니다. 모델의 크기를 늘리는 데에는 한계가 있기 떄문에, 주어진 모델 capacity에서 뛰어난 효율성을 보여주어야 한다는 EfficientDet의 철학이 제대로 반영된 결과라고 할 수 있을 것 같습니다. 유사한 크기의 다른 네트워크에 대한 스펙을 아래 표로 정리하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/result-ap.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문에서는 여러가지 ablation study를 진행하였는데, 우선 EfficientDet 내의 EfficientNet과 BiFPN을 각각 ResNet과 FPN으로 바꾼 모델과 성능을 비교해보았습니다. 각각을 다른 구조로 치환한 두 실험에서 모두 AP가 3~4정도 낮아진 걸로 보아 두 구조 모두 성능에 중요한 역할을 하는 것을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/ablation-backbone-fpn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 BiFPN 구조 우수성을 보여주기 위해, EfficientDet을 다른 여러 multi-scale feature fusion 방식으로 바꾸어 결과를 비교하였습니다. 마찬가지로 공정한 비교를 위해 FPN 블럭을 반복하여 parameter 개수를 비슷하게 맞춰주었습니다. 결과에서 BiFPN은 기존의 방법 중 가장 뛰어난 성능을 내던 PANet의 AP를 따라잡거나 뛰어 넘는 모습을 보여주었으며, 훨씬 작은 수의 메모리와 FLOPs를 이용하며 효율성 측면에서 압도적인 모습을 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/ablation-fpn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;학습이 진행됨에 따라 multi-scale feature fusion에 사용하는 weight 값의 변화 과정을 softmax와 fast normalization fusion 각각을 사용했을 때에 대해 그려보았습니다. 두 실험에서 모두 모델이 데이터에 따라 어떤 resolution의 input을 강조해야하는지를 잘 학습하고 있는 것 같았고, 결과적으로 유사한 weight 값으로 수렴하였음을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/ablation-weight1.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;assets/images/efficientdet/ablation-weight2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;EfficientDet 구조에서 각 구성 요소의 hyperparameter를 동시에 높여서 학습시키는 compound scaling 도 각각의 hyperparameter를 하나씩 키워서 학습시킬 때보다 훨씬 더 높은 AP 결과를 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/efficientdet/result-compound-scaling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;EfficientDet은 발표일 기준 COCO object detection task에 대해 SOTA의 성능 및 효율성을 보여준 획기적인 논문입니다. 실제로 공식 github 코드를 받아 자율주행 및 문자인식 데이터셋으로 fine-tuning하여 사용해보았는데, 다른 모델들과 비교하여 꽤나 뛰어난 성능을 보여주었습니다. 또한 각자의 자원 제약 조건(GPU 메모리 크기, 최대 추론 시간)에 맞는 최적의 모델을 선택할 수 있기 때문에 제가 사용하고 있는 GPU 메모리에 맞는 모델을 능동적으로 결정할 수 있어 효용성이 크다고 느꼈습니다.
다만 Compound scaling을 적용함에 있어서 일정한 비율로 scale-up을 해줄 때 좀 더 최적화된 heuristic 또는 규칙이 있을 수도 있겠다는 생각이 듭니다. 어떤 모델이 EfficientDet의 성능 또는 효율성을 뛰어넘을 수 있을지 기대하며 이번 리뷰는 마치겠습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.11946.pdf&quot;&gt;https://arxiv.org/pdf/1905.11946.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf&quot;&gt;https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/google/automl/tree/master/efficientdet&quot;&gt;https://github.com/google/automl/tree/master/efficientdet&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="Object-Detection" />
      

      
        <category term="Object-Detection" />
      

      
        <summary type="html">원문 : Tan, Mingxing, Ruoming Pang, and Quoc V. Le. “Efficientdet: Scalable and efficient object detection.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">End-to-End Object Detection with Transformers (DETR)</title>
      <link href="http://localhost:4000/DETR" rel="alternate" type="text/html" title="End-to-End Object Detection with Transformers (DETR)" />
      <published>2020-09-14T20:00:00+09:00</published>
      <updated>2020-09-14T20:00:00+09:00</updated>
      <id>http://localhost:4000/DETR</id>
      <content type="html" xml:base="http://localhost:4000/DETR">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot;&gt;Carion, Nicolas, et al. “End-to-End Object Detection with Transformers.” arXiv preprint arXiv:2005.12872 (2020). &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/detr/detr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본 글은 Facebook AI에서 2020년 arxiv에 발표한 &lt;strong&gt;&lt;em&gt;End-to-End Object Detection with Transformers (DETR)&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다. 리뷰를 읽기 전 Transformer가 익숙하지 않으신 분들은 Attention is All You Need 관련 리뷰를 보고 나서 읽으시는 것을 권장드립니다.&lt;/p&gt;

&lt;p&gt;객체 탐지(Object detection)에 관한 연구는 인공지능 연구가 시작된 이래로 지속적으로 발전해왔습니다. Regional-CNN, Faster R-CNN, Single Shot Detector, YOLO 등 많은 방법론들이 제안되었으며 학습의 효율성과 성능은 점점 향상되고 있습니다. 객체 탐지 문제의 목표는 input 이미지에서 찾을 수 있는 모든 객체들에 대한 class와 bounding box 정보를 얻는 것입니다. 이는 단순히 이미지를 분류(classification)하는 문제에 비해 훨씬 더 복잡하기 때문에, 기존의 여러 방법론들에서는 전/후처리 과정을 추가하거나 customize된 모델 구조를 활용하는 방법으로 문제를 해결하였습니다.&lt;/p&gt;

&lt;p&gt;본문에서 소개한 DETR은 Transformer 구조와 Bipartite matching 기법을 활용하여 기존의 방법론들과 달리 end-to-end로 결과를 도출하였습니다. 기존에 꼭 필요했던 non-maximum suppression(NMS)나 anchor generation 과정이 생략되어 간단하고 빠르게 결과를 얻을 수 있습니다. 또한 Faster R-CNN과 비교했을 때 뒤지지 않는 성능을 보여주었습니다. 그럼 DETR에 대해서 지금부터 파헤쳐 보겠습니다.&lt;/p&gt;
&lt;h2 id=&quot;set-prediction-with-bipartite-matching-loss&quot;&gt;Set prediction with bipartite matching loss&lt;/h2&gt;
&lt;p&gt;DETR은 output으로 정해진 개수(N)의 객체에 대한 class와 bounding box를 도출합니다. 이 때 output의 개수(본문에서 N=100)는 일반적인 이미지에서 존재하는 객체의 개수보다 충분히 큰 값으로 사전에 설정해줍니다. Class 집합에는 사전에 정의한 class 외에 no-object class를 추가하여, output 중 객체가 없는 경우에 no-object class에 배정하도록 합니다.&lt;/p&gt;

&lt;p&gt;이제 학습을 위해서 결과로 나온 N쌍의 class와 bounding box를 target의 class, bounding box와 비교하여 손실 함수를 계산해야 합니다. 이 때 N개의 output에 대한 (class, bounding box)가  각각 target의 어떤 (class, bounding box)와 매칭되어 손실 함수를 계산할 것인지 결정해야합니다. 이를 이분 매칭(bipartite matching)이라고 부릅니다. Grid search로 모든 경우에 대한 matching loss를 계산하여 비교하게 되면 총 &lt;em&gt;O(n!*n)&lt;/em&gt; 의 complexity를 가지지만 본문에서는 Hungarian 알고리즘을 활용하여 complexity를 &lt;em&gt;O(n&lt;sup&gt;3&lt;/sup&gt;)&lt;/em&gt; 로 향상시켰습니다.&lt;/p&gt;

&lt;p&gt;이렇게 이분 매칭을 활용하여 유일한 최적의 output-target 조합을 찾으면, output에 대한 loss를 계산하여 back-propagation에 활용할 수 있습니다. 만약 output을 도출하는 네트워크가 다른 추가적인 처리 모듈 없이 one-stage로 구성되어 있다면, end-to-end로 direct하게 객체 탐지 결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 하나의 target object에 대해 여러 output을 생성해버리는 near-duplicate prediction 문제도 이분 매칭을 활용하게 되면 결국 하나의 output-target 쌍으로 매칭이 되기 때문에, 학습 과정에서 어느 정도 해결된다고 할 수 있습니다. 이는 이후에 살펴볼 decoder 단의 attention layer와 함께 DETR이 NMS 모듈 없이도 잘 동작할 수 있는 근거가 됩니다.&lt;/p&gt;
&lt;h2 id=&quot;detr-architecture&quot;&gt;DETR Architecture&lt;/h2&gt;
&lt;p&gt;DETR의 구조는 크게 CNN Backbone, Transformer(encoder-decoder), Feed-forward의 세 부분으로 나눌 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/detr/detr-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN Backbone&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Backbone 구조는 3-color channel image를 받아 feature extraction을 하는 CNN 구조를 사용했습니다. 본문에서는 ImageNet을 활용하여 학습한 ResNet50이나 ResNet101을 이용하였습니다. CNN backbone을 거쳐서 나온 feature map(본문에서는 channels=2048, height과 width는 input의 1/32)은 1x1 convolution 과정을 통해 channel 차원을 감소한 뒤에 형변환을 거쳐서 Transformer로 들어갔습니다.&lt;/p&gt;

&lt;p&gt;Transformer는 input 사이의 순서를 고려하지 않고 병렬적으로 attention score를 연산합니다. 따라서 일반적으로 sequential 데이터를 넣어줄 때는 순서 정보를 포함한 positional encoding 벡터를 더해서 넣어주는데요, DETR에서도 Transformer encoder로 들어가기 전에 pixel들 사이의 순서 정보(여기서는 pixel의 위치 정보라고 할 수 있겠네요)를 더해주기 위하여 spatial positional encoding 벡터를 더해주었습니다. Positional encoding은 Vanilla Transformer에서 사용했던 1D sinosoidal encoding을 2D로 일반화하여 사용하였으며 ablation study 과정에서 sinosoidal 대신 linear layer을 학습한 learned positional encoding을 사용하기도 하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transformer (Encoder-Decoder)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transformer 구조는 2017년 발표된 Attention is All You Need 논문에서 제안된 것으로, sequential한 데이터 간의 연관성을 병렬적으로 파악하여 자연어 처리 및 음성 처리 등의 분야에서 활발히 활용되고 있습니다. DETR은 다차원 행렬의 형태를 띈 이미지를 sequential한 형태로 변경하여 Transformer에 넣어주면 pixel 간의 연관성 및 유사도(compatibility)를 거시적으로 파악할 수 있을 것이라는 관점에서 고안되었습니다.&lt;/p&gt;

&lt;p&gt;Encoder input으로는 sequence의 pixel값을 embedding 벡터로 변환하여 positional encoding을 더해준 값을 사용하였습니다. Encoder는 Multi-haed self-attention 메커니즘을 활용하여 전체 이미지 내에서 pixel들 사이의 관계 정보를 추출하였고, 이를 attention score에 반영하게끔 학습하였습니다. 실제로 학습된 encoder에서 어떤 input 이미지에 대한 encoder의 attention score를 시각화했을 때, 아래 그림과 같이 같은 객체 내에 포함되는 pixel들 사이의 attention score가 높은 경향을 보였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/detr/encoder-attention-score.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Decoder 부분도 Vanilla Transformer의 decoder 블럭과 같이 multi-head self-attention과 encoder-decoder attention을 활용하여 구현하였습니다. 다만 기존처럼 sequence output을 auto-regressive하게 하나씩 얻어내는 방식이 아니라 parallel하게 N개의 output을 한번에 얻는 방식을 활용하였습니다. 이것에 대해 조금 설명하자면 decoder가 해주는 multi-head self-attention 연산은 input의 순서와 무관하게 같은 결과를 도출하는 permutation-invariant한 연산입니다. 따라서 N개의 input이 서로 다른 객체 탐지 결과(class 및 bounding box)를 도출하려면 input 자체가 서로 다른 embedding 벡터로 구성되어야 합니다. DETR에서는 이 N개의 decoder input을 &lt;em&gt;object query&lt;/em&gt; 라고 부릅니다. Object query 값은 positional encoding과 마찬가지로 1D sinosoidal encoding 벡터를 이용하거나 linear layer을 학습시켜서 나온 output 벡터를 이용하였습니다.&lt;/p&gt;

&lt;p&gt;이러한 non-autoregressive한 decoder 구조는 autoregressive한 구조에 비해 inference에 대한 시간적-계산적 효율성이 높습니다. 기존의  Autoregressive한 decoding 과정은 각 과정마다 하나의 output만을 얻을 수 있기에 N=100인 경우 100번의 inference를 거쳐야 하나의 이미지에 대한 객체 탐색 결과를 얻을 수 있습니다. 반면에 DETR에서 활용한 구조는 100개의 output을 병렬적으로 decoding 할 수 있기 때문에 빠르고 효율적입니다.&lt;/p&gt;

&lt;p&gt;Encoder의 역할이 이미지의 전체 pixel들 중 같은 객체의 pixel들 사이에 높은 attention score를 부여하여 객체의 존재 여부와 형태를 파악했다면, decoder의 기본적인 역할은 그 객체가 어떤 객체인지를 파악하는 데에 있습니다. 아래의 그림은 decoder의 attention score(encoder-decoder attention)를 시각화한 그림인데요, 탐지된 객체에서 class와 bounding box 정보를 추출해내기 위해 객체의 머리나 다리 같은 가장자리 부분에 대한 attention score가 크게 나타난 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/detr/decoder-attention-score.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 decoder는 한 객체에 대해 생성된 여러 개의 중복된 prediction을 제거해주는 역할도 하고 있습니다. Decoder의 self-attention 메커니즘을 통해 모든 object query가 서로 pair-wise relation에 대한 연산을 하면서 같은 예측값을 제거하는 방향으로 학습이 되는 형태입니다.&lt;/p&gt;

&lt;p&gt;이를 증명하기 위해 본문에서는 decoder의 output에 non-max suppression(NMS) 처리를 하여 기존의 output의 average precision(AP) 값을 비교해보았습니다. Input과 가까운 몇 단계의 decoder block에서는 NMS 연산을 통해 향상된 결과를 얻을 수 있었지만, decoder layer가 쌓이면 쌓일수록 그 정도가 줄어들다가 마지막 layer에서는 NMS가 서로 다른 object에 대한 prediction을 제거하여 오히려 성능을 떨어트렸습니다. 이는 decoder layer를 거치면 거칠수록 near-duplicate prediction들이 자연스럽게 제거되었고 결국 NMS 없이도 좋은 결과를 얻을 수 있었던 것으로 해석할 수 있습니다.
&lt;img src=&quot;assets/images/detr/decoder-nms.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Decoder의 학습을 돕기 위해 각 decoder 블럭에 class와 bounding box를 예측하는 feed-forward 네트워크를 추가하여 loss를 계산하였습니다. Decoder에 연결된 feed-forward 네트워크는 서로 weight parameter를 공유하고, 계산한 loss function은 역전파를 통해 decoder의 학습 성능을 높히게 됩니다. 논문에서는 이를 auxiliary loss라고 부르며 F1/DICE loss를 사용하였다고 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feed-forward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Feed-forward 네트워크는 N개의 decoder output을 독립적인 input으로 받아서 각각의 class와 bounding box 값을 예측합니다. 네트워크는 3-layer의 perceptron과 ReLu activation function으로 구성되었습니다. Class output은 softmax layer을 통과하여 각 class에 대한 확률값을 나타내고, bounding box output은 이미지의 원래 크기에 따라 normalize된 box의 중심 좌표와 너비, 높이에 대한 값을 나타냅니다. Class의 결과로는 기존에 정의한 class 외에 no-object라는 class를 추가하여 예측된 영역(slot)에 객체가 없는 경우를 표현하였으며, 다른 몇몇 모델에서 사용하는 background class와 유사한 역할을 합니다.&lt;/p&gt;
&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;p&gt;지금부터는 DETR의 학습에 대해서 설명하겠습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DETR은 COCO 2017의 object detection 및 panoptic segmentation 데이터셋을 이용하여 학습하였습니다. 약 120000장과 5000장의 이미지를 학습과 검증에 활용하였습니다. 학습 데이터는 평균 7개, 최대 63개의 객체를 포함하고 있었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Matching cost&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DETR은 모델을 통해 예측한 100개의 output과 이미지에 실제로 존재하는 객체들을 매칭해서, 최적의 조합을 찾은 후에 손실 함수를 계산합니다. 이 때, 매칭에 사용되는 cost는 class와 bounding box의 유사도를 잘 나타낼 수 있는 값으로 설정해야 합니다. 본문에서는 class에 대한 확률값과 bounding box에 대한 L1 loss, IOU loss를 이용해 matching cost를 정의하였습니다.
&lt;img src=&quot;assets/images/detr/matching-cost.png&quot; alt=&quot;&quot; /&gt;
이후 Hungarian 알고리즘을 통해 최적의 cost 값을 가지는 output-target 조합을 정하고 나서 손실 함수를 계산하게 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Loss function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;손실 함수는 matching cost와 비슷하게 class에 대한 negative log-likelihood와 bounding box에 대한 L1 loss, IOU loss를 1:5:2의 비율로 가중합하여 정의하였습니다. 
&lt;img src=&quot;assets/images/detr/loss-function.png&quot; alt=&quot;&quot; /&gt;
손실 함수로 IOU loss를 사용하는 이유는, DETR이 작은 객체 검출에 대한 성능이 부족하기 때문에 이를 보완하기 위해서입니다. DETR의 Transformer encoder는 모든 pixel에 대한 global relation을 파악하기 때문에 큰 객체에 대한 탐지 성능이 높습니다. 하지만 DETR은 CNN backbone에서 하나의 feature map만을 추출하여 객체 검출에 활용하기 때문에, Feature Pyramid Network(FPN)와 같이 여러 scale의 feature map을 추출하여 객체를 검출하는 밤식과 비교해서 작은 객체에 대한 탐지 성능이 떨어집니다. L1 loss 만을 이용하면, 작은 객체는 손실 함수에 작게 기여할 수밖에 없습니다. 하지만 IOU 값은 작은 객체든 큰 객체든 면적이 겹치는 비율에 따라 결정되기 때문에, IOU loss를 이용하면 작은 객체도 손실 함수에 크게 기여할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;손실 함수는 각 이미지가 포함하는 객체의 개수에 독립적인 값이어야 하기 때문에, 항상 마지막에 객체 개수로 나눠주어 normalize 하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;그 외 Details&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Weight parameter의 초기값은 Xavier initialization을 활용하여 설정해주었고, Learning rate은 10&lt;sup&gt;-4&lt;/sup&gt;를 이용하여 학습하다가 200 epoch가 지나면 10&lt;sup&gt;-5&lt;/sup&gt;로 바꿔주었습니다.&lt;/p&gt;

&lt;p&gt;최적화는 Adam W optimizer을 사용하였으며, 일반화를 위해 학습 과정에서 dropout 기법(p=0.1)을 사용하였습니다.&lt;/p&gt;

&lt;p&gt;데이터 증강을 위해 random resize, random crop, random horizontal flip을 통한 scale augmentation를 진행하였습니다.&lt;/p&gt;

&lt;p&gt;Baseline 모델은 16개의 V100 GPU로 3일 동안 300 epoch를 학습하였습니다. (학습 시간이 아주 길게 필요합니다.)&lt;/p&gt;
&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/detr/result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DETR은 Faster R-CNN과 비교했을 때 구조적으로 훨씬 간단하고 빠른 추론 속도를 보이면서 높은 AP score의 객체 탐지 결과를 도출하였습니다.&lt;/p&gt;

&lt;p&gt;다만 앞서 말씀드렸듯 single-scale feature map으로부터 결과를 도출하기 때문에, 작은 객체에 대한 탐지 능력이 큰 객체에 비해 떨어지는 경향을 보였습니다.
&lt;img src=&quot;assets/images/detr/missing.png&quot; alt=&quot;&quot; /&gt;
또한 60개 이상의 많은 객체를 포함한 이미지에 대해서는 잘 동작하지 못하는 모습을 보여주었습니다.
평균적으로 학습한 이미지들이 7개 정도의 객체를 가지고 있었기 때문에, 60개 이상의 객체를 가진 이미지는 out-of-distribution 데이터이고 그에 따라 네트워크가 잘 반응하지 못하는 것 같습니다.&lt;/p&gt;
&lt;h2 id=&quot;panoptic-segmentation&quot;&gt;Panoptic segmentation&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/detr/panoptic-result.png&quot; alt=&quot;&quot; /&gt;
Panoptic segmentation이라는 개념은 semantic segmentation과 instance segmentation을 융합한 개념으로, 배경 관련 객체인 `stuff`와 배경이 아닌 객체인 `thing`을 instance 단위로 모두 구별해주는 segmentation입니다.&lt;/p&gt;

&lt;p&gt;Faster R-CNN에 segmentation head를 추가하여 Mask R-CNN을 구현했듯, DETR의 decoder output에 head를 추가하여 panoptic segmentation 결과를 얻을 수 있습니다. Segmentation head의 구조는 아래와 같습니다.
&lt;img src=&quot;assets/images/detr/panoptic-head.png&quot; alt=&quot;&quot; /&gt;
Decoder의 output으로 나온 각 객체들과 인코딩 된 원본 이미지를 multi-head attention 메커니즘에 통과시키면, 원본 이미지에서 각 객체와 연관성이 높은 pixel을 찾을 수 있습니다. 이렇게 나온 output을 FPN 형태의 CNN에 넣어주어 각 객체에 대한 mask image를 얻고, pixel별로 argmax 연산을 하여, 최종적으로 전체 이미지에 대한 segmentation map을 얻었습니다.&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;DETR은 이분 매칭(bipartite matching)을 통한 최적의 손실 함수 계산과 Transformer 구조를 통해 pixel 사이의 관계를 파악하여 객체 탐지 태스크를 end-to-end로 학습하였습니다. 기존의 Transformer와 달리 non-autoregressive하게 decoding하기 때문에 inference cost를 최소화할 수 있었습니다. 또한 decoder output에 몇 가지 layer를 추가하여 panoptic segmentation 태스크를 수행하는 등의 확장성도 보여주었습니다. 최근에 발표된 EfficientDet이나 DeTectORs 만큼의 성능을 보여주지는 못했지만, 복잡한 모듈이나 Custom layer 없이 간단하게 구현할 수 있고 end-to-end로 직접 결과를 얻을 수 있다는 점에서 큰 가치가 있는 논문이라고 생각합니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot;&gt;https://arxiv.org/abs/2005.12872&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/detr&quot;&gt;https://github.com/facebookresearch/detr&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="Object-Detection" />
      

      
        <category term="Object-Detection" />
      

      
        <summary type="html">원문 : Carion, Nicolas, et al. “End-to-End Object Detection with Transformers.” arXiv preprint arXiv:2005.12872 (2020).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Attention is All You Need</title>
      <link href="http://localhost:4000/attention-is-all-you-need" rel="alternate" type="text/html" title="Attention is All You Need" />
      <published>2020-09-07T20:00:00+09:00</published>
      <updated>2020-09-07T20:00:00+09:00</updated>
      <id>http://localhost:4000/attention-is-all-you-need</id>
      <content type="html" xml:base="http://localhost:4000/attention-is-all-you-need">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;본 글은 Google Brain에서 2017 NIPS에 발표한 &lt;strong&gt;&lt;em&gt;Attention is All You Need&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰이며 동시에 &lt;strong&gt;제 첫 논문 리뷰글&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;자연어 처리(NLP)등의 분야에서는 순서를 가진 sequence형 데이터를 학습하기 위해 그동안 많은 모델을 제시했습니다. 대표적으로 encoder와 decoder 방식을 활용한 RNN 또는 CNN이 있는데, 이들은 순서가 있는 데이터의 특성상 순차적으로 input을 학습하고 output을 도출할 수밖에 없었습니다.
&lt;img src=&quot;assets/images/attention-is-all-you-need/1-seqtoseq.png&quot; alt=&quot;1-seqtoseq&quot; /&gt;
순차적으로 데이터를 처리하는 방식은 근본적으로 병렬화(parallelization)를 불가능하게 만들어 계산적 효율성을 제한할뿐만 아니라 긴 문장이 들어왔을 때 여러 layer를 거치면서 앞의 input에 대한 정보가 희미해지는 등의 문제가 있었습니다. 이를 해결하기 위해 ‘output을 도출할 때 어떤 input에 집중해야 하는지’에 대한 정보를 추가하는 attention 메커니즘을 추가하기도 하였지만(seq2seq 모델) 여전히 학습 속도 저하에 대한 한계가 존재했습니다.
&lt;img src=&quot;assets/images/attention-is-all-you-need/2-transformer.png&quot; alt=&quot;2-transformer&quot; /&gt;
위 논문에서 제안한 Transformer는 &lt;strong&gt;병렬화&lt;/strong&gt;를 가능하게 하여 학습의 효율성과 모델의 성능을 기존보다 월등히 끌어올렸다는 점에서 NLP 및 다양한 분야에 많은 기여를 했다고 생각합니다. 또한 순차적인 과정 없이 attention 메커니즘만을 이용하였기 때문에 input과 output 사이에 포괄적인 상관관계(global dependency)를 이끌어냈습니다.&lt;/p&gt;
&lt;h2 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h2&gt;
&lt;p&gt;Transformer의 핵심 개념은 attention 메커니즘, 그 중에서도 self-attention 메커니즘이라고 할 수 있습니다.
Transformer는 attention 정보를 얻기 위해 각 sequential input에 대한 &lt;strong&gt;&lt;em&gt;Query, Key, Value&lt;/em&gt;&lt;/strong&gt; 값을 생성하고, Query와 Key 값을 매칭하여 input (NLP에서는 단어, vision에서는 pixel) 사이의 연관성 또는 유사도(compatibility)를 나타내는 attention score를 계산합니다.
Attention score는 해당 input과 다른 sequence inputs 사이의 의미론적 연관성을 나타내는 값이기 때문에, 이를 기존의 sequence input에 곱하면 전체 sequence 내에서 각 input이 갖는 의미를 강조한 새로운 sequence를 얻을 수 있습니다. 
&lt;img src=&quot;assets/images/attention-is-all-you-need/3-attention-mechanism.png&quot; alt=&quot;3-attention-mechanism&quot; /&gt;
Attention layer에 들어가기 전에 모든 input은 그 특성을 가장 잘 나타낼 수 있는 다차원의 embedding 벡터로 변환됩니다. Embedding 벡터는 d&lt;sub&gt;model&lt;/sub&gt; 의 차원을 가지게 되고, 이는 각 encoder 또는 decoder 블럭을 지나도 동일하게 유지됩니다. 위 그림에서 d&lt;sub&gt;model&lt;/sub&gt;=4로 표현되었지만, 실제 논문에서는 d&lt;sub&gt;model&lt;/sub&gt;=512를 사용하였습니다. Input sequence를 embedding 벡터로 변환하는 과정은 학습시킨 Linear layer을 통과함으로써 이루어집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/4-attention-mechanism.png&quot; alt=&quot;4-attention-mechanism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Self-attention 과정에서 embedding 벡터는 W&lt;sup&gt;Q&lt;/sup&gt;, W&lt;sup&gt;K&lt;/sup&gt;, W&lt;sup&gt;V&lt;/sup&gt; 의 세 행렬을 통해 각각 query, key, value 벡터로 변환됩니다. Self-attention 과정이 일반적인 attention 메커니즘과 다른 점은, query, key, value 벡터를 하나의 동일한 벡터로부터 생성한다는 점입니다. 변환 행렬의 크기를 (d&lt;sub&gt;model&lt;/sub&gt; x d&lt;sub&gt;query&lt;/sub&gt;, d&lt;sub&gt;model&lt;/sub&gt; x d&lt;sub&gt;key&lt;/sub&gt;, d&lt;sub&gt;model&lt;/sub&gt; x d&lt;sub&gt;value&lt;/sub&gt;)라고 한다면 output 벡터의 차원은 d&lt;sub&gt;query&lt;/sub&gt;, d&lt;sub&gt;key&lt;/sub&gt;, d&lt;sub&gt;value&lt;/sub&gt;가 됩니다. (본문에서는 d&lt;sub&gt;query&lt;/sub&gt; = d&lt;sub&gt;key&lt;/sub&gt; = d&lt;sub&gt;value&lt;/sub&gt; = 64로 설정하였습니다.)&lt;/p&gt;

&lt;p&gt;여기서 W&lt;sup&gt;Q&lt;/sup&gt;, W&lt;sup&gt;K&lt;/sup&gt;, W&lt;sup&gt;V&lt;/sup&gt;는 우리가 학습시켜야 하는 Transformer의 parameter입니다. 세 행렬 W&lt;sup&gt;Q&lt;/sup&gt;, W&lt;sup&gt;K&lt;/sup&gt;, W&lt;sup&gt;V&lt;/sup&gt;에 적당한 값을 넣어줘야 input 사이의 관계를 가장 잘 나타내는 attention score를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/5-attention-mechanism.png&quot; alt=&quot;5-attention-mechanism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 query 벡터와 모든 sequence input의 key 벡터 사이의 연관성을 계산해야 합니다. Self-attention에서 Query와 Key 행렬의 크기는 같고 그 결과로 나온 query와 key 벡터의 차원도 같기 때문에, 내적(inner-product)을 통해 두 벡터의 유사도를 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 내적을 통해 계산된 attention score를 d&lt;sub&gt;key&lt;/sub&gt;의 제곱근으로 나눠주었습니다. 이는 embedding 또는 key 벡터의 차원이 증가함에 따라 score 값이 커지는 현상을 방지하기 위해서이며, 역전파 과정에서 발생하는 gradient의 크기를 안정화하는 효과가 있습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 softmax 연산을 통해 합이 1인 형태로 결과값을 normalize하여 최종적인 attention score을 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/6-attention-mechanism.png&quot; alt=&quot;6-attention-mechanism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 각 value 벡터에 attention score를 곱해서 최종적인 attention layer의 output을 얻게됩니다. 이는 각 벡터들 사이의 연관성(또는 유사도)를 가중합한 value 벡터입니다. 이것으로 Self-attention 메커니즘이 마무리되고, 결과로 얻은 벡터는 feed-forward network를 거쳐 다음 encoder block의 input 역할을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/7-attention-mechanism.png&quot; alt=&quot;7-attention-mechanism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 모든 과정은 행렬 연산으로 아주 빠르게 처리할 수 있습니다. 행렬 연산이라는 attention 메커니즘의 특성 덕분에 Transformer에서는 기존 RNN 기반 네트워크에서 이루지 못한 &lt;strong&gt;&lt;em&gt;sequential 데이터의 병렬화&lt;/em&gt;&lt;/strong&gt; 가 가능해집니다.&lt;/p&gt;
&lt;h2 id=&quot;multi-head-attention&quot;&gt;Multi-head attention&lt;/h2&gt;
&lt;p&gt;Transformer에서는 attention의 성능을 높이기 위해 &lt;strong&gt;&lt;em&gt;Multi-head attention&lt;/em&gt;&lt;/strong&gt;을 도입하였습니다. Multi-head attention이란, weight를 공유하지 않는 N(number of heads)개의 Query, Key, Value 행렬을 병렬적으로 두어 서로 다른 attention score을 계산할 수 있도록 한 방식입니다. (본문에서 N=8 입니다.)
Multi-head attention을 이용하는 이유는, &lt;strong&gt;다양한 관점에서 단어(또는 pixel) 사이의 관계를 파악하기 위해서&lt;/strong&gt;입니다. 아래 아주 유명한 예시가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/8-multi-head-attention.png&quot; alt=&quot;8-multi-head-attention&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;The animal didn't cross the street because it was too tired&lt;/blockquote&gt;
&lt;p&gt;위 문장에서, `it` 이라는 단어는 `animal`을 지칭할 수도 있고 `street`을 지칭할 수도 있습니다.
우리는 `it`이 `animal`을 지칭하는 것을 알고 있지만, 만약 학습이 잘못되어 `street`의 attention score가 커지게 된다면 좋은 translation 성능을 내지 못할 것입니다.
이러한 오류를 방지하기 위해 Transformer 에서는 여러 개의 &lt;strong&gt;&lt;em&gt;Attention head&lt;/em&gt;&lt;/strong&gt; 를 병렬적으로 두어 각 module에서 서로 다른 관점의 attention score을 가지게 하였습니다.
서로 다른 관점으로 input sequence를 파악하길 바라는 의도로 추가하였으니, weight sharing을 하지 않는 것은 당연하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/9-multi-head-attention.png&quot; alt=&quot;9-multi-head-attention&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multi-head attention을 거치게 되면 N개의 output을 얻게 됩니다. 다음 layer를 통과하기 전에 N개의 output을 하나로 합쳐야 하는데 본 논문에서는 단순히 concatenate 연산을 통해 해결하였습니다. 이후 feed-forward network를 거쳐 input과 같은 차원을 가지는 벡터로 변환됩니다.
Multi-head attention을 이용하더라도 일반적으로 weight parameter 개수는 일정하게 유지되는데, N개의 head를 둠으로써 행렬 개수가 N개로 늘어나는 만큼 행렬의 차원이 1/N배로 줄어들기 때문입니다.&lt;/p&gt;
&lt;h2 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/10-positional-encoding.png&quot; alt=&quot;10-positional-encoding&quot; /&gt;
그렇다면 Transformer는 순차적인 구조가 아닌데 어떻게 sequential 데이터의 위치 정보를 전달할 수 있을까요? 본문에서는 그에 대한 답으로 &lt;strong&gt;&lt;em&gt;positional encoding&lt;/em&gt;&lt;/strong&gt;을 제안합니다.&lt;/p&gt;

&lt;p&gt;Positional encoding이란 embedding 벡터가 encoder 또는 decoder에 들어가기 전에 &lt;strong&gt;위치정보를 지닌 벡터를 더해주는 과정&lt;/strong&gt;을 의미합니다. 만약 position encoding을 가하지 않는다면, 문장 맨 앞에 나온 `Animal` 이라는 단어와 문장 맨 뒤에 나온 `Animal`이 같은 값을 가진 채로 네트워크에 입력되게 됩니다. 하나는 주어 역할을 하고, 다른 하나는 목적어 역할을 할 수도 있는데 말입니다.&lt;/p&gt;

&lt;p&gt;Transformer에서는 embedding 벡터에 위치에 따라 값이 달라지는 값을 더해줌으로써 sequence의 순서를 고려하였습니다.
Positional encoding 벡터에는 여러 종류가 있을 수 있지만, 본문에서는 sinosoidal positional encoding 형식을 활용하였습니다.
&lt;img src=&quot;assets/images/attention-is-all-you-need/equation1.gif&quot; alt=&quot;equation-1&quot; /&gt;
d&lt;sub&gt;model&lt;/sub&gt;=512인 경우에 대한 positional encoding 값은 colormap 형식으로 아래와 같이 나타낼 수 있습니다.
20개(y-axis)의 input에 대해 512차원(x-axis)의 연속적이며 상대적인 정보를 잘 나타내고 있습니다.
&lt;img src=&quot;assets/images/attention-is-all-you-need/11-positional-encoding.png&quot; alt=&quot;11-positional-encoding&quot; /&gt;
이외에 positional encoding 벡터를 학습 과정에서 결정하는 Learned positional encoding 방법이 있습니다. 다만 Learned positional encoding 방법은, 이전에 학습을 진행한 적 없던 긴 sequence의 input이 들어왔을 때, 적절한 positional encoding을 가하지 못하게 되는 한계점이 있습니다.&lt;/p&gt;
&lt;h2 id=&quot;transformer-architecture&quot;&gt;Transformer Architecture&lt;/h2&gt;
&lt;p&gt;이제 Transformer가 어떤 구조를 가지고 있으며 각 구조는 어떤 역할을 하는지 살펴보겠습니다.
&lt;img src=&quot;assets/images/attention-is-all-you-need/12-transformer.png&quot; alt=&quot;12-transformer&quot; /&gt;
Transformer는 여러 개의 encoder 블럭과 decoder 블럭으로 구성되어 있습니다. 본문에서는 각각 6개의 블럭을 사용하였습니다. Input sequence가 같은 구조의 encoder/decoder 블럭을 연속해서 통과해야 하므로, 각 블럭의 input과 output은 항상 같은 크기를 가져야 합니다. 이를 위해 보통 마지막 layer에 feed-forward network를 배치하여 차원을 맞춰주는 역할을 합니다.&lt;/p&gt;

&lt;h4 id=&quot;1-encoder-blocks&quot;&gt;1. Encoder blocks&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/13-encoder.png&quot; alt=&quot;13-encoder&quot; /&gt;
하나의 Encoder block은 두 부분의 sub-layer로 구분할 수 있습니다.
첫 번째 layer은 Transformer의 핵심 개념인 self-attention을 수행하는 layer입니다.
Self-attention을 거쳐 얻은 output은 두 번째 layer인 feed-forward layer를 통과합니다.&lt;/p&gt;

&lt;p&gt;각 sub-layer의 output은 residual connection과 layer normalization 과정을 거치게 됩니다. Residual connection은 여러 encoder 블럭을 지나게 되면서 positional encoding 정보가 희미해지는 현상을 방지하기 위해서 추가되었습니다. 또한 layer normalization은 parameter 분포를 일정하게 만들어 학습의 안정성 및 성능을 향상시키기 위해 추가되었습니다.&lt;/p&gt;

&lt;h4 id=&quot;2-decoder-blocks&quot;&gt;2. Decoder blocks&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/attention-is-all-you-need/14-decoder.png&quot; alt=&quot;14-decoder&quot; /&gt;
Decoder는 encoder의 두 sub-layer 사이에 attention을 가해주는 sub-layer가 하나 더 추가된 형태로 구성되었습니다.&lt;/p&gt;

&lt;p&gt;이 sub-layer은 decoder의 self-attention layer의 output을 query로, encoder의 output을 key와 value로 받는 attention layer입니다. 즉 decoder의 input이 encoder에서 학습된 representative input의 어떤 부분과 유사하며, 따라서 올바른 output을 도출하기 위해서는 input의 어떤 부분에 집중해야 하는지를 파악하는 부분이라고 할 수 있습니다.
Sub-layer 내의 연산은 W&lt;sup&gt;K&lt;/sup&gt;, W&lt;sup&gt;V&lt;/sup&gt;(weight parameter)가 encoder의 output 벡터를 key와 value 행렬로 변환하고, W&lt;sup&gt;Q&lt;/sup&gt;가 decoder의 첫 번째 sub-layer의 output 벡터를 query 행렬로 변환하는 형태로 진행됩니다.&lt;/p&gt;

&lt;p&gt;또한 decoder의 첫 번째 sub-layer인 self-attention layer은 attention을 학습할 때 아직 보거나 도출되지 않은 output sequence 값들에 대해 아무런 영향을 받지 않도록 attention score의 뒷부분에 &lt;strong&gt;&lt;em&gt;masking&lt;/em&gt;&lt;/strong&gt;을 가해줍니다.
이는 내적을 통해서 계산된 score 아주 큰 음수를 더하는 더하는 형태로 이루어지는데, softmax 연산을 거치면서 0에 수렴하는 attention score가 만들어지기 때문입니다.&lt;/p&gt;

&lt;p&gt;각 sub-layer의 output은 encoder에서와 마찬가지로 residual connection과 layer normalization 과정을 거칩니다. Input과 output의 크기는 항상 같게 연산되고, decoder는 병렬적이지 않고 순차적으로 결과를 예측합니다.&lt;/p&gt;

&lt;p&gt;결과적으로 탄생한 Transformer의 전체 구조는 다음과 같습니다.
&lt;img src=&quot;assets/images/attention-is-all-you-need/15-transformer.gif&quot; alt=&quot;15-transformer&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;p&gt;Transformer의 학습은 Workshop on Machine Translation(WNT) 데이터셋 중 English-German과 English-French를 활용하여 진행하였습니다.&lt;/p&gt;

&lt;p&gt;학습 과정은 Adam optimizer로 최적화하였고, 일정한 학습 단계가 지나면 learning rate가 일정한 비율로 감소하게끔 스케줄링하였습니다.&lt;/p&gt;

&lt;p&gt;네트워크의 일반화를 위해 각 sub-layer의 output에 dropout(p=0.1)을 적용하였으며 손실 함수를 계산할 때 one-hot label이 아닌 soft label (eta = 0.1)을 이용하는 label smoothing 기법을 사용하였습니다. 이러한 일반화 과정을 거치면 Transformer의 예측이 불확실해지는 경향을 보이기에 perplexity metric 관점에서는 손해를 보이지만, over-fitting을 방지하여 정확도 및 BLEU score 등의 metric 관점에서는 향상된 성능을 기대할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Transformer는 순차적인 학습 모델의 내재적인 한계에 의한 학습의 비효율성을 Self-attention 메커니즘을 통한 병렬화로 단번에 해결한 획기적인 발명이라고 생각합니다. Attention을 가하는 과정에서도 다양한 관점의 compatibility를 얻기 위해 병렬적으로 여러 개의 head를 학습하는 multi-head attention 기법이나, sequence의 순서 정보를 가하기 위한 positional encoding 기법 등 많은 창의적인 기법도 구조의 성능을 높이기 위해 고안되었습니다. 자연어와 같은 순차적인 데이터를 다루는 ML 연구자분들은 한번쯤 꼭 읽어보시면 좋을 것 같습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/analytics-vidhya/seq2seq-model-and-the-exposure-bias-problem-962bb5607097&quot;&gt;https://medium.com/analytics-vidhya/seq2seq-model-and-the-exposure-bias-problem-962bb5607097&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="NLP" />
      

      
        <category term="NLP" />
      

      
        <summary type="html">원문 : Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017.</summary>
      

      
      
    </entry>
  
</feed>
