<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://rauleun.github.io/tag/nlp/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://rauleun.github.io/" rel="alternate" type="text/html" />
  <updated>2021-04-13T11:01:41+09:00</updated>
  <id>https://rauleun.github.io/tag/nlp/feed.xml</id>

  
  
  

  
    <title type="html">RE Tech Archive | </title>
  

  
    <subtitle>machine learning research notes</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">BERT - Pretraining of Deep Bidirectional Transformers for Language Understanding 리뷰</title>
      <link href="https://rauleun.github.io/BERT" rel="alternate" type="text/html" title="BERT - Pretraining of Deep Bidirectional Transformers for Language Understanding 리뷰" />
      <published>2020-11-03T09:00:00+09:00</published>
      <updated>2020-11-03T09:00:00+09:00</updated>
      <id>https://rauleun.github.io/BERT</id>
      <content type="html" xml:base="https://rauleun.github.io/BERT">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” arXiv preprint arXiv:1810.04805 (2018).
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/201102-BERT/bert.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 Google AI Language에서 2019년 arxiv에 발표한 &lt;strong&gt;&lt;em&gt;Bert: Pre-training of deep bidirectional transformers for language understanding&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Transformer Encoder 구조를 활용하여 자연어에 대한 representation을 pre-training 하였습니다. Unsupervised learning 방식으로 학습을 진행하기 위해 Masked Language Model, Nest Sequence Prediction의 두 가지 학습 방식을 이용하였으며, 11개의 자연어 학습 능력 평가 지표에서 SOTA를 달성했다고 합니다.&lt;/p&gt;

&lt;p&gt;그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;feature-based-and-fine-tuning&quot;&gt;Feature-based and Fine-tuning&lt;/h2&gt;
&lt;p&gt;언어 모델을 pre-training 방식으로 학습하여 여러 가지 task에 적용하는 것은 효율성과 성능의 측면에서 우수했기 때문에 자연어 분야의 학습 전략으로 널리 활용되어 왔습니다. 모델을 pre-training 한 뒤에, task에 알맞게 이용하는 방식에는 크게 두 가지가 있습니다.&lt;/p&gt;

&lt;p&gt;우선 Feature-based 방식은 pre-training 과정에서 각 단어에 대한 representation이 완벽하게 학습되었다고 가정하기 때문에, task-specific한 재학습 과정에서 pre-trained network의 parameter들을 바꾸지 않습니다. 대신에 pre-trainied network를 통해 얻은 Embedding vector를 task에 맞는 network에 통과시켜서 그 network의 parameter를 학습시키는 방향으로 task를 수행합니다.
반면에 Fine-tuning 방식은 downstream task를 학습할 때 pre-trained network의 parameter를 전부 조정해줍니다. Feature-based 방식에 비해 Network capacity가 커지기 때문에 일반적으로 높은 정확도를 보이지만, 계산량도 마찬가지로 늘어난다는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;BERT는 Fine-tuning 방식을 기반으로 한 모델입니다. BERT를 각 downstream task 별로 어떻게 fine-tuning 했는지는 아래에서 자세히 다루겠습니다.&lt;/p&gt;
&lt;h2 id=&quot;bert-model-architecture&quot;&gt;BERT Model Architecture&lt;/h2&gt;
&lt;p&gt;BERT는 Vaswani의 논문에서 발표된 Transformer의 Encoder 구조를 그대로 차용하였습니다. 뒤에서 자세히 설명드리겠지만 encoder의 self-attention 메커니즘은 병렬적으로 output을 생성하기 때문에, 두 방향(bidirectional)에 대한 추론을 통해 학습할 수 있습니다. 이는 BERT가 기존의 단방향 모델(OpenAI GPT) 또는 제약이 있는 양방향 모델(Bi-LSTM을 활용한 ELMo 등)보다 좋은 성과를 내는 데에 큰 기여를 하게 됩니다.&lt;/p&gt;

&lt;p&gt;논문에서는 두 개의 모델을 공개하였는데, transformer layer 개수(12/24), hidden size(768/1024), self-attention head의 개수(12/16)에 따라 BERT base 모델과 BERT large 모델로 구분하였습니다. 각각의 parameter 개수는 110M/340M으로 BERT large가 약 세 배 정도의 capacity를 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;BERT는 단일 문장에 대한 downstream task 뿐만 아니라 두 개 이상의 문장에 대해서도 잘 작동할 수 있도록 input의 형태를 변형하였습니다. BERT의 input인 sequence는 한 개 또는 두 개 이상의 문장으로 구성되었습니다. 각 sequence은 항상 [CLS] token으로 시작되는데, 이 [CLS] token에 대한 output은 classification이 필요한 task의 경우 그 probability value로 이용하였습니다. 만약 한 sequence가 두 개 이상의 문장으로 구성된 형태라면 첫 번째 문장과 두 번째 문장 사이에 [SEP] token을 넣어서 구분해주었습니다. 또한, 두 문장을 구분하기 위한 추가적인 장치로 첫 번째 문장과 두 번째 문장에 서로 다른 sentence embedding을 더해주었습니다.&lt;/p&gt;

&lt;p&gt;문장 구성에 대한 이해를 위해 downstream task 몇 가지를 예를 들어 설명하겠습니다. 만약 한 문장 안에 하나의 빈칸을 뚫어 그 단어를 예측하는 형태의 task는, input으로 하나의 문장이 들어가고 output으로는 한개의 단어가 나오면 충분할 것입니다. 하지만 어떤 문단과 그에 대한 질문의 답을 찾는 복잡한 언어 추론 형태의 task에서는, 질문이 input의 첫 번째 문장으로 들어가고 문단이 input의 두 번째 문장으로 들어가며 두 문장에 대한 명확한 구분이 필요할 것입니다. 또한 output으로는 질문에 대한 답에 해당되는 문단의 위치를 start/end token으로 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/201102-BERT/input-representation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과적으로 BERT의 input representation은 token embedding, positional embedding, segment embedding의 합으로 구성됩니다. 우선 token embedding은 original input이 WordPiece Tokenization을 통해서 변환된 결과입니다. 또한 Transformer 논문에서 input에 순서 정보를 반영하기 위해서 활용한 positional embedding을 BERT에서도 마찬가지로 더해주었습니다. 마지막으로 두 개 이상의 문장이 input으로 들어올 수 있기 때문에 이를 구분하기 위한 segment embedding을 더해주었습니다.&lt;/p&gt;
&lt;h2 id=&quot;pre-training-strategy&quot;&gt;Pre-training Strategy&lt;/h2&gt;
&lt;p&gt;BERT의 pre-training에는 중요한 두 가지의 unsupervised task가 활용되었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Masked Language Model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;자연어를 잘 이해하고 있는 모델을 얻기 위해서는, 양방향 학습을 하는것이 아주 중요하다고 알려져 있습니다. 하지만 이전까지 대부분의 모델들은 LTR(Left to Right) 또는 RTL(Right to Left)이라고 불리는 단방향 학습이나 Bi-LSTM의 output을 이어붙이는 불완전한 양방향 학습을 통해 얻어지곤 했었습니다. BERT는 Transformer 구조와 Masked LM을 활용하여 multi-layer의 완벽한 양방향 학습을 구현하였습니다.&lt;/p&gt;

&lt;p&gt;Masked Language Model은 문장의 일부분을 masking token으로 치환해서 input으로 넣어주고, 해당 mask token을 원래 단어로 올바르게 복원하도록 학습하는 전략입니다. BERT에서는 문장의 모든 토큰 중 15%를 임의로 선택해 치환하였습니다.&lt;/p&gt;

&lt;p&gt;하지만 이 과정에서 발생할 수 있는 문제가 있습니다. 학습 과정에서는 문장의 일부가 Mask 토큰으로 바뀌어 있는 input을 거의 대부분 보게 되지만, 실제 downstream task에 적용할 때는 Mask 토큰이 없는 문장을 주로 보게 됩니다. 따라서 training과 test 환경의 mismatch가 발생하여 성능이 저하됩니다.&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해서 논문에서는 15%의 확률로 선택된 토큰들에 대해서, (1) 80%는 [Mask] 토큰으로 변경하고, (2) 10%는 다른 임의의 토큰으로 변경하고, (3) 10%는 원래 토큰으로 그대로 두는 전략을 이용하였습니다. 이렇게 전혀 다른 토큰을 추가함으로써 모델의 학습이 방해될 수 있다는 지적에 대해, 저자는 전체 토큰의 1.5%의 만에 해당되기 때문에 큰 영향이 없을 것이라고 답했으며 실제 실험을 통해 증명하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/201102-BERT/ablation-mask.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MLM에 대한 예시를 몇개 보여드리면, 기존의 “My dog is hairy”라는 문장에 대해 4번째 토큰이 선택되었다면, 해당 문장은
(1) 80%의 확률로 “My dog is [Mask]”
(2) 10%의 확률로 “My dog is apple”
(3) 10%의 확률로 “My dog is hairy”
로 변환될 수 있습니다.&lt;/p&gt;

&lt;p&gt;BERT와 비교되는 유명한 pre-trained language representation 모델들은 단방향 학습이나 불완전한 양방향 학습을 통해 생성되었습니다. 아래는 BERT와 OpenAI GPT, ELMo의 모델 구조를 비교한 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/201102-BERT/elmo-gpt.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 보이듯 완벽한 Bidirectional 구조를 가진 BERT와는 달리, OpenAI GPT는 Transformer decoder 구조를 차용한듯한 왼쪽에서 오른쪽을 향하는 단방향의 구조를 가지고 있습니다. 또한 ELMo 같은 경우에는, 왼쪽에서 오른쪽을 향하는 LSTM과 오른쪽에서 왼쪽을 향하는 LSTM의 output을 concatenate하여 양방향의 특성을 모두 가지는 representation을 생성하기 위한 구조를 보여주었습니다. 다만 두 LSTM의 결과를 이어붙인 것에 지나지 않기 때문에 완전한 양방향 학습이라고 보기는 어려우며, 계산량도 매우 많고 느리다는 단점이 있습니다. 따라서 Layer를 이어붙여서 깊게 쌓을 수 없고, 높은 성능을 보여주기 어렵게 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Next Sentence Prediction&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Question and Answering(QA) 또는 Natural Language Inference(NLI) 등 몇 개의 downstream task는 두 문장 사이의 관계를 이해해야만 해결할 수 있습니다. 언어 모델이 문장 사이의 관계를 학습할 수 있게 하기 위해서 BERT는 NSP라는 학습 전략을 활용하였습니다. NSP는 corpus에서 A와 B라는 두 개의 문장을 고르되, 50%는 A와 B가 내용적으로 연결되게 선택하고 나머지 50%는 A와 B가 내용적으로 전혀 관련이 없게 선택합니다. 각각은 IsNext와 NotNext라는 binary output으로 labeling한 뒤 이를 예측하게끔 언어 모델을 학습시킵니다.&lt;/p&gt;

&lt;p&gt;BERT의 NSP 정확도는 최종적으로 97~98%이었으며, 꽤나 높은 문장 간의 이해도를 보이게 됩니다. 이처럼 쉽고 간단한 NSP 학습 전략은 QA와 NLI 등의 downstream task 성능도 크게 향상시켜주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pre-training data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BERT는 pre-training corpus로 800M 개의 단어로 구성된 BooksCorpus와 2,500M개의 단어로 구성된 English Wikipedia dataset을 활용했습니다. (list, table, header 등은 학습에서 제외하였습니다.)&lt;/p&gt;
&lt;h2 id=&quot;fine-tuning-bert&quot;&gt;Fine-tuning BERT&lt;/h2&gt;
&lt;p&gt;Pre-training된 BERT 모델은 fine-tuning을 통해 downstream task에 적용하였습니다. Fine-tuning은 BERT 모델에 task-specific한 input-output 데이터를 넣고 end-to-end로 모든 parameter를 학습하였습니다. Pre-training 과정에 비해 훨씬 더 가볍게 진행되었으며, Cloud TPU 1개로 1시간 정도 학습하였습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 BERT의 성능을 검증하기 위해 11개의 downstream task에 대해 fine-tuning을 진행했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GLUE
&lt;img src=&quot;assets/images/201102-BERT/result-glue.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;8개의 task로 구성된 The General Language Understanding Evaluation(GLUE) task에 대해서 SOTA의 성능을 보여주었다. 모든 GLUE task에 대해 32의 batch size로 3 epoch만큼 fine-tuning 해주었으며, [CLS] 토큰의 output을 마지막 classification layer의 input으로 활용하여 K개의 class에 대해 분류했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SQuAD
&lt;img src=&quot;assets/images/201102-BERT/result-squad.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SQuAD는 The Stanford Question Answering Dataset의 약자로 100k개 이상의 질문-정답 쌍으로 구성되어 있습니다. 질문과 정답을 포함한 Wikipedia 문단을 input으로 넣으면, 문단 내의 정답 text 위치를 찾아내어야 합니다. BERT는 위에서 설명한 NSP 전략을 활용해 문장 사이의 관계를 이해하도록 학습되었기 때문에, SQuAD에 대해서도 SOTA의 성능을 보여줄 수 있었습니다.&lt;/p&gt;

&lt;p&gt;SQuAD 2.0은 정답이 없을 확률에 대해서도 label이 되었는데, 이 또한 마찬가지로 BERT는 SOTA의 성능을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SWAG
&lt;img src=&quot;assets/images/201102-BERT/result-swag.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SWAG는 The Situations With Adversarial Generations의 약자로 113k개의 문장 쌍으로 구성되어 있습니다. 한 문장이 주어질 때, 뒤에 나오기에 적절한 문장을 4개의 보기 중에 선택하는 형태로 구성되었습니다. [CLS] 토큰을 활용하여 4개의 선택지에 대한 score를 얻어서 평가하였습니다. BERT는 SWAG 데이터셋에 대해서도 마찬가지로 SOTA의 성능을 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/201102-BERT/fine-tuning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;ablation-studies&quot;&gt;Ablation Studies&lt;/h2&gt;

&lt;p&gt;BERT의 성능은 다양한 downstream task들에서 SOTA의 수준으로 확인되었습니다. 그렇다면 앞에서 제안한 BERT의 어떤 특징들이 우수한 성능을 발휘할 수 있게 해준걸까요? 논문에서는 Ablation study를 통해 다양한 장치들의 역할을 분석하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pre-training Strategies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BERT는 MLM과 NSP라는 두 개의 Task로 Pre-training 되었습니다. 논문에서는 각각의 task가 얼마나 중요한지를 보여주기 위해 NSP를 없애고 학습시켜보기도 하고, NSP를 없애고 단방향으로의(LTR) 학습을 진행해보기도 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/201102-BERT/ablation-pretraining.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NSP를 없애자 GLUE와 SQuAD의 성능이 동시에 크게 저하되었습니다. 문장 간의 이해도를 높이는 데 NSP가 중요한 역할을 하고 있으며, 일반적인 Language understanding에도 영향을 미치는 것으로 보입니다.&lt;/p&gt;

&lt;p&gt;또한 MLM을 통한 bidirectional 학습에서 LTR의 단방향 학습으로 변경했을 때에는 SQuAD의 성능이 급격히 저하되었습니다. SQuAD는 단방향 토큰 추론에 대한 task가 아니기 때문에 방향성이 제한됨에 따라 성능이 저하되는 것으로 추측할 수 있을 것 같습니다. 저자는 이를 보완하기 위해서 Bi-LSTM 구조를 모델 위에 얹어서 학습했는데, SQuAD의 성능은 향상시켜주었지만 일반적인 GLUE 결과를 저하시켰습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model Size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BERT 모델의 크기와 fine-tuning task의 정확도 간의 관계를 확인하기 위한 ablation study도 진행되었습니다. 다양한 layer 개수, hidden unit의 크기, attention head의 개수의 BERT 모델들이 같은 조건 하에서 학습되었습니다. 결과는 널리 알려진 바와 같이 모델의 크기가 커질수록 모델 capacity가 커지며 성능이 향상되었습니다. 또한 큰 capacity의 모델을 pre-training 한 경우에는 아주 작은 scale의 task에 대해서도 좋은 성능을 보여주었습니다. 이는 task의 scale이 아주 작은 경우에 BERT의 효용성이 높다는 의미인데, task-specific한 모델을 설계해서 scratch부터 학습하는 것보다 충분한 pre-training을 통해 얻은 general representation을 initial parameter로 fine-tuning을 하는 것이 더 좋은 성능을 보여주기 때문입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature-based BERT&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;기존의 BERT는 downstream task에 대해 fine-tuning 방식으로 학습하였습니다. 하지만 fine-tuning 방식은 모델의 구조를 크게 바꾸지 않기 때문에 모든 경우의 downstream task를 represent하기 어렵고, 모든 parameter를 update하므로 계산량도 많다는 단점이 있습니다. 때문에 ELMo와 같은 feature-based approach로 BERT를 활용했을 때 결과가 어떨지에 대해 ablation study가 진행되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/201102-BERT/ablation-feature.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각 단어의 Entity를 찾는 NER task에 대해 다양한 방식의 feature-based 학습을 진행했는데, SOTA와 비슷한 성능을 보여주었습니다. 이로써 BERT는 fine-tuning과 feature-based approach에 모두 효과적이라는 것이 확인되었습니다.&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;BERT는 다양한 NLP task에 대해 SOTA를 달성하면서, 발표 직후부터 지금까지 NLP 분야에 많은 영향력을 끼치고 있는 모델입니다. Bidirectional training을 위해 기존에 활용되지 않던 Masked Model Language를 도입했고, Next Sentence Prediction task를 통해 문장 사이의 관계도 효과적으로 학습하였습니다. 최근까지도 BERT를 기반으로 하거나 BERT를 뛰어넘기 위해 도전하는 논문들이 계속해서 출간되고 있습니다.&lt;/p&gt;

&lt;p&gt;BERT를 읽다 보니 NLP 관련 연구들에 관심이 더 생기는 것 같습니다. 시간이 되면 NLP 관련 논문들을 더 리뷰해보겠습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.04805ㅠ&quot;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/google-research/bert&quot;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="nlp" />
      

      
        <category term="nlp" />
      

      
        <summary type="html">원문 : Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” arXiv preprint arXiv:1810.04805 (2018).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Attention is All You Need 리뷰</title>
      <link href="https://rauleun.github.io/attention-is-all-you-need" rel="alternate" type="text/html" title="Attention is All You Need 리뷰" />
      <published>2020-09-07T20:00:00+09:00</published>
      <updated>2020-09-07T20:00:00+09:00</updated>
      <id>https://rauleun.github.io/attention-is-all-you-need</id>
      <content type="html" xml:base="https://rauleun.github.io/attention-is-all-you-need">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;본 글은 Google Brain에서 2017 NIPS에 발표한 &lt;strong&gt;&lt;em&gt;Attention is All You Need&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰이며 동시에 &lt;strong&gt;제 첫 논문 리뷰글&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;자연어 처리(NLP)등의 분야에서는 순서를 가진 sequence형 데이터를 학습하기 위해 그동안 많은 모델을 제시했습니다. 대표적으로 encoder와 decoder 방식을 활용한 RNN 또는 CNN이 있는데, 이들은 순서가 있는 데이터의 특성상 순차적으로 input을 학습하고 output을 도출할 수밖에 없었습니다.
&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/1-seqtoseq.png&quot; alt=&quot;1-seqtoseq&quot; /&gt;
순차적으로 데이터를 처리하는 방식은 근본적으로 병렬화(parallelization)를 불가능하게 만들어 계산적 효율성을 제한할뿐만 아니라 긴 문장이 들어왔을 때 여러 layer를 거치면서 앞의 input에 대한 정보가 희미해지는 등의 문제가 있었습니다. 이를 해결하기 위해 ‘output을 도출할 때 어떤 input에 집중해야 하는지’에 대한 정보를 추가하는 attention 메커니즘을 추가하기도 하였지만(seq2seq 모델) 여전히 학습 속도 저하에 대한 한계가 존재했습니다.
&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/2-transformer.png&quot; alt=&quot;2-transformer&quot; /&gt;
위 논문에서 제안한 Transformer는 &lt;strong&gt;병렬화&lt;/strong&gt;를 가능하게 하여 학습의 효율성과 모델의 성능을 기존보다 월등히 끌어올렸다는 점에서 NLP 및 다양한 분야에 많은 기여를 했다고 생각합니다. 또한 순차적인 과정 없이 attention 메커니즘만을 이용하였기 때문에 input과 output 사이에 포괄적인 상관관계(global dependency)를 이끌어냈습니다.&lt;/p&gt;
&lt;h2 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h2&gt;
&lt;p&gt;Transformer의 핵심 개념은 attention 메커니즘, 그 중에서도 self-attention 메커니즘이라고 할 수 있습니다.
Transformer는 attention 정보를 얻기 위해 각 sequential input에 대한 &lt;strong&gt;&lt;em&gt;Query, Key, Value&lt;/em&gt;&lt;/strong&gt; 값을 생성하고, Query와 Key 값을 매칭하여 input (NLP에서는 단어, vision에서는 pixel) 사이의 연관성 또는 유사도(compatibility)를 나타내는 attention score를 계산합니다.
Attention score는 해당 input과 다른 sequence inputs 사이의 의미론적 연관성을 나타내는 값이기 때문에, 이를 기존의 sequence input에 곱하면 전체 sequence 내에서 각 input이 갖는 의미를 강조한 새로운 sequence를 얻을 수 있습니다. 
&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/3-attention-mechanism.png&quot; alt=&quot;3-attention-mechanism&quot; /&gt;
Attention layer에 들어가기 전에 모든 input은 그 특성을 가장 잘 나타낼 수 있는 다차원의 embedding 벡터로 변환됩니다. Embedding 벡터는 d&lt;sub&gt;model&lt;/sub&gt; 의 차원을 가지게 되고, 이는 각 encoder 또는 decoder 블럭을 지나도 동일하게 유지됩니다. 위 그림에서 d&lt;sub&gt;model&lt;/sub&gt;=4로 표현되었지만, 실제 논문에서는 d&lt;sub&gt;model&lt;/sub&gt;=512를 사용하였습니다. Input sequence를 embedding 벡터로 변환하는 과정은 학습시킨 Linear layer을 통과함으로써 이루어집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/4-attention-mechanism.png&quot; alt=&quot;4-attention-mechanism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Self-attention 과정에서 embedding 벡터는 W&lt;sup&gt;Q&lt;/sup&gt;, W&lt;sup&gt;K&lt;/sup&gt;, W&lt;sup&gt;V&lt;/sup&gt; 의 세 행렬을 통해 각각 query, key, value 벡터로 변환됩니다. Self-attention 과정이 일반적인 attention 메커니즘과 다른 점은, query, key, value 벡터를 하나의 동일한 벡터로부터 생성한다는 점입니다. 변환 행렬의 크기를 (d&lt;sub&gt;model&lt;/sub&gt; x d&lt;sub&gt;query&lt;/sub&gt;, d&lt;sub&gt;model&lt;/sub&gt; x d&lt;sub&gt;key&lt;/sub&gt;, d&lt;sub&gt;model&lt;/sub&gt; x d&lt;sub&gt;value&lt;/sub&gt;)라고 한다면 output 벡터의 차원은 d&lt;sub&gt;query&lt;/sub&gt;, d&lt;sub&gt;key&lt;/sub&gt;, d&lt;sub&gt;value&lt;/sub&gt;가 됩니다. (본문에서는 d&lt;sub&gt;query&lt;/sub&gt; = d&lt;sub&gt;key&lt;/sub&gt; = d&lt;sub&gt;value&lt;/sub&gt; = 64로 설정하였습니다.)&lt;/p&gt;

&lt;p&gt;여기서 W&lt;sup&gt;Q&lt;/sup&gt;, W&lt;sup&gt;K&lt;/sup&gt;, W&lt;sup&gt;V&lt;/sup&gt;는 우리가 학습시켜야 하는 Transformer의 parameter입니다. 세 행렬 W&lt;sup&gt;Q&lt;/sup&gt;, W&lt;sup&gt;K&lt;/sup&gt;, W&lt;sup&gt;V&lt;/sup&gt;에 적당한 값을 넣어줘야 input 사이의 관계를 가장 잘 나타내는 attention score를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/5-attention-mechanism.png&quot; alt=&quot;5-attention-mechanism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 query 벡터와 모든 sequence input의 key 벡터 사이의 연관성을 계산해야 합니다. Self-attention에서 Query와 Key 행렬의 크기는 같고 그 결과로 나온 query와 key 벡터의 차원도 같기 때문에, 내적(inner-product)을 통해 두 벡터의 유사도를 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 내적을 통해 계산된 attention score를 d&lt;sub&gt;key&lt;/sub&gt;의 제곱근으로 나눠주었습니다. 이는 embedding 또는 key 벡터의 차원이 증가함에 따라 score 값이 커지는 현상을 방지하기 위해서이며, 역전파 과정에서 발생하는 gradient의 크기를 안정화하는 효과가 있습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 softmax 연산을 통해 합이 1인 형태로 결과값을 normalize하여 최종적인 attention score을 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/6-attention-mechanism.png&quot; alt=&quot;6-attention-mechanism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 각 value 벡터에 attention score를 곱해서 최종적인 attention layer의 output을 얻게됩니다. 이는 각 벡터들 사이의 연관성(또는 유사도)를 가중합한 value 벡터입니다. 이것으로 Self-attention 메커니즘이 마무리되고, 결과로 얻은 벡터는 feed-forward network를 거쳐 다음 encoder block의 input 역할을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/7-attention-mechanism.png&quot; alt=&quot;7-attention-mechanism&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 모든 과정은 행렬 연산으로 아주 빠르게 처리할 수 있습니다. 행렬 연산이라는 attention 메커니즘의 특성 덕분에 Transformer에서는 기존 RNN 기반 네트워크에서 이루지 못한 &lt;strong&gt;&lt;em&gt;sequential 데이터의 병렬화&lt;/em&gt;&lt;/strong&gt; 가 가능해집니다.&lt;/p&gt;
&lt;h2 id=&quot;multi-head-attention&quot;&gt;Multi-head attention&lt;/h2&gt;
&lt;p&gt;Transformer에서는 attention의 성능을 높이기 위해 &lt;strong&gt;&lt;em&gt;Multi-head attention&lt;/em&gt;&lt;/strong&gt;을 도입하였습니다. Multi-head attention이란, weight를 공유하지 않는 N(number of heads)개의 Query, Key, Value 행렬을 병렬적으로 두어 서로 다른 attention score을 계산할 수 있도록 한 방식입니다. (본문에서 N=8 입니다.)
Multi-head attention을 이용하는 이유는, &lt;strong&gt;다양한 관점에서 단어(또는 pixel) 사이의 관계를 파악하기 위해서&lt;/strong&gt;입니다. 아래 아주 유명한 예시가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/8-multi-head-attention.png&quot; alt=&quot;8-multi-head-attention&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;The animal didn't cross the street because it was too tired&lt;/blockquote&gt;
&lt;p&gt;위 문장에서, `it` 이라는 단어는 `animal`을 지칭할 수도 있고 `street`을 지칭할 수도 있습니다.
우리는 `it`이 `animal`을 지칭하는 것을 알고 있지만, 만약 학습이 잘못되어 `street`의 attention score가 커지게 된다면 좋은 translation 성능을 내지 못할 것입니다.
이러한 오류를 방지하기 위해 Transformer 에서는 여러 개의 &lt;strong&gt;&lt;em&gt;Attention head&lt;/em&gt;&lt;/strong&gt; 를 병렬적으로 두어 각 module에서 서로 다른 관점의 attention score을 가지게 하였습니다.
서로 다른 관점으로 input sequence를 파악하길 바라는 의도로 추가하였으니, weight sharing을 하지 않는 것은 당연하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/9-multi-head-attention.png&quot; alt=&quot;9-multi-head-attention&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multi-head attention을 거치게 되면 N개의 output을 얻게 됩니다. 다음 layer를 통과하기 전에 N개의 output을 하나로 합쳐야 하는데 본 논문에서는 단순히 concatenate 연산을 통해 해결하였습니다. 이후 feed-forward network를 거쳐 input과 같은 차원을 가지는 벡터로 변환됩니다.
Multi-head attention을 이용하더라도 일반적으로 weight parameter 개수는 일정하게 유지되는데, N개의 head를 둠으로써 행렬 개수가 N개로 늘어나는 만큼 행렬의 차원이 1/N배로 줄어들기 때문입니다.&lt;/p&gt;
&lt;h2 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/10-positional-encoding.png&quot; alt=&quot;10-positional-encoding&quot; /&gt;
그렇다면 Transformer는 순차적인 구조가 아닌데 어떻게 sequential 데이터의 위치 정보를 전달할 수 있을까요? 본문에서는 그에 대한 답으로 &lt;strong&gt;&lt;em&gt;positional encoding&lt;/em&gt;&lt;/strong&gt;을 제안합니다.&lt;/p&gt;

&lt;p&gt;Positional encoding이란 embedding 벡터가 encoder 또는 decoder에 들어가기 전에 &lt;strong&gt;위치정보를 지닌 벡터를 더해주는 과정&lt;/strong&gt;을 의미합니다. 만약 position encoding을 가하지 않는다면, 문장 맨 앞에 나온 `Animal` 이라는 단어와 문장 맨 뒤에 나온 `Animal`이 같은 값을 가진 채로 네트워크에 입력되게 됩니다. 하나는 주어 역할을 하고, 다른 하나는 목적어 역할을 할 수도 있는데 말입니다.&lt;/p&gt;

&lt;p&gt;Transformer에서는 embedding 벡터에 위치에 따라 값이 달라지는 값을 더해줌으로써 sequence의 순서를 고려하였습니다.
Positional encoding 벡터에는 여러 종류가 있을 수 있지만, 본문에서는 sinosoidal positional encoding 형식을 활용하였습니다.
&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/equation1.gif&quot; alt=&quot;equation-1&quot; /&gt;
d&lt;sub&gt;model&lt;/sub&gt;=512인 경우에 대한 positional encoding 값은 colormap 형식으로 아래와 같이 나타낼 수 있습니다.
20개(y-axis)의 input에 대해 512차원(x-axis)의 연속적이며 상대적인 정보를 잘 나타내고 있습니다.
&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/11-positional-encoding.png&quot; alt=&quot;11-positional-encoding&quot; /&gt;
이외에 positional encoding 벡터를 학습 과정에서 결정하는 Learned positional encoding 방법이 있습니다. 다만 Learned positional encoding 방법은, 이전에 학습을 진행한 적 없던 긴 sequence의 input이 들어왔을 때, 적절한 positional encoding을 가하지 못하게 되는 한계점이 있습니다.&lt;/p&gt;
&lt;h2 id=&quot;transformer-architecture&quot;&gt;Transformer Architecture&lt;/h2&gt;
&lt;p&gt;이제 Transformer가 어떤 구조를 가지고 있으며 각 구조는 어떤 역할을 하는지 살펴보겠습니다.
&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/12-transformer.png&quot; alt=&quot;12-transformer&quot; /&gt;
Transformer는 여러 개의 encoder 블럭과 decoder 블럭으로 구성되어 있습니다. 본문에서는 각각 6개의 블럭을 사용하였습니다. Input sequence가 같은 구조의 encoder/decoder 블럭을 연속해서 통과해야 하므로, 각 블럭의 input과 output은 항상 같은 크기를 가져야 합니다. 이를 위해 보통 마지막 layer에 feed-forward network를 배치하여 차원을 맞춰주는 역할을 합니다.&lt;/p&gt;

&lt;h4 id=&quot;1-encoder-blocks&quot;&gt;1. Encoder blocks&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/13-encoder.png&quot; alt=&quot;13-encoder&quot; /&gt;
하나의 Encoder block은 두 부분의 sub-layer로 구분할 수 있습니다.
첫 번째 layer은 Transformer의 핵심 개념인 self-attention을 수행하는 layer입니다.
Self-attention을 거쳐 얻은 output은 두 번째 layer인 feed-forward layer를 통과합니다.&lt;/p&gt;

&lt;p&gt;각 sub-layer의 output은 residual connection과 layer normalization 과정을 거치게 됩니다. Residual connection은 여러 encoder 블럭을 지나게 되면서 positional encoding 정보가 희미해지는 현상을 방지하기 위해서 추가되었습니다. 또한 layer normalization은 parameter 분포를 일정하게 만들어 학습의 안정성 및 성능을 향상시키기 위해 추가되었습니다.&lt;/p&gt;

&lt;h4 id=&quot;2-decoder-blocks&quot;&gt;2. Decoder blocks&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/14-decoder.png&quot; alt=&quot;14-decoder&quot; /&gt;
Decoder는 encoder의 두 sub-layer 사이에 attention을 가해주는 sub-layer가 하나 더 추가된 형태로 구성되었습니다.&lt;/p&gt;

&lt;p&gt;이 sub-layer은 decoder의 self-attention layer의 output을 query로, encoder의 output을 key와 value로 받는 attention layer입니다. 즉 decoder의 input이 encoder에서 학습된 representative input의 어떤 부분과 유사하며, 따라서 올바른 output을 도출하기 위해서는 input의 어떤 부분에 집중해야 하는지를 파악하는 부분이라고 할 수 있습니다.
Sub-layer 내의 연산은 W&lt;sup&gt;K&lt;/sup&gt;, W&lt;sup&gt;V&lt;/sup&gt;(weight parameter)가 encoder의 output 벡터를 key와 value 행렬로 변환하고, W&lt;sup&gt;Q&lt;/sup&gt;가 decoder의 첫 번째 sub-layer의 output 벡터를 query 행렬로 변환하는 형태로 진행됩니다.&lt;/p&gt;

&lt;p&gt;또한 decoder의 첫 번째 sub-layer인 self-attention layer은 attention을 학습할 때 아직 보거나 도출되지 않은 output sequence 값들에 대해 아무런 영향을 받지 않도록 attention score의 뒷부분에 &lt;strong&gt;&lt;em&gt;masking&lt;/em&gt;&lt;/strong&gt;을 가해줍니다.
이는 내적을 통해서 계산된 score 아주 큰 음수를 더하는 더하는 형태로 이루어지는데, softmax 연산을 거치면서 0에 수렴하는 attention score가 만들어지기 때문입니다.&lt;/p&gt;

&lt;p&gt;각 sub-layer의 output은 encoder에서와 마찬가지로 residual connection과 layer normalization 과정을 거칩니다. Input과 output의 크기는 항상 같게 연산되고, decoder는 병렬적이지 않고 순차적으로 결과를 예측합니다.&lt;/p&gt;

&lt;p&gt;결과적으로 탄생한 Transformer의 전체 구조는 다음과 같습니다.
&lt;img src=&quot;assets/images/200907-attention-is-all-you-need/15-transformer.gif&quot; alt=&quot;15-transformer&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;p&gt;Transformer의 학습은 Workshop on Machine Translation(WNT) 데이터셋 중 English-German과 English-French를 활용하여 진행하였습니다.&lt;/p&gt;

&lt;p&gt;학습 과정은 Adam optimizer로 최적화하였고, 일정한 학습 단계가 지나면 learning rate가 일정한 비율로 감소하게끔 스케줄링하였습니다.&lt;/p&gt;

&lt;p&gt;네트워크의 일반화를 위해 각 sub-layer의 output에 dropout(p=0.1)을 적용하였으며 손실 함수를 계산할 때 one-hot label이 아닌 soft label (eta = 0.1)을 이용하는 label smoothing 기법을 사용하였습니다. 이러한 일반화 과정을 거치면 Transformer의 예측이 불확실해지는 경향을 보이기에 perplexity metric 관점에서는 손해를 보이지만, over-fitting을 방지하여 정확도 및 BLEU score 등의 metric 관점에서는 향상된 성능을 기대할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Transformer는 순차적인 학습 모델의 내재적인 한계에 의한 학습의 비효율성을 Self-attention 메커니즘을 통한 병렬화로 단번에 해결한 획기적인 발명이라고 생각합니다. Attention을 가하는 과정에서도 다양한 관점의 compatibility를 얻기 위해 병렬적으로 여러 개의 head를 학습하는 multi-head attention 기법이나, sequence의 순서 정보를 가하기 위한 positional encoding 기법 등 많은 창의적인 기법도 구조의 성능을 높이기 위해 고안되었습니다. 자연어와 같은 순차적인 데이터를 다루는 ML 연구자분들은 한번쯤 꼭 읽어보시면 좋을 것 같습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/analytics-vidhya/seq2seq-model-and-the-exposure-bias-problem-962bb5607097&quot;&gt;https://medium.com/analytics-vidhya/seq2seq-model-and-the-exposure-bias-problem-962bb5607097&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="nlp" />
      

      
        <category term="nlp" />
      

      
        <summary type="html">원문 : Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017.</summary>
      

      
      
    </entry>
  
</feed>
