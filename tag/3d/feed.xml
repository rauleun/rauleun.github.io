<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://rauleun.github.io/tag/3d/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://rauleun.github.io/" rel="alternate" type="text/html" />
  <updated>2021-04-26T19:36:01+09:00</updated>
  <id>https://rauleun.github.io/tag/3d/feed.xml</id>

  
  
  

  
    <title type="html">RE Tech Archive | </title>
  

  
    <subtitle>machine learning research notes</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">RandLA-Net - Efficient Semantic Segmentation of Large-Scale Point Clouds 리뷰</title>
      <link href="https://rauleun.github.io/RandLA-Net" rel="alternate" type="text/html" title="RandLA-Net - Efficient Semantic Segmentation of Large-Scale Point Clouds 리뷰" />
      <published>2021-04-23T09:00:00+09:00</published>
      <updated>2021-04-23T09:00:00+09:00</updated>
      <id>https://rauleun.github.io/RandLA-Net</id>
      <content type="html" xml:base="https://rauleun.github.io/RandLA-Net">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/abs/1911.11236&quot;&gt;Hu, Qingyong, et al. “Randla-net: Efficient semantic segmentation of large-scale point clouds.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘은 2020년 CVPR에서 발표한 &lt;strong&gt;&lt;em&gt;RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰를 진행해보려 합니다.&lt;/p&gt;

&lt;p&gt;RandLA-Net은 큰 크기의 3D point cloud 데이터셋에 대해 시간 및 메모리 측면에서 효율적으로 Segmentation task를 수행하고, 결과적으로 높은 Segmentation 정확도를 보여주었습니다. 그렇다면 RandLA-Net에 대한 소개를 시작하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;large-scale-point-clouds&quot;&gt;Large-scale Point Clouds&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/1-large_scale_dataset.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PointNet을 시작으로, deep learning network를 이용해 3D point cloud 데이터를 처리하는 연구가 활발히 진행되고 있습니다. 하지만 몇몇 모델(&lt;a href=&quot;https://rauleun.github.io/Superpoint-Graphs&quot;&gt;Superpoint Graphs&lt;/a&gt;)을 제외한 대부분의 모델들은 연산량 및 메모리의 한계때문에 작은 크기의 점들로 구성된 point cloud 데이터만을 input으로 넣을 수 있고, 결국 공간을 1x1 제곱미터 또는 4,096개씩 단위로 subsampling해서 input으로 넣어주곤 합니다.&lt;/p&gt;

&lt;p&gt;Subsampling을 하지 않은 large-scale point cloud 데이터셋을 그대로 이용하게 되면 subsampling 과정을 통해서 손실되는 정보가 없고, 공간 전체의 구조를 보면서 feature를 추출할 수 있기 때문에 복잡하고 큰 기하학적 구조도 학습할 수 있으며, voxelization이나 graph construction 등 별도의 전/후처리 과정이 필요하지 않습니다. 하지만 연산량 및 메모리적으로 무거워지고, 복잡한 기하학적 구조 때문에 학습이 잘 되지 않을 수도 있습니다.&lt;/p&gt;

&lt;p&gt;기존 모델들에서 연산량이 많아 병목이 생기는 부분은 점들에 대해 down-sampling을 진행하는 부분이었습니다. 예를 들어 PointNet이나 GACnet, Point Transformer 등은 모두 farthest point sampling(FPS) 알고리즘을 통해 점들을 선택하는데, FPS 알고리즘의 시간복잡도는 &lt;em&gt;O(N2)&lt;/em&gt; 으로 점들의 개수의 제곱에 비례해서 커지게 됩니다. 따라서 점들의 개수가 많아지면, 계산량이 급격하게 많아져 학습 시간이 길어지고 학습의 효율성이 떨어집니다.&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해 RandLA-Net에서는 &lt;strong&gt;random sampling (RS)&lt;/strong&gt;을 이용해 down-sampling을 진행했습니다. Random sampling은 &lt;em&gt;O(1)&lt;/em&gt; 의 시간복잡도를 가진 연산이기 때문에, 점들의 개수가 많아져도 연산량이 늘어나지 않습니다. 다만 RS 알고리즘은 임의로 점들을 선택하기 때문에, 이를 이용하는 경우 중요한 점들을 소실하여 전체 point cloud에 대한 대표성을 잃어버릴 수 있습니다. RandLA-Net에서는 이를 보완하기 위해 강력한 &lt;strong&gt;local feature aggregation module&lt;/strong&gt;을 제안하여 point cloud의 중요한 feature들을 유지할 수 있도록 네트워크를 설계했습니다.&lt;/p&gt;

&lt;h2 id=&quot;efficient-sampling&quot;&gt;Efficient Sampling&lt;/h2&gt;

&lt;p&gt;Point cloud를 구성하는 점들의 개수를 줄이면서 인접한 점들의 특징을 통합해주는 downsampling 과정은 convolution 기반 네트워크의 pooling layer와 같은 역할을 하는 중요한 과정입니다. 지금까지 연구된 다양한 downsampling 방법론들을 소개하고 장단점 및 특징을 정리해보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Farthest Point Sampling (FPS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;FPS는 N개의 점들 중 서로 가장 거리가 먼 K개의 점들을 추출하는 sampling 알고리즘입니다. 전체 point set에 대해서 골고루 점들을 추출할 수 있기 때문에 정보를 잘 소실하지 않고, 이 때문에 가장 널리 이용되는 sampling 알고리즘입니다. 하지만 시간복잡도가 &lt;em&gt;O(N2)&lt;/em&gt; 으로 크기 때문에, 점들의 개수가 많은 point set에 이용하기에는 연산량이 많아 비효율적입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inverse Density Inportance Sampling (IDIS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;IDIS는 각 점들의 density를 측정하고, 그에 따라 N개의 점들 중 density가 높은 K개의 점들을 추출하는 sampling 알고리즘입니다. 시간복잡도는 &lt;em&gt;O(N)&lt;/em&gt; 으로 FPS보다 효율적입니다. 다만 density가 높은 점들 위주로 sampling을 하기 때문에, sparse하지만 중요하게 여겨지는 일부 정보들을 소실할 수 있습니다. 또한 &lt;em&gt;O(N)&lt;/em&gt; 의 시간복잡도로는 실시간 3D segmentation을 수행하기에 여전히 연산량이 많습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Random Sampling (RS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RS는 N개의 점들 중 임의로 K개의 점들을 추출하는 sampling 알고리즘입니다. 시간복잡도는 &lt;em&gt;O(1)&lt;/em&gt; 로 전체 점의 개수에 무관하게 결정됩니다. 이렇듯 RS는 계산량 측면에서 가장 효율적이지만, sampling한 점들이 전체 점들을 대표하지 못하고 중요한 정보를 잘 잃어버리기 때문에 일반적으로 사용하지 않는 방법입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generator-based Sampling (GS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GS는 학습된 네트워크를 통해 point cloud를 잘 대표할 수 있는 점들을 추출합니다. 하지만 기존의 점들과 sampling 이후의 점들을 matching하는 과정에서 FPS 알고리즘이 들어가기 때문에, 계산량 측면에서 비효율적입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Continuous Relaxation based Sampling (CRS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CRS는 모든 점들에 대한 가중합을 통해 sampling point들을 추출하는 방법이라고 합니다. 이 때 모든 점들의 feature vector를 input으로 받는 아주 큰 차원의 weight matrix가 필요하기 때문에 메모리 측면에서 비효율적입니다. 일반적으로 점들의 개수가 많아지면, 수백GB의 GPU 메모리가 필요하기 때문에 잘 사용하지 않는다고 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Policy Gradient based Sampling (PGS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PGS는 Markov decision process를 통해 sampling을 위한 확률분포값을 학습합니다. N개의 점들 중 어떠한 K개의 점들을 선택했을 때 reward가 가장 높을 지를 exploration 과정을 통해서 학습하는데, 점들의 개수가 많아지게 되면 exploration space가 커지게 되어 reward가 높은 지점을 잘 찾지 못하게 됩니다. 즉, large point cloud 데이터셋에 대해서는 손실함수가 잘 수렴하지 않아 네트워크 학습이 잘 되지 않는다고 합니다.&lt;/p&gt;

&lt;p&gt;정리해보면, FPS, IDIS, GS는 연산량 측면에서 비효율적이고, CRS는 메모리 측면에서 비효율적이며, PGS는 학습이 잘 안된다는 문제가 발생합니다. 반면에 random sampling은 계산량 측면에서 효율적이고 메모리가 많이 필요하지도 않습니다. 따라서 large-scale point cloud 데이터셋을 다루기에는 가장 적절한 sampling 기법입니다. 다만, random sampling는 임의의 점들을 선정하는 방법이기 때문에 sampling 과정에서 중요한 점들이 소실될 수 있습니다. 이를 보완하기 위해서 RandLA-Net에서는 강력한 local feature aggregation module을 제안하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;local-feature-aggregation&quot;&gt;Local Feature Aggregation&lt;/h2&gt;

&lt;p&gt;RandLA-Net은 &lt;em&gt;Dilated Residual Block&lt;/em&gt;을 이용해서 feature vector를 통합해주었습니다. Dilated residual block은 &lt;em&gt;Local Spatial Encoding(LocSE) block&lt;/em&gt;과 &lt;em&gt;Attentive Pooling block&lt;/em&gt; 으로 구성되어 있습니다. 각각이 어떤 역할을 하는지 확인해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/2-feature_aggregation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Local Spatial Encoding (LocSE)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/15-position_feature.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LocSE block은 크게 두 부분으로 구성되어 있습니다. 우선 하나의 중심점(&lt;em&gt;p&lt;/em&gt;)에 대해 인접한 K개의 점들을(&lt;em&gt;p1 ~ pk&lt;/em&gt;) KNN 알고리즘을 통해 선정합니다. 이후 중심점과 각 인접한 점 사이의 위치 관계를 담은 relative point position vector를 위와 같은 형식으로 추출합니다. 이 때 &lt;em&gt;p&lt;/em&gt; 와 &lt;em&gt;pk&lt;/em&gt; 는 점들의 xyz 좌표를 의미하고, +는 concatenation 연산을, ㅣㅣ는 유클리드 거리 연산을 의미합니다. Relative point position vector는 총 10차원의 벡터이며(3+3+3+1), 중심점과 주변 점들 사이의 공간적인 위치 정보를 담고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/16-position_feature_encoding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이후 각 벡터를 MLP layer에 통과시켜서 feature vector와 같은 차원으로 변환하는데, 이를 &lt;em&gt;relative point position encoding&lt;/em&gt; 이라고 부릅니다. 논문에서는 relative point position encoding 과정이 기하학적 패턴이나 복잡한 점들 간의 구조를 파악하는 데 효과적이라고 이야기하고 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Attentive Pooling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/4-attentive_score.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LocSE를 통해서 얻은 spatial encoding vector는 feature vector와의 concatenation을 통해 총 &lt;em&gt;K x 2d&lt;/em&gt; 크기의 행렬로 변환됩니다. Attentive pooling은 K개의 vector에 attention score를 곱해주어서 하나의 통합된 feature vector를 생성하는 과정입니다. 이 때 Attention score는 local feature vector를 input으로 받아서 shared MLP와 softmax layer를 통과시켜 얻었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/5-multiplication_score.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이후 K개의 vector에 attention score를 가중합하면, 주변 점들의 위치 관계와 특징을 담은 informative feature vector &lt;em&gt;f&lt;/em&gt; 를 얻을 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dilated Residual Block&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/3-dilated_residual.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dilated residual block은 앞서 설명드린 LocSE block과 attentive pooling block으로 구성되어 있습니다. Down-sampling을 통해 점들의 개수를 줄여주면, 각 점들에 대한 receptive field를 확장시켜서 feature vector를 통합해주는 것이 중요합니다. 실제로 CNN의 경우 convolution이 반복적으로 일어나면서 feature의 크기가 줄어들기 때문에, feature vector의 각 pixel에 대한 receptive field는 커지게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/6-receptive_field.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LocSE-Attentive pooling으로 이어지는 feature aggregation 과정의 경우, 인접한 K개의 점들의 특징이 집약된 feature vector를 얻을 수 있습니다. LocSE-Attentive pooling 과정을 두 번 거치게 되면, 인접한 각 점들도 K개의 점들의 특징이 통합된 점들이기 때문에, 위의 그림처럼 대략 &lt;em&gt;K^2&lt;/em&gt; 개의 점들의 공간적 관계와 특징을 통합한 벡터를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;Feature aggregation 과정은 반복하면 할수록 receptive field도 커지고 feature vector의 정보량도 늘어나지만, 연산 효율이 떨어지고 over-fitting에 취약해집니다. 따라서 RandLA-Net에서는 하나의 dilated residual block당 LocSE-Attentive pooling을 두 번 반복해서 적절한 범위의 점들을 통합해주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/14-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전체 RandLA-Net은 위의 그림과 같은 구조로 구성되었습니다. 학습시에는 Adam optimizer를 이용해주었고, nearest neighbor의 개수는(K) 16을 기본으로 설정했습니다. 또한 대략 10만개 정도의 고정된 점들에 대해 batch 단위로 학습을 진행하였고, test 과정에서는 전체 점들을 한번에 넣어주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Efficiency of Random Sampling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우선 random sampling의 효율성을 확인하기 위해, 앞서 설명한 다양한 sampling 방법들을 이용해 시간 및 메모리 측면에서의 효율성을 측정했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/7-sampling_efficiency.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과적으로 적은 수의 점들을 대상으로는 sampling 방법들 사이에 큰 차이가 없지만, 점들의 개수가 많아지면 많아질수록 FPS/IDIS/GS/CRS/PGS는 학습 시간이 길거나 메모리를 많이 소비하는 문제를 야기했습니다. 논문에서는 이러한 비효율성 때문에 대부분의 모델들(PointNet, GACNet 등)의 학습 과정이 적을 수의 점들로 구성된 point cloud dataset으로만 진행되었다고 주장했습니다. 그에 비해 RS는 높은 시간-메모리적 효율성을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Efficiency of RandLA-Net&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/8-model_efficiency.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 표는 많은 수의 점들로 구성된 point cloud dataset에 대한 여러 모델들의 효율성을 비교한 결과입니다. 각 모델의 parameter 개수 및 SemanticKITTI dataset에 대한 학습 시간을 비교하였습니다. 또한 전체 점들을 한번에 넣었을 때 추론 시간도 측정하였습니다. RandLA-Net은 모델 크기가 크지 않음에도 불구하고, 가장 빠른 학습 시간 및 추론 시간을 보여주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;3d-segmentation-results&quot;&gt;3D Segmentation results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;S3DIS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/12-s3dis_result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RandLA-Net은 다양한 dataset에 대해 3D semantic segmentation을 수행하였습니다. 우선 실내 dataset인 S3DIS에 대해 SOTA에 준하는 성능을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic3D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/9-semantic3d-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 실외 dataset인 Semantic3D에 대해서는 기존 KPConv의 성능을 뛰어넘으며 SOTA의 성능을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic-KITTI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/10-semantickitti_result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 SLAM LiDAR dataset인 Semantic-KITTI에 대해서도 SOTA에 준하는 성능을 보여주었습니다. RangeNet 등의 projection 기반 방법들도 좋은 성능을 보여주었는데, RandLA-Net은 RangeNet에 비하면 40배 적은 parameter로 구성되어 있기 때문에 연산량 차원에서 훨씬 효율적이라고 이야기합니다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/13-ablation_study.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RandLA-Net의 구성 요소들을 약간 바꿔보는 형식으로 제거 학습을 진행했습니다. LocSE를 없애거나 attentive pooling을 max-, mean-, sum-pooling 등으로 바꿔보았습니다. 그리고 dilated residual block에서 feature aggregation unit을 1개로 줄여보는 등의 실험을 했는데, 모두 기존의 모델 구조보다 저하된 성능을 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210423-RandLA-Net/17-ablation_encoding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 LocSE block에서 relative point position vector의 구성에 따라 segmentation 결과가 어떻게 변하는지 비교해보았습니다. Relative point position vector를 구성하는 여러가지 요소들 중에, 특히 상대적인 위치 정보에 대한 component가 작은 공간의 기하학적 구조를 담고 있기 때문에 중요한 역할을 한다는 것을 알 수 있었습니다.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;RandLA-Net은 downsampling 방법으로 RS를 처음으로 이용하여 연산량 측면에서 효율성을 극대화하였습니다. 사실 RS는 sampling의 안정성 측면에서 조금 극단적인 측면이 있어서 잘 이용되지 않았었는데요. Feature aggregation만 잘 해준다면 random하게 점들을 골라도 충분히 좋은 성능이 나온다는 것을 보여주는 연구였습니다. 다양한 dataset에서 좋은 성능을 나타냈기 때문에 상황이 맞는다면 고려해볼만한 모델이라고 생각합니다. 궁금한 점이 있으시다면 댓글로 남겨주세요. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.11236&quot;&gt;https://arxiv.org/abs/1911.11236&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/aRI0U/RandLA-Net-pytorch&quot;&gt;https://github.com/aRI0U/RandLA-Net-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3d" />
      

      
        <category term="3d" />
      

      
        <summary type="html">원문 : Hu, Qingyong, et al. “Randla-net: Efficient semantic segmentation of large-scale point clouds.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Point Transformer 리뷰</title>
      <link href="https://rauleun.github.io/Point-Transformer" rel="alternate" type="text/html" title="Point Transformer 리뷰" />
      <published>2021-03-02T09:00:00+09:00</published>
      <updated>2021-03-02T09:00:00+09:00</updated>
      <id>https://rauleun.github.io/Point%20Transformer</id>
      <content type="html" xml:base="https://rauleun.github.io/Point-Transformer">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/abs/2012.09164&quot;&gt;Zhao, Hengshuang, et al. “Point transformer.” arXiv preprint arXiv:2012.09164 (2020).&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 2020년 발표된 &lt;strong&gt;&lt;em&gt;Point Transformer&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;Transformer 구조는 Natural language processing(NLP) 분야에서 처음 소개된 이후로, NLP 뿐만 아니라 object classification 및 detection 등 다양한 task에 적용되어 뛰어난 성능을 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;Point Transformer는 point cloud classification 및 segmentation task에 transformer 구조를 적용한 논문인데, 구체적으로는 self-attention 연산을 이용해서 feature를 추출하였습니다. Transformer의 self-attention mechanism은 permutation-invariant한 특성을 가지고 있기 때문에 point cloud 형태의 데이터에 바로 적용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;논문에서는 이런 self-attention layer를 이용해서 U-net 형태의 구조를 가진 네트워크를 형성하였습니다. 그럼 구체적으로 Point Transformer가 어떤 구조로 구성되어 있는지 설명드리겠습니다. (Transformer 또는 self-attention에 대한 이해가 부족하신 분들은 &lt;a href=&quot;https://rauleun.github.io/attention-is-all-you-need&quot;&gt;Attention is all you need&lt;/a&gt; 리뷰를 보고 오시면 좋습니다.)&lt;/p&gt;

&lt;h2 id=&quot;point-transformer-layer&quot;&gt;Point Transformer Layer&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/1-point-transformer-layer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Point Transformer network에서는 point transformer layer를 정의하고, 이를 통해 점들 간의 관계를 고려한 feature vector를 multi-scale로 추출해주었습니다. 이는 vector-based self-attention 연산에 기반을 두고 있는데, &lt;em&gt;scalar-based&lt;/em&gt; 와 &lt;em&gt;vector-based self-attention&lt;/em&gt; 은 attention 값의 형태가 scalar인지 vector인지에 따라 구별됩니다. Vector-based self-attention의 경우에는 attention vector가 feature vector와 같은 크기로 정해져서, elementwise 곱을 통해서 최종 output vector를 얻게 됩니다.&lt;/p&gt;

&lt;p&gt;이러한 self-attention 연산은 기본적으로 &lt;em&gt;set operator&lt;/em&gt; 이기 때문에, set 형태의 구조를 가진 point cloud 연산에 특화되어 있습니다. 논문에서는 이를 활용하여 위와 같은 구조의 point transformer layer를 구성하였습니다. 구조는 생각보다 간단합니다. 한 점과 그 점의 kNN에 해당하는 점들을 input으로 하고, 점들 간 feature vector의 차이와 attention vector를 곱해서 output vector를 추출합니다.&lt;/p&gt;

&lt;p&gt;기본적으로 point transformer layer에서 얻은 output vector는, input point cloud의 점의 개수와 같은 수의 점들로 구성되어 있으며, vector의 차원은 중간에 존재하는 MLP 또는 linear layer의 input/output size에 따라 달라질 수 있습니다. 따라서 여러 layer를 이어붙일 수도 있고 up/down-sampling layer를 이용해서 U-Net 형태의 구조를 가진 network를 구성할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;또한 특이한 점은 기존의 transformer처럼 global하게 모든 점을 input으로 넣는 것이 아니라, 각 점에 대해 인접한 k개의 점만 선택해서 input으로 이용한다는 점입니다. 아마 sub-sampling을 하지 않고 모든 점들을 넣게 되면 input의 길이가 너무 길어져서 attention을 제대로 학습할 수 없고, 한 점에 대한 계산량도 많이지기 때문이 아닐까 생각됩니다. 논문에서는 연관되지 않는 점들에 대한 attention이 noise로 작용해서 성능을 떨어트렸다고 이야기하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/2-point-transformer-block.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 point transformer layer의 위/아래에 vector의 차원 조정을 위한 linear layer와 residual connection을 더해서 위의 그림과 같은 구조의 point transformer block을 형성했습니다. 이 block은 feature vector들의 정보를 통합하는 역할을 하며 추후에 설명드릴 point transformer network의 중요한 구성 요소로 이용됩니다.&lt;/p&gt;

&lt;h2 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h2&gt;

&lt;p&gt;기존 transformer에서는 sequence 내 요소들의 순서를 나타내기 위해 각 embedding vector에 positional encoding vector를 더해주었습니다. Point transformer에서도 마찬가지로 positional encoding vector를 이용했는데, 각 점들의 xyz 좌표값이 점들 간의 위치 관계를 담고 있기 때문에 이를 활용하여 positional encoding vector를 생성하였습니다.&lt;/p&gt;

&lt;p&gt;Positional encoding vector는 2개의 linear layer와 하나의 ReLU로 구성된 MLP를 학습시켜 얻었습니다(learnable positional embedding). 생성된 positional encoding vector는 self-attention 연산 중간에(attention generation 및 feature transformation) 더해줌으로써 점들 간의 위치 관계나 구조가 layer의 output에 반영될 수 있게 구현되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;point-transfomer-network&quot;&gt;Point Transfomer Network&lt;/h2&gt;

&lt;p&gt;Point transformer network는 point cloud classification 및 segmentation task를 수행하기 위해 point transformer block을 활용하여 구성한 network입니다. Network는 point transformer block 및 pooling layer 등을 활용하여 구성되었고, continuous convolution이나 graph convolution 등 기존 연구들에서 feature extraction을 위해 사용했던 연산은 일절 활용되지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/4-point-transformer-network.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전체적인 구조는 위의 그림과 같습니다. Image segmentation 분야에서 주로 사용되는 U-Net 구조와 유사한데, down-sampling과 up-sampling 과정을 통해 5개 scale의 point set에서 정보를 추출하였습니다. 또한 같은 scale의 point set들은 residual connection으로 연결해주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/3-transition-block.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림에서 왼쪽에 보이는 down-sampling layer는 서로 가장 거리가 먼 N개의 점들을 선택해주는 farthest point sampling 알고리즘을 이용했고, 해당 점의 feature vector는 주변 점들의 feature vector들에 대해 local하게 max-pooling 연산을 수행해서 얻었습니다. 오른쪽의 up-sampling layer는 segmentation task의 경우에 점들 별로 feature vector를 얻어야 하기 때문에 점들의 개수를 다시 늘려주기 위해 필요합니다. 이는 더 많은 점들로 구성된 point set에 대한 interpolation과 skip-connection layer로 구성되었습니다.&lt;/p&gt;

&lt;p&gt;Network의 마지막은 linear layer들로 구성된 MLP를 통해 합쳐진 feature vector들의 차원을 적절하게 조정해주었습니다. 특히 segmentation model에서는 각 점들에 대한 embedding vector 별로 classification logit을 추출해서 해당 점의 class 정보를 얻을 수 있게 구성되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;Point Transformer Network의 classification / segmentation version의 성능을 측정하기 위해 다양한 실험을 진행했습니다. ModelNet40, ShapeNet, S3DIS 등의 대표적인 dataset을 이용하여 성능을 평가했습니다. 결과는 대부분 SOTA!를 달성했습니다. 다만 공식적으로 code 및 model이 공개되지 않았고, 때문에 정확한 reproduction 및 평가 결과가 확인되지 않는 것이 아쉬웠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/5-s3dis-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S3DIS dataset을 이용해서 실내 환경에 대한 semantic segmentation 성능을 평가하였습니다. Area 5를 제외한 나머지로 모델을 학습하고, Area 5로 모델을 평가하였습니다. mIOU 및 OA가 70.4%, 90.8%를 기록하며 SOTA의 성능을 보여주었습니다. (기존에 KPConv에서 달성한 SOTA 성능을 뛰어넘었습니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/8-s3dis-vis-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 결과를 시각화한 것인데, 의자의 다리 등 detail한 부분까지도 잘 classification 하는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shape Classification&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/6-modelnet-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ModelNet40 dataset을 이용해서 shape classification 성능을 평가하였습니다. 기존 dataset에서 점들을 2048개 정도로 sub-sampling하여 input으로 활용하였습니다. 마찬가지로 graph convolution 기반의 방법들이나, continuous convolution 기반의 KPConv를 제치고 SOTA의 성능을 달성하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/9-modelnet-vis-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Point transformer network를 통해서 나온 최종 embedding vector의 타당성을 보이기 위해, 임의의 물체의 embedding vector와 거리가 가까운 embedding vector들의 원래 모습을 검색해보았습니다. 위의 그림에서 알 수 있듯 차에 대한 embedding들이 서로 가깝고, 책상에 대학 embedding이 가깝게 잘 mapping되는 것을 확인할 수 있었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-studies&quot;&gt;Ablation studies&lt;/h2&gt;

&lt;p&gt;그 외에 몇 가지 간단한 ablation study를 진행했습니다. 우선 주변 몇 개의 점들까지 neighboring point로 볼 것인가에 대한 &lt;em&gt;k&lt;/em&gt; 값을 바꿔가면서 결과를 비교했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/10-ablation-k.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;k&lt;/em&gt; 값을 16으로 두었을 때 가장 성능이 좋았습니다. &lt;em&gt;k&lt;/em&gt; 를 4나 8 정도로 두면, 주변의 구조나 관계에 대한 정보가 부족해서 학습 성능이 충분히 나오지 않는 것으로 보였습니다. 반면에 &lt;em&gt;k&lt;/em&gt; 를 32나 64 정도로 크게 잡으면, 너무 많은 점들이 들어오게 되어 연관성이 없는 점들이 noise로 작용하고, 결국 정확도를 떨어트렸습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210302-PointTransformer/11-ablation-attention.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 self-attention 연산의 종류에 대해서 실험했습니다. 논문에서는 vector-based attention을 기준으로, 아예 attention을 사용하지 않았을 때(MLP, MLP + pooling)와 scalar-based attention을 사용했을 때의 결과를 비교했습니다. Vector-based self-attention을 이용했을 때 전반적인 성능이 가장 잘 나왔는데, feature vector의 각 channel이 이론적으로는 서로 independent하기 때문에 각각에 맞는 attention weight를 가해줄 수 있을 때 feature 변환이 가장 효율적으로 이뤄지는 것으로 설명했습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Point transformer는 point cloud를 transformer 구조를 이용해서 다룬 초창기 논문인데도 graph convolution, continuous convolution 기반의 논문들을 제치고 SOTA의 성능을 달성하였습니다. Point cloud가 set 형태의 data이기 때문에, 그에 맞는 set operator를 이용했을 때 성능이 월등하게 잘 나오지 않았나 싶습니다. Transformer를 적용한 초창기 연구이기 때문에 추후에 더 뛰어난 model들이 많이 발표되지 않을까 생각됩니다. 개인적인 바람으로는 official code를 공개해주었으면 좋겠습니다.&lt;/p&gt;

&lt;p&gt;그럼 이만 마무리하겠습니다. 궁금하거나 잘못된 점이 있다면 댓글 부탁드리겠습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.09164&quot;&gt;https://arxiv.org/abs/2012.09164&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3d" />
      

      
        <category term="3d" />
      

      
        <summary type="html">원문 : Zhao, Hengshuang, et al. “Point transformer.” arXiv preprint arXiv:2012.09164 (2020).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">KPConv - Flexible and Deformable Convolution for Point Clouds 리뷰</title>
      <link href="https://rauleun.github.io/KPConv" rel="alternate" type="text/html" title="KPConv - Flexible and Deformable Convolution for Point Clouds 리뷰" />
      <published>2021-02-07T09:00:00+09:00</published>
      <updated>2021-02-07T09:00:00+09:00</updated>
      <id>https://rauleun.github.io/KPConv</id>
      <content type="html" xml:base="https://rauleun.github.io/KPConv">&lt;p&gt;원문 : &lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/html/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.html&quot;&gt;Thomas, Hugues, et al. “Kpconv: Flexible and deformable convolution for point clouds.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 2019년 ICCV에서 소개된 &lt;strong&gt;&lt;em&gt;Kpconv: Flexible and deformable convolution for point clouds&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;Kernel point convolution(KPConv)는 3D point cloud 형태의 데이터를 처리하기 위한 여러가지 방법들 중 graph나 3D voxel 등의 형태로 변환하지 않고 point cloud에 직접 convolution을 적용하는 류의 방법입니다. 하나의 convolution kernel은 여러 개의 kernel point들로 구성이 되어있고 각 kernel point마다 연속적인 값을 가지는 kernel weight을 배치하여 주변 점들에 대한 convolution 연산을 수행했습니다. 이 때 kernel point의 개수와 위치를 유동적으로 설정해줌으로써 network capacity를 조절할 수 있고, kernel의 형태를 point cloud의 기하학적 형태에 최적화하여 설정해줄 수 있습니다. KPConv는 3D point cloud classification이나 segmentation 등의 task에서 SOTA의 성능을 기록하였습니다. 그럼 지금부터 KPConv를 파헤쳐보겠습니다!&lt;/p&gt;

&lt;h2 id=&quot;kernel-point-convolution&quot;&gt;Kernel point convolution&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/1-kpconv.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한 점에 대한 kernel point convolution 연산은 일정한 반지름 &lt;em&gt;r&lt;/em&gt; 내부에 있는 주변 점들을 대상으로 합니다. 일반적으로 이웃한 점을 정의할 때 이용하는 kNN과 달리, 반지름을 정의하여 내부에 있는 모든 점들을 이웃한 점으로 정의하게 되면, 점 밀도의 변화에 대해 robust하며 해당 점들에 대한 일정 크기의 kernel을 정의하기가 훨씬 더 수월해집니다. 이렇게 구 형태의 kernel 유효 범위를 정의하게 되면, 범위 내에 특정 개수의 kernel point들을 배치하여 kernel point 별로 kernel weight을 할당합니다. Kernel weight은 각 kernel point에 대해 correlation function과 상수 (&lt;em&gt;W&lt;/em&gt;) 의 곱으로 정의하며, correlation function은 kernel point와 점의 거리가 가까울수록 커지는 linear function을 이용합니다. 예를 들면, 위의 그림에서 각 kernel point에 대해 kernel point와의 거리가 멀어질수록 filter value의 절대값이 작아지는 것을 볼 수 있습니다. Filter value의 부호나 크기는 학습에 의해 정해지는 kernel의 상수값 &lt;em&gt;W&lt;/em&gt;에 따라 달라지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/2-convolution.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 여러 개의 kernel point들로 구성된 각 kernel들을 point cloud의 모든 점에 대입하여 convolution 연산을 해줍니다. Kernel의 개수에 따라서 output feature의 차원이 결정됩니다. 또한 모든 kernel 내 filter value들은 학습에 의해 정해집니다. 이는 위의 그림처럼 image에 적용하는 2D convolution과 정확히 같은 형태입니다. 두 경우 모두 각 Kernel에 대해 convolution 연산(elementwise 곱의 합)을 적용하고, 이 결과를 kernel 별로 쌓아 새로운 feature vector를 형성합니다.&lt;/p&gt;

&lt;p&gt;논문에서는 사전에 정의되어 고정된 kernel point 위치를 사용하는 rigid KPConv와 주변 점들의 분포에 따라 유동적으로 변화하는 kernel point 위치를 사용하는 deformable KPConv의 두 가지 연산을 정의하는데, rigid KPConv의 경우 classification이나 part segmentation 등 간단한 task에서, deformable KPConv의 경우 semantic segmentation과 같은 어려운 task에서 좋은 성능을 보여주었다고 합니다. 그럼 각각에 대해서 설명해보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;rigid-kernel-point-convolution&quot;&gt;Rigid kernel point convolution&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/12-rigid-kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rigid KPConv는 정해진 몇 개의 kernel point에 대해 kernel weight을 배치하여 convolution 연산을 수행합니다. 가장 효율적으로 convolution 연산을 수행하기 위해 repulsive potential과 attractive potential의 합을 최소화하는 최적화 방정식을 정의하였습니다. 이 때 효율적인 convolution 연산이라는 것은, 각 kernel point들의 correlation range가 kernel의 유효 범위를 모두 포함하며 kernel point 간에 겹치는 범위는 최소화하는 것을 의미합니다. Kernel point의 개수를 &lt;em&gt;K&lt;/em&gt; 라고 할 때, 최적화 방정식의 해는 &lt;em&gt;K&lt;/em&gt; 값에 따른 가장 효율적인 kernel point들의 배치입니다. 아래 그림에서는 몇 개의 &lt;em&gt;K&lt;/em&gt; 값에 대한 kernel point의 배치를 보여주고 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;deformable-kernel-point-convolution&quot;&gt;Deformable kernel point convolution&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rigid kernel point convolution을 정의하면 효율적으로 point cloud가 존재하는 공간의 정보를 모을 수 있습니다. 여기에서 좀 더 나아가서 kernel point들의 위치를 학습을 통해 결정할 수 있다면, 주변 점들의 분포를 가장 이상적으로 표현하도록 kernel point들이 배치될 것이므로, 고정되어 있을 때보다 convolution 연산이 가지는 정보의 capacity가 훨씬 커질것입니다. 또한 3D point cloud 형식의 특성상 점들의 분포가 규칙적이지 않으므로, deformable convolution을 적용하게 되면 점들이 존재하지 않는 공간에 대한 무의미한 kernel point 배치를 최소화할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/3-deformable.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 위해 &lt;em&gt;deformable version&lt;/em&gt; 의 network에서는, rigid KPConv 연산을 통해 K개 점에 대한 shift vector를 추출해주는 layer를 추가하였습니다. (Shift vector는 xyz 좌표를 포함하므로 총 3K개의 좌표를 추출합니다.) 이렇게 얻어진 벡터를 local shift로 정의하고 이에 따라 kernel point의 위치를 옮겨줍니다. Network 학습 과정에서는 local shift를 생성하는 rigid KPConv layer와 이를 통해 output feature를 생성하는 deformable KPConv layer를 동시에 학습하였습니다.&lt;/p&gt;

&lt;p&gt;하지만 이렇게 학습을 진행하게 되면 input point와 접점이 없는 kernel point들은 back propagation 과정에서 gradient 값을 잃게 되고, 결국 network는 kernel point를 잃어버리게 됩니다. 이는 point 들의 분포가 일정하지 않은 3D point cloud의 특성 때문인데, 이를 해결하기 위해 가장 가까운 input neighboring point와의 거리를 제한하는 regularization loss와 kernel point 간의 범위 중복을 최소화하기 위한 repulsive loss를 추가하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/13-deformable-kernel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 loss들을 통해 network는 위의 그림처럼 input point cloud의 geometry와 맞는 형태의 local shift를 추출하였습니다. Regularization loss가 없을 때에는 kernel point들이 input point cloud와 멀리 위치하는 경우들이 존재하였습니다. 이러한 경우에는 convolution 연산이 주변 점들의 정보를 잘 통합하지 못하여 특징 벡터의 표현력이 떨어질 수 밖에 없게됩니다. 하지만 regularization loss를 통해 input point들이 존재하는 영역들에 kernel point를 배치한다면, convolution을 통해서 얻은 특징 벡터가 주변 구조를 더 잘 표현하게 됩니다.&lt;/p&gt;

&lt;p&gt;위에서 소개한 두 가지 convolution 연산은 block 형태로 통합되어 전체 network를 구성하는 데에 사용하였습니다. 두 convolution block은 모두 skip-copnnection, batch normalization, leaky ReLU 등을 이용하여 학습의 안정성과 성능을 도모하였습니다. 아래에 구조를 담은 그림을 참고하면, deformable block에서는 KPConv 연산을 통해 3K 크기의 local shift vector를 추출하는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/10-network-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;kernel-point-network&quot;&gt;Kernel Point Network&lt;/h2&gt;

&lt;p&gt;논문에서는 KPConv 연산을 활용하여 network를 구축하였습니다. Network는 U-Net과 유사하게 pooling layer과 upsampling layer를 이용하여 multi-scale의 feature vector를 추출하였습니다. 이 때 pooling을 하면서 점들을 sub-sampling 하는 과정이 필요한데, input point의 density에 independent한 grid subsampling 방법을 이용하였습니다. 이는 점들을 voxel 형태의 grid에 놓은 뒤, 각 grid의 무게중심에 해당하는 점들만을 sampling하는 방법입니다. Pooling 과정은 앞서 설정한 grid의 cell size를 2배씩 키워서 output point의 개수를 줄여나간 뒤에, KPConv를 통해 각 cell 내부의 점들에 대한 feature vector를 통합해주었습니다. 이를 strided KP-Conv라고 부르기도 하였습니다.&lt;/p&gt;

&lt;p&gt;Network parameter는 cross validation을 통해서 결정했습니다. Kernel point의 개수는 15, convolution radius와 kernel point radius는 각각 단위길이의 2.5, 1.5배를 이용하였습니다. Subsampling cell size는 pooling layer마다 2배씩 증가하게끔 설정해주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/11-network-architecture-fig.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Network는 classification 및 segmentation에 대한 각각의 task 별로 하나씩 구성하였습니다. 위의 그림에서 Classification을 위한 network는 KP-CNN, segmentation을 위한 network는 KP-FCNN이라고 부릅니다. KP-FCNN은 KP-CNN의 encoder 부분을 공유하지만, segmentation task의 특징 상 점들 별로 하나 씩의 class output을 도출해야 하기 때문에, nearest upsampling 과정을 통해 점들의 개수를 맞춰준 후에 point-wise feature를 추출했습니다. 반면에 KP-CNN은 encoder를 통해 얻은 feature vector로부터 fully-connected layer를 활용하여 곧바로 class 정보를 추출하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;3D Shape Classification and Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/4-modelnet-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모델은 가장 일반적인 3D classification dataset인 ModelNet40과 part segmentation dataset인 Shapenet을 이용하여 성능을 평가하였습니다. Grid subsampling을 이용했기에 점들의 개수는 data마다 달랐지만, KPConv 과정에서 variable batch size에 대한 normalization 처리를 해주었기 때문에 아무런 문제가 되지 않았습니다. 또한 point cloud의 크기를 바꾸거나, 좌우 반전 및 점들을 일부 제거하는 등의 augmentation 과정을 통해 dataset의 개수를 늘려주었습니다. KPConv를 이용한 두 모델은 각 dataset에 대해 SOTA의 성능을 달성하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3D Scene Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/5-segment-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3D scene segmentation task에서는 indoor, outdoor scene에 대한 Scannet, S3DIS, Semantic3D 등의 dataset을 이용하여 성능을 평가하였습니다. 보통 3D scene dataset의 크기가 굉장히 크기 때문에, 부분적으로 구 형태의 subcloud를 분리하고 그에 대해서만 segmentation 작업을 수행했습니다. Network에 input으로 들어가는 구는 2m 또는 4m의 반지름을 가지게끔 설정해주었습니다. Rigid convolution과 deformable convolution을 이용한 network는 각 task에서 모두 뛰어난 성공을 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/6-segment-result-fig.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;특히 deformable convolution을 활용한 network는 더 큰 capacity를 가지고 있기 때문에 dataset이 더 크고 다양한 data로 구성되어있을 때 더 좋은 성능을 보여주었습니다. 아래의 그림처럼 kernel point의 개수가 줄어들어도, deformable convolution이 충분한 표현력을 가지고 있기 때문에 성능이 크게 줄어들지 않는 것을 확인할 수 있었습니다. 또한 deformable convolution은 class의 종류가 훨씬 다양한 indoor segmentation에서 좋은 성능을 보여주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210207-KPConv/7-kernel-point-iou.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Kernel point convolution은 point cloud에 직접 convolution을 적용하는 방법으로 classification 및 segmentation task에 대한 SOTA의 성능을 보여주었습니다. 논문에서는 kernel point의 위치가 고정된 rigid KPConv와 객체의 형태에 따라 변화하는 deformable KPConv를 제안하여 dataset의 크기나 다양성에 맞게 convolution block을 선택할 수 있게 제안하였습니다. 또한 연산 방법의 성능을 검증하기 위해 다양한 실험을 진행하였고, github에 코드도 잘 정리되어 있으니 좋은 성능의 3D classification 및 segmentation model이 필요하다면 꼭 알아야 할 연구라고 생각됩니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf&quot;&gt;https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/HuguesTHOMAS/KPConv&quot;&gt;https://github.com/HuguesTHOMAS/KPConv&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3d" />
      

      
        <category term="3d" />
      

      
        <summary type="html">원문 : Thomas, Hugues, et al. “Kpconv: Flexible and deformable convolution for point clouds.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs 리뷰</title>
      <link href="https://rauleun.github.io/Superpoint-Graphs" rel="alternate" type="text/html" title="Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs 리뷰" />
      <published>2021-01-29T09:00:00+09:00</published>
      <updated>2021-01-29T09:00:00+09:00</updated>
      <id>https://rauleun.github.io/Superpoint-Graphs</id>
      <content type="html" xml:base="https://rauleun.github.io/Superpoint-Graphs">&lt;p&gt;원문 : &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2018/html/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.html&quot;&gt;Landrieu, Loic, and Martin Simonovsky. “Large-scale point cloud semantic segmentation with superpoint graphs.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 2018년 CVPR에서 소개된 &lt;strong&gt;&lt;em&gt;Large-scale point cloud semantic segmentation with superpoint graphs&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/1-spg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존에 진행된 많은 AI 기반의 point cloud 연구는 좋은 성능을 보여주었지만, input size의 한계 때문에 적은 수의 점들로 구성된 point cloud에 대해서만 적용할 수 있었습니다. Neural network는 몇 백만개 이상의 점들로 구성된 LiDAR scan을 직접 다루기 어려웠고, down-sampling 하는 등의 후처리를 거쳐서 network의 input으로 이용했습니다. 하지만 이는 point cloud의 장점인 물체에 대한 정교한 표현력을 떨어트릴 수밖에 없었습니다. 오늘 소개드릴 연구는 기존의 연구와는 달리 몇 백만개 단위의 점들로 구성된 point cloud를 대상으로 semantic segmentation을 수행하였습니다. 논문에서는 유사한 구조의 점들을 superpoint라는 하나의 점으로 모아서 새로운 그래프(superpoint graph)를 구성했습니다. Superpoint graph(SPG)는 많은 점들로 구성되어 물체 간의 의미론적 관계에 대한 풍부한 정보를 담고 있기 때문에 semantic segmentation 성능을 크게 끌어올릴 수 있었습니다.&lt;/p&gt;

&lt;p&gt;수학적인 내용이 많아 어려웠지만, 최대한 잘 정리해보겠습니다 :)&lt;/p&gt;

&lt;h2 id=&quot;superpoint-graph-spg&quot;&gt;Superpoint Graph (SPG)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/2-spg-process.png&quot; alt=&quot;&quot; /&gt;
SPG는 point cloud로부터 크게 세 단계를 거쳐서 생성됩니다. 우선, 전체 point cloud에서 기하학적으로 비슷한 구조를 가진 점들을 묶어서 작은 여러 개의 point cloud로 분리합니다. 이후, 분리된 각각의 point cloud를 하나의 점으로 변환하는데, 이를 논문에서는 superpoint라고 정의합니다. 이 때 분리된 point cloud 내에 속한 점들의 feature vector를 통합해서 각 superpoint의 embedding vector를 추출합니다. 마지막으로 superpoint 간의 연결 관계를 파악해서 edge를 연결하고, 이를 graph convolution network에 넣어서 segmentation task를 수행합니다. 그럼 각각의 부분에 대해서 자세히 설명하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Geometric Partitioning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Superpoint graph를 구성하기 위해서는 우선 많은 수의 점들로 구성된 point cloud를 작은 점들의 단위로 분리해주어야 합니다. 이 때 단순하고 비슷한 기하학적 모양을 가진 점들을 묶어줌으로써, 점들이 전반적으로 균일한 의미론적 특징을 가지고 실제로도 같은 class에 해당하게끔 설정해주었습니다. 또한 많은 수의 점들이 partitioning 과정을 거쳐야 하기 때문에 계산량 측면에서 효율적인 방법을 이용해주어야 합니다. 논문에서는 global energy model을 통해서 점들을 분리해주었습니다.&lt;/p&gt;

&lt;p&gt;Global enerygy model은 각 점마다 주변 점들의 모양에 대한 특징을 담은 geometric vector를 계산하고, geometric vector가 비슷한 점들 끼리 연결해주는 partitioning 방법입니다. 논문에서는 각 점과 주변 점들에 대한 선형성(linearity), 평면성(planarity), 분산성(scattering), 수직성(verticality)와 높이를 geometric vector로 이용했습니다. 이후 점들의 geometric vector들에 대한 최적화 문제를 푸는 방식으로 connected components를 찾아주었습니다. Point cloud의 크기가 커질 경우에 geometric vector가 nonconvex 하거나 noncontinuous 할 수 있기 때문에, 이러한 경우 graph cut 알고리즘의 하나인 persuit cut을 이용해서 graph를 여러 개로 나눠 크기를 줄여주었습니다. (Graph cut 알고리즘은 분리된 점들 간의 edge distance가 가장 멀게끔 graph를 두 개로 나누는 방법입니다.)&lt;/p&gt;

&lt;p&gt;이렇게 partitioning을 통해서 connected components &lt;em&gt;S = {S1, S2, …, Sk}&lt;/em&gt; 를 얻을 수 있는데, 각 component들은 기하학적으로 간단한 구조를 가지며 component 내의 점들은 같은 구조를 가지게 됩니다. 앞으로는 이를 superpoint라고 부르고, 이를 통해 graph를 형성해서 segmentation을 수행할 것입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Superpoint Graph Construction&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이제 각 superpoint를 node로 하는 하나의 graph를 형성할 것입니다. 일단 superpoint를 형성하기 전, 전체 input point cloud에 대해 인접한 점들을 연결하는 Voronoi adjacency graph를 형성합니다. Superpoint 간의 연결관계는 superpoint 내에 속한 점들이 Voronoi adjacency graph 상에서 연결되었는지에 따라 결정합니다. 예를 들면, &lt;em&gt;S&lt;/em&gt; 와 &lt;em&gt;T&lt;/em&gt; 라는 superpoint가 있고 내부에 각각 &lt;em&gt;s1&lt;/em&gt; 과 &lt;em&gt;t1&lt;/em&gt; 이라는 점이 소속되어 있다고 하면, &lt;em&gt;s1&lt;/em&gt; 과 &lt;em&gt;t1&lt;/em&gt; 이 연결되었다면 &lt;em&gt;S&lt;/em&gt; 와 &lt;em&gt;T&lt;/em&gt; 도 인접한 것으로 간주합니다. 반대로 만약 &lt;em&gt;S&lt;/em&gt; 와 &lt;em&gt;T&lt;/em&gt; 내의 모든 점들 간에 연결 관계가 없다면, &lt;em&gt;S&lt;/em&gt; 와 &lt;em&gt;T&lt;/em&gt; 는 인접하지 않은 것으로 간주합니다. Superpoint 간의 연결된 edge는 Superedge 라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/3-feature.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 superedge feature를 정의해야 합니다. Superedge feature은 superpoint 내 점들 간의 모양이나 크기에 따라 정의합니다. 논문에서는 위의 7개 특징에 대한 13차원의 superedge feature를 통해 superpoint의 특징 및 주변 superpoint 간의 연결 관계를 표현했습니다. 이 때 &lt;em&gt;length, surface, volume&lt;/em&gt; 등의 feature은 x,y,z 좌표값의 covariance matrix에 대한 eigenvalue를 통해 principle component에 대한 크기를 계산하고, 이 eigenvalue의 곱으로(각각 &lt;em&gt;e1&lt;/em&gt; , &lt;em&gt;e1 x e2&lt;/em&gt; , &lt;em&gt;e1 x e2 x e3&lt;/em&gt;) 표현했습니다. 또한 centroid offset 이나 length ratio 등은 graph의 방향에 따라 값이 달라지기 때문에, superpoint graph는 directed graph의 형태를 띄게 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Superpoint Embedding&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Superpoint 내의 점들은 같은 기하학적 특성을 공유하므로, 각 superpoint 별로 embedding vector를 추출할 수 있습니다. 논문에서는 PointNet을 통해서 superpoint의 contextual information을 추출합니다. 이 때 같은 superpoint 내의 점들은 기하학적으로 간단하고 서로 유사하기 때문에, GPU efficiency를 위해 몇 개의 점들만 골라서 embedding을 해도 reliably represent 할 수 있습니다. 따라서 저자는 128개의 점들을 sampling해서 이에 대해서만 embedding vector를 추출했고, 만약 한 superpoint에 128개 이하의 점이 있다면 그대로 사용했습니다. (PointNet은 max-pooling을 통해서 embedding vector를 추출했기 때문에 점의 개수가 바뀌어도 같은 크기의 embedding vector를 얻을 수 있었습니다.) 다만 점의 개수가 40개 이하인 경우에는 embedding을 0으로 두었는데, superpoint의 대표성이 떨어져서 성능 저하를 유발했기 때문입니다. 이렇게 superpoint 내에서 sub-sampling을 하게 되면 메모리 측면에서 효율적일 뿐만아니라 augmentation을 해주는 효과도 있었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Contextual Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Superpoint, superedge를 통해 superpoint graph를 정의했다면, 이제 이를 graph convolution을 통해 segmentation 해야 합니다. 이는 Gated Graph Neural Networks와 Edge-Conditioned Convolution을 이용해서 진행되었습니다. 간단히 설명드리자면 Gated Recurrent Unit(GRU)를 통해 각 superpoint에 대한 embedding을 update했는데, 이 때 이용되는 incoming message vector를 주변 superpoint와의 graph convolution을 통해서 얻었습니다. Graph convolution은 앞서 말한 edge-conditioned convolution 기반의 방법으로 진행되었는데, multi-layer perceptron을 통해 구현된 filter generating network가 edge feature vector로부터 attention weight을 계산하면, 이 weight를 기반으로 주변 점들의 feature vector를 dynamic하게 더해주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Geometric partitioning 단계는 unsupervised하게 학습되었고, superpoint embedding과 contextual segmentation 과정은 supervised하게 동시에 학습되었습니다. Superpoint 내의 점들은 같은 label을 가진 점들로 가정하였기 때문에, 점들의 label들 중 대다수에 해당하는 것으로 지정했습니다. Superpoint graph의 크기가 커서 GPU limit을 뛰어넘는 경우에는, SPG로부터 몇개의 superpoint(512개)만을 sub-sampling해서 크기가 작은 SPG를 형성하고 기존 SPG와 같은 연결 관계(superedge)를 형성한 후에 작은 SPG에 대해서 학습을 진행했습니다. 이렇게 하면 메모리 이슈도 해결할 수 있고, 동시에 data augmentation의 효과도 가져갈 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/5-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 논문들에서와 마찬가지로 가장 일반적인 3D semantic point cloud segmentation 평가 지표인 Semantic3D와 S3DIS dataset을 이용한 성능 평가가 진행되었습니다. 성능은 IOU와 overall accuracy로 측정하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic3D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/4-semantic3d-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Semantic3D는 30억개 이상의 점들로 구성된 가장 큰 LiDAR dataset입니다. SPG는 기존의 SOTA 모델보다 12mIOU points나 높은 SOTA의 성능을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;S3DIS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/6-s3dis-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S3DIS는 실내 환경에 대한 3D RGB point cloud dataset입니다. Area 5를 제외한 나머지 영역에 대해 학습을 진행했고, area 5를 이용해서 평가하였습니다. 전반적인 성능에 대해 SOTA를 기록했지만, white board와 같은 경우 partitioning 과정에서 wall과 제대로 구별이 되지 않으며 평균보다 낮은 IOU를 기록하기도 했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/7-voxelization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 많은 점들에 대한 inference 시간을 줄이기 위해, voxelization 형태의 전처리를 진행하고 결과를 보여주었습니다. 위의 표에서 알 수 있듯이 적절한 크기의 voxel 단위로 점들의 개수를 줄여준 경우에, inference 속도도 빨라지고 정확도도 증가하는 것을 볼 수 있었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ablations&quot;&gt;Ablations&lt;/h2&gt;
&lt;p&gt;SPG 모델에 활용된 여러가지 모듈들의 성능을 검증하기 위해 ablation study가 진행되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210129-superpoint-graphs/8-ablation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우선 superpoint graph에 대해 graph convolution을 통해서 contextual information을 추출하는 것이 성능 향상에 얼마나 많은 영향을 주었는지를 확인했습니다. 기존 모델을 Perfect model이라고 할 때, superpoint graph를 GRU가 아닌 일반적인 PointNet을 활용해 처리하는 모델을 Unary model로 하여 두 성능을 비교했습니다. 결과는 22mIOU points에 가까운 성능 하락이 발생하여 graph convolution 기반의 모델을 통해 contextual information을 파악하는 것이 얼마나 중요한지를 보여주었습니다.&lt;/p&gt;

&lt;p&gt;그 외에 GRU + ECC를 이용하는 대신 CRF-RNN을 이용해보는 등 CRF를 후처리로 하는 모델들과의 비교, GRU 기반의 구조를 수정한 모델에 대한 성능 비교 등을 진행했습니다. 결과는 SPG Perfect 모델의 압승이었습니다. 중요하진 않은 것 같아서 넘어가겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Superpoint Graph는 많은 점들을 partitioning 과정을 통해 효율적으로 graph convolution 연산에 적용하는 방법을 제시했습니다. 기존과 달리 많은 수의 점들로 구성된 dataset으로 학습을 진행할 수 있었고, 디테일하고 많은 정보량 덕분인지 월등히 좋은 성능을 보여주었습니다. 또한 graph congolution 연산에 적용해 contextual information도 알맞게 추출하여 성능 향상에 기여하였습니다. 많은 점들에 대한 학습을 가능하게 한 실험적인 좋은 논문이라고 생각됩니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.09869&quot;&gt;https://arxiv.org/abs/1711.09869&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v51/landrieu16.html&quot;&gt;http://proceedings.mlr.press/v51/landrieu16.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3d" />
      

      
        <category term="3d" />
      

      
        <summary type="html">원문 : Landrieu, Loic, and Martin Simonovsky. “Large-scale point cloud semantic segmentation with superpoint graphs.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Graph Attention Convolution for Point Cloud Semantic Segmentation 리뷰</title>
      <link href="https://rauleun.github.io/GACnet" rel="alternate" type="text/html" title="Graph Attention Convolution for Point Cloud Semantic Segmentation 리뷰" />
      <published>2021-01-27T09:00:00+09:00</published>
      <updated>2021-01-27T09:00:00+09:00</updated>
      <id>https://rauleun.github.io/GACnet</id>
      <content type="html" xml:base="https://rauleun.github.io/GACnet">&lt;p&gt;원문 : &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf&quot;&gt;Wang, Lei, et al. “Graph attention convolution for point cloud semantic segmentation.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 Stanford에서 2019년 CVPR에서 발표한 &lt;strong&gt;&lt;em&gt;Graph attention convolution for point cloud semantic segmentation&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 graph attention convolution 연산을 제안하였는데, 이는 graph convolution kernel에 이웃한 점들과의 연관성에 따라 attention을 가하여 연관성이 높은 점들에 focus된 feature vector를 추출해줄 수 있도록 설계했습니다. 이를 통해 feature contamination을 방지하고 구조적 feature을 잘 추출하여 높은 segmentation 성능을 달성했습니다.
&lt;img src=&quot;assets/images/210127-GACnet/1-example.png&quot; alt=&quot;&quot; /&gt;
3D Point cloud의 semantic segmentation 분야는 PointNet 논문을 기점으로 활발하게 연구되고 있습니다. 최근에는 point cloud를 graph 형태로 표현하여 CNN 기반의 네트워크를 통해 segmentation 하려는 시도들이 많이 있습니다. 하지만 일반적인 graph convolution 연산에 이용하는 convolution kernel은 일정한 값으로 고정되어 있고, 이는 보통 주변 점들의 feature vector를 homogeneous하게 더하여 새로운 feature로 변환해줍니다. 예를 들어, 위의 그림에서 1번 점에 대한 graph convolution output은 1~5번 점들의 feature를 정해진 비율로 더해서 얻게 되고, 이는 1번 점의 클래스와 주변 점들의 클래스(table 또는 chair)를 고려하지 않고 얻은 것이기 때문에 좋은 output feature vector가 아닙니다. 이는 이웃한 점들과의 구조적인 연결 관계를 파악하게 어렵게 만들기 때문에 결과적으로 명확하지 않은 경계선을 만들거나 부분적으로 잘못된 segmentation 영역을 생성하는 등의 문제를 야기합니다.&lt;/p&gt;

&lt;p&gt;이러한 standard convolution kernel의 단점을 해결하기 위해 제안된 것이 graph attention convolution입니다. Graph attention convolution은 convolution 연산이 가해지는 점과 그 주변 점들의 공간적/특징적 정보들을 이용하여 attention weight을 계산하는데, 이는 연관성이 떨어지는 점들이 output feature vector에 관여하는 것을 막아줍니다. 이를 통해 point cloud에 적용하는 convolution kernel의 실질적인 receptive field는 해당하는 점과 주변 점들의 특징에 의해 동적으로 변화하게 됩니다.&lt;/p&gt;

&lt;p&gt;논문에서는 앞서 제안한 graph attention convolution과 graph coarsening 및 graph interpolation 기법을 통해서 graph pyramid network 구조를 구현하였습니다. 그렇다면 graph attention convolution(GAC)부터 GACnet까지 차근차근 살펴보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;graph-attention-convolution&quot;&gt;Graph Attention Convolution&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/2-gac.png&quot; alt=&quot;&quot; /&gt;
Point cloud는 각 점들을 vertex로 하고 인접한 점들을 연결한 선은 edge로 하는 graph 형태의 구조로 변환할 수 있습니다. 이 때 각 점은 &lt;em&gt;f&lt;/em&gt; 차원의 feature vector를 가질 수 있습니다. Graph attention convolution 연산은 우선 기존 &lt;em&gt;f&lt;/em&gt; 차원의 input feature vector를 &lt;em&gt;k&lt;/em&gt; 차원의 output feature vector로 변환하는 것으로 시작합니다. 이 변환은 미분 가능한 형태의 어떠한 함수로도 진행할 수 있는데, 논문에서는 multi-layer perceptron을 이용했습니다.&lt;/p&gt;

&lt;p&gt;이제 각 output feature vector에 attention weight을 elementwise하게 곱해서 convolution 연산을 수행할 것입니다. Attention weight은 output feature vector과 같은 &lt;em&gt;k&lt;/em&gt; 차원의 벡터여야 합니다. GAC에서는 점들 간의 좌표 차이와 feature vector 차이를 이용해서 attention weight을 추출하였습니다. 따라서 attention weight을 계산하는 함수는 &lt;em&gt;xyz&lt;/em&gt; 좌표의 3차원에 input feature vector의 &lt;em&gt;k&lt;/em&gt; 차원을 더한 &lt;em&gt;(3+k)&lt;/em&gt; 차원의 input을 받아서 output feature vector의 &lt;em&gt;k&lt;/em&gt; 차원의 output을 도출합니다. Sharing attention mechanism이라고 부르는 위 변환도 마찬가지로 미분가능한 어떠한 함수로도 진행할 수 있으며 논문에서는 multi-layer perceptron을 이용했습니다.&lt;/p&gt;

&lt;p&gt;Attention weight은 output feature 값의 안정성을 위해 exponential normalization 과정을 거치게 됩니다. 이제 위에서 얻은 &lt;em&gt;k&lt;/em&gt; 차원의 output feature vector에 attention weight을 element-wise 곱한 후 학습가능한 bias 값을 더하여서 GAC의 최종 output vector를 얻습니다.&lt;/p&gt;

&lt;p&gt;GAC 연산은 일반적인 graph attention mechanism과 다르게 attention weight을 얻을 때 점들의 위치 정보와 특징 정보의 차이를 동시에 이용한다는 특징이 있습니다. 이를 통해 점들 사이의 위치적 관계를 고려한 attention 값을 얻을 수 있고, 비슷한 특징을 가진 점들에 더 큰 attention을 가할 수도 있습니다. 또한 feature의 channel별로 attention을 가했기 때문에, 이상적으로 channel-wise independent한 feature vector의 특징을 더 잘 살릴 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;gacnet&quot;&gt;GACnet&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/3-gacnet.png&quot; alt=&quot;&quot; /&gt;
Feature pyramid network(FPN) 기반의 구조는 image segmentation 또는 object detection 분야에서 흔하게 이용되는 유용한 구조입니다. 논문에서는 이 구조를 차용한 graph pyramid network 형태의 구조를 위의 그림과 같이 구현하였습니다. FPN에서 feature extraction를 위해 활용되었던 convolution layer는 graph attention convolution layer가 수행합니다. 앞서 설명드린 것처럼 GAC layer는 점들의 위치 및 특징 정보를 활용하여 graph의 local feature vector를 잘 추출해낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게 얻은 local feature vector는 graph coarsening 및 pooling 과정을 통해 하나의 feature vector로 통합되었습니다. Graph coarsening은 PointNet++에서 소개된 방법을 그대로 이용했는데, furthest point sampling algorithm을 적용하여 여러 개의 중심점들을 선정하고, 각 중심점 주위의 점들을 grouping해서 중심점 단위의 group으로 모아주었습니다. 이후 각 중심점 group마다 max 또는 mean pooling을 적용하여 하나의 feature vector를 도출하였습니다.&lt;/p&gt;

&lt;p&gt;Graph pooling을 거치면 graph의 resolution이 작아지게 되는데, segmentation 등의 task에서는 원래 점들과 같은 개수의 feature map을 output으로 얻어야 하기 때문에 작아진 graph를 interpolation을 통해 up-sampling해주는 과정이 필수적입니다. 이를 feature interpolation layer이라고 부르는데, 마찬가지로 PointNet++에서 제안된 spatial distance 기반의 interpolation 방법을 적용하였습니다. (PointNet++ 리뷰를 참고하시면 도움이 됩니다!) GAC 및 graph pooling layer를 통해 학습된 feature vector는 feature interpolation layer를 통해 서서히 finest scale로 복원됩니다. 논문에서는 더 풍부한 semantic feature를 추출하기 위해 interpolation의 각 단계에서 skip-connection을 통해 down-sampling 되기 전의 feature vector와 합쳐주었습니다. 또한 feature refinement를 위해 최종적으로 학습된 feature vector를 GAC layer에 통과해주었습니다.&lt;/p&gt;

&lt;p&gt;Initial feature vector는 점의 높이, RGB, geo-feature로 구성했습니다. Geo-feature란 finest scale graph에서 이웃한 점들에 대한 covariance matrix의 eigenvalue 값으로 정의하였습니다. 각 구성 특징들의 역할은 후에 ablation study에서 다시 설명드리겠습니다. 초기 그래프는 각 점에 대해 특정 반지름 내에 존재하는 점들 중 random하게 &lt;em&gt;K&lt;/em&gt; 개의 점을 sampling하여 연결하여 생성하였습니다. 이 때 kNN이 아니라 random sampling을 통해서 edge를 구성한 이유는 point cloud의 density가 달라지더라도 이와 무관하게 일정한 범위의 점들에 대해 sampling 할 수 있기 때문입니다. 이렇게 되면 이웃한 두 점이 항상 쌍방으로 연결되지는 않게 되고, graph는 방향성이 있는 edge를 가진 directed graph의 형태가 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;gac-and-conditional-random-fieldcrf&quot;&gt;GAC and Conditional Random Field(CRF)&lt;/h2&gt;
&lt;p&gt;Image segmentation 분야의 유명한 네트워크 모델인 DeepLab에서는 명확한 경계선 구분을 위해 conditional random field(CRF) algoritm을 이용한 후처리 방법을 제안하였습니다. 이는 IOU 및 결과물에 대한 artifact reduction에 많은 공헌을 했고 이후에 이를 기준삼아 많은 segmentation 연구에서 CNN의 output에 CRF를 적용했습니다. CRF는 좌표 및 RGB 값이 비슷한 점들을 같은 label로 일치시켜 segmentation 경계선을 깔끔하고 detail하게 만들어주었습니다.&lt;/p&gt;

&lt;p&gt;앞서 설명드린 GAC도 CRF와 마찬가지로 위치 및 특징 정보를 활용하여 feature vector를 추출해주는 역할을 합니다. 특히 input feature vector 간의 차이를 이용하여 attention weight을 계산하기 때문에 비슷한 input feature를 가지는 점들에 대해 일관적인 output feature를 도출합니다. 이는 CRF와 정확히 같은 특징을 공유하기 때문에 GACnet에서는 더 이상 CRF 과정을 거치지 않아도 됩니다. 실제로 CRF model을 RNN 형식으로 구현하여 후처리하는 방법은 CNN과 독립적으로 진행되기 때문에 end-to-end로 결과를 얻을 수 없게 되는 등의 번거로움이 발생합니다. 또한 input feature의 유사도를 기반으로 layer를 거칠 때마다 더 semantic한 feature vector를 추출할 수 있기 때문에 segmentation 성능도 훨씬 더 잘나오게 되는 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;논문에서는 GAC 및 GACnet을 검증하기 위해 S3DIS와 Semantic3D라는 두 가지 3D semantic segmentation benchmark를 이용했습니다. GACnet의 성능은 IOU 및 overall accuracy를 이용하여 평가했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;S3DIS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/7-gacnet-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S3DIS는 6개의 실내 공간에 대한 3D RGB point cloud dataset입니다. 각 점은 13개 카테고리로 labeling 되어 있습니다. 논문에서는 6개의 공간 중 5번 공간을 testing set으로, 나머지 공간을 training set으로 하여 네트워크를 학습 및 평가하였습니다. 이렇게 이전에 전혀 보지 못한 공간에 대해 test하게 되면 task가 어려워지기 때문에 초기에는 성능이 잘 안나올 수 있지만, 네트워크 모델이 잘 일반화 되었는지를 평가하기에 좋습니다. 각 공간을 방 별로 먼저 분리하고, 각 방을 &lt;em&gt;1.2m x 1.2m&lt;/em&gt; 의 블럭으로 분리하여, 각 블럭 별로 random sampling된 4096 개의 점을 하나의 dataset으로 이용하였습니다. 방은 분리하는 과정에서 가장자리 &lt;em&gt;0.1m&lt;/em&gt; 씩은 물체의 일부밖에 보이지 않을 수 있기 때문에 buffer area로 설정하여 학습 및 loss 계산에 포함하지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/4-s3dis-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과는 위와 같습니다. GACnet은 대부분의 클래스에서 SOTA의 성능을 보여주었습니다. 특히나 GACnet은 S3DIS dataset에서 검출하기 어려운 벽 위의 창문이나 보드와 같은 클래스들도 잘 검출하였는데, 이는 GAC가 공간 정보 뿐만 아니라 RGB 값을 포함한 특징 정보도 활용하기 때문에 공간적 구조가 명확하지 않더라도 구별을 잘 해낸 것으로 생각합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Semantic3D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Semantic3D는 LiDAR에서 얻은 약 40억개 이상의 점들로 구성된 3D point cloud dataset입니다. 점들에 대한 feature는 RGB와 intensity 값으로 구성되어 있고, 각 점은 8개의 카테고리 중 하나로 labeling 되어있습니다. 또한 Semantic3D는 외부 풍경에 대한 dataset이기 때문에 일반적으로 물체의 크기가 큽니다. 따라서 논문에서는 공간을 &lt;em&gt;4m x 4m&lt;/em&gt;의 블럭으로 구분하였고, 블럭 별로 4096 개의 점을 sampling하여 하나의 dataset으로 이용하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/210127-GACnet/5-semantic-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GACnet은 Semantic3D dataset에서도 SOTA의 성능을 보여주었습니다. 특히 Semantic3D에서는 차나 건물 등의 물체들이 가려져 있는 경우가 많은데, GACnet은 구조적인 feature learning 덕분인지 가려져 있는 물체에 대해서도 좋은 성능을 보여주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h2&gt;
&lt;p&gt;GAC 및 GACnet을 자세히 분석/검증하기 위해 논문에서는 몇 가지 항목에 대한 ablation study를 진행했습니다.
&lt;img src=&quot;assets/images/210127-GACnet/6-ablation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GAC 연산의 효율성&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우선 GAC 연산의 효율성을 검증하기 위해 저자는 GACnet의 GAC 부분을 PointNet에서 활용한 max pooling layer로 치환해서 성능을 측정하고 비교했습니다. Max-pooling 함수는 특징을 통합하여 분류하는 object classification 성능은 좋았지만, 정교한 경계선 설정이 중요한 segmentation task에서는 local한 정보들을 많이 소실하여 비교적 성능이 좋지 못했습니다. 반면에 GAC는 두 task 모두 뛰어난 성능을 보여주었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위치 및 특징 정보들의 성능 향상 기여&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GAC에서 attention weight은 위치 좌표 값의 차이와 feature vector 간의 차이를 input으로 계산하였습니다. 저자는 각각을 하나씩 제거하여 얻은 attention weight을 이용하여 성능을 측정해보았는데, 두 경우 모두 성능 하락이 발생했습니다. 특히 점의 높이, RGB, geo-feature 등으로 구성된 feature vector를 제거하였을 때, 명확한 경계선 설정에 어려움을 겪었다고 이야기합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CRF-RNN 과의 비교&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GAC 모듈을 CRF-RNN으로 치환했을 때의 성능도 비교하였습니다. 두 경우 모두 유사한 성능을 보여주었는데, CRF와 GAC가 특징 벡터의 유사도를 이용하여 label을 도출하는 근본적으로 같은 성질을 공유하기 때문으로 보입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;초기 feature vector 구성에 따른 효과&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GAC의 input으로 활용하는 초기 feature vector는 점의 높이, RGB, geo-feature로 구성하였습니다. 각각 성능에 어떤 영향을 주는지를 분석하기 위해, 특정 component를 제외하고 성능을 측정하였습니다. 결과는 전반적인 성능 하락이 발생하여 각 component가 성능에 주요한 역할을 하는 것을 할 수 있었습니다. 
&lt;img src=&quot;assets/images/210127-GACnet/8-geo-feature.png&quot; alt=&quot;&quot; /&gt;
이 중 흥미로운 결과가 있었는데, geo-feature가 training accuracy에는 별다른 영향을 주지 않았는데 test accuracy에는 비교적 큰 성능 향상에 기여했습니다. 이는 covariance matrix의 eigenvalue로부터 추출한 geo-feature 값이 전반적인 low-level feature에 대한 정보를 포함하고 있고, 결과적으로 네트워크의 일반화에 기여한 것으로 보입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;데이터 소실 및 잡음에 대한 강도 실험
&lt;img src=&quot;assets/images/210127-GACnet/9-robustness-test.png&quot; alt=&quot;&quot; /&gt;
마지막으로 데이터의 일부가 소실되거나 Gaussian noise가 추가되었을 때 classification 성능이 어떻게 변화하는지를 실험하였습니다. GAC 모듈을 max-pooling 함수로 치환하여 비교하였는데, 두 경우 모두 GACnet이 훨씬 더 robust한 결과를 보여주었습니다. 아마 max-pooling 함수를 이용하게 되면, noise 값이 크게 튀는 경우에 이를 대표적인 feature 값으로 인식되어 classification 정확도를 낮추지 않았을까 생각됩니다. 반면에 GAC는 상대적 위치정보 뿐만아니라 여러 성분들(RGB 등)로 구성된 특징 정보를 활용하기 때문에 (정보량이 많기 때문에), 방해 요소들이 있어도 robust한 결과를 보여주는 것 같습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Graph attention convolution은 주변 점들에 따라 attention weight을 다르게 가하는 방식을 통해 dynamic한 receptive field를 가지는 convolution kernel를 생성할 수 있었고, 이를 통해 feature vector를 추출했습니다. Edge convolution 등의 방법과 유사하면서도, 점들의 특징 벡터의 차이를 이용하여 비슷한 label을 가진 점들을 확실하게 일치시켜 주는 (CRF가 기존에 해주던 방식의) 연산 방식이 SOTA의 성능을 만들 수 있지 않았나 생각이 듭니다. 긴 글 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf&quot;&gt;https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yanx27/GACNet&quot;&gt;https://github.com/yanx27/GACNet&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3d" />
      

      
        <category term="3d" />
      

      
        <summary type="html">원문 : Wang, Lei, et al. “Graph attention convolution for point cloud semantic segmentation.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PointNet++ - Deep Hierarchical Feature Learning on Point Sets in a Metric Space 리뷰</title>
      <link href="https://rauleun.github.io/PointNet++" rel="alternate" type="text/html" title="PointNet++ - Deep Hierarchical Feature Learning on Point Sets in a Metric Space 리뷰" />
      <published>2021-01-21T09:00:00+09:00</published>
      <updated>2021-01-21T09:00:00+09:00</updated>
      <id>https://rauleun.github.io/PointNet++</id>
      <content type="html" xml:base="https://rauleun.github.io/PointNet++">&lt;p&gt;원문 : &lt;a href=&quot;https://papers.nips.cc/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf&quot;&gt;Qi, Charles Ruizhongtai, et al. “Pointnet++: Deep hierarchical feature learning on point sets in a metric space.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 Stanford에서 2017년 NIPS에 발표한 &lt;strong&gt;&lt;em&gt;Pointnet++: Deep hierarchical feature learning on point sets in a metric space&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Point cloud 형식의 데이터를 Deep learning 분야에 적용시킨 선구적인 논문인 PointNet의 후속편으로, local한 특징을 잡아내지 못하는 기존의 PointNet을 보완하여 classification 및 segmentation 성능을 크게 끌어올렸습니다.&lt;/p&gt;

&lt;p&gt;그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;pointnet&quot;&gt;PointNet&lt;/h2&gt;
&lt;p&gt;PointNet++에 대하여 설명드리기 전에, 전편인 PointNet에 관해서 간단히 짚고 넘어가겠습니다. (자세한 설명은 PointNet 논문 리뷰를 참고해주세요.) Point cloud는 3차원 공간 내의 물체를 표현하기 위한 데이터의 한 형식으로 각 점들에 대한 좌표값으로 구성되어 있습니다. Point cloud는 sparse한 점들의 집합이기 때문에 계산량 및 메모리 차원에서 효율적이지만, 순서가 없는 집합 형태의 데이터이기 때문에 neural network가 이를 처리하려면 permutation-invariant한 특성을 가진 layer를 포함하고 있어야 했습니다. PointNet에서는 symmetric function 중의 하나인 max-pooling 함수를 활용하여 point cloud 데이터에 대한 global한 feature vector를 추출하였습니다. 하지만 max-pooling 함수의 특성상 최대값을 제외한 local한 정보들을 소실될 수밖에 없고 이는 정교한 경계선 설정이 중요한 segmentation task 등에서 성능 저하의 요인이 되었습니다. PointNet++에서는 몇 가지 구조를 제안하여 local한 feature vector를 추출하였습니다.&lt;/p&gt;

&lt;p&gt;Local한 특징을 추출하는 것은 중요합니다. CNN에서도 다양한 receptive field를 가지는 convolution kernel을 활용함으로써 local한 특징을 추출하였고, 이는 2D image들에 대한 폭발적인 성능 향상을 이뤄냈습니다. PointNet++는 계층적 구조의 neural network를 활용하여 다양한 scale의 local feature를 추출하였고, 이를 통합하여 SOTA의 classification 및 segmentation 성능을 기록할 수 있었습니다. 그럼 PointNet++에 대해 좀 더 자세히 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;pointnet-1&quot;&gt;PointNet++&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210121-pointnet++/1-pointnet++.png&quot; alt=&quot;&quot; /&gt;
PointNet++는 &lt;strong&gt;Set abstraction layer&lt;/strong&gt;과 &lt;strong&gt;Density adaptive layer&lt;/strong&gt;로 구성되어 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;set-abstraction&quot;&gt;Set Abstraction&lt;/h2&gt;
&lt;p&gt;PointNet++에서는 set abstraction이라고 부르는 과정을 통해 point cloud의 local feature를 추출하였습니다. Set abstraction을 거치면, point 들의 집합은 이전보다 적은 수의 점들로 구성된 point cloud로 추상화되며, semantic한 정보를 담게 됩니다.&lt;/p&gt;

&lt;p&gt;Set abstraction은 세 단계로 구분할 수 있습니다. 우선 sampling 단계에서는 point cloud의 local한 부분집합의 중심에 해당하는 중요한 몇 개의 점들을 선택합니다. 이후 grouping 단계에서는 sampling 단계에서 찾은 중요한 몇 개의 점들과 이웃한 점들을 찾고 하나의 집합으로 구성합니다. 마지막으로 pointnet 단계에서는 local한 점들의 집합에 대한 패턴을 encoding하여 feature vector를 추출합니다.&lt;/p&gt;

&lt;p&gt;예를 들어 Point cloud가 &lt;em&gt;N&lt;/em&gt; 개의 점들로 구성되어 있고 각 점은 &lt;em&gt;d&lt;/em&gt; 차원의 좌표를 가지며 &lt;em&gt;C&lt;/em&gt; 차원의 feature vector를 가진다고 가정하겠습니다. Set abstraction 과정을 거치면 &lt;em&gt;N&lt;/em&gt; x &lt;em&gt;(d+C)&lt;/em&gt; 크기의 input 행렬이 &lt;em&gt;N’&lt;/em&gt; x &lt;em&gt;(d+C’)&lt;/em&gt; 크기의 행렬로 변환되어 출력됩니다. 이 때 &lt;em&gt;N’&lt;/em&gt; 는 subsampling된 점들의 개수이고, &lt;em&gt;C’&lt;/em&gt; 는 중심점 행렬의 feature vector의 차원입니다.&lt;/p&gt;

&lt;p&gt;Sampling 단계에서는 &lt;em&gt;N&lt;/em&gt; 개의 점들 중 &lt;em&gt;N’&lt;/em&gt; 개의 점을 중심점으로 선택합니다. 이 &lt;em&gt;N’&lt;/em&gt; 개의 점들은 전체 점들 중 나머지 점들과의 거리가 가장 먼 점들로 구성됩니다. 이때 이 거리라는 개념은 Uclidian distance 일 수도 있고 그 외 다른 거리 관련 metric일 수도 있습니다. 논문에서는 이 거리가 가장 먼 점들의 집합을 Farthest point sampling (FPS) 알고리즘을 반복적으로 적용해 얻었다고 설명하였습니다. 또한, 이렇게 서로 최대한 떨어져 있는 점들을 선택하게 되면 random sampling을 통해서 점들을 선택하는 것보다 더 일반적이고 전체적인 범위의 point들을 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;Grouping 단계에서는 sampling 단계에서 추출한 &lt;em&gt;N’&lt;/em&gt; 개 중심점들의 주변 점들을 선택하여 local한 영역으로 묶어줍니다. 각 중심점과 묶이는 주변 점들의 개수는 영역마다 다를 수 있는데, 후에 설명드릴 pointnet 단계에서 점들의 개수와 상관 없이 같은 크기의 feature vector로 변환해주기 때문입니다. 또한 이웃한 점들을 선택하는 방식도 두 가지로 나뉠 수 있습니다. 우선, ball query 방식은 반지름 &lt;em&gt;r&lt;/em&gt; 을 정해 이 반지름 내에 있는 점들을 모두 이웃한 점으로 선택하는 방식입니다. 또한, kNN 방식은 정해진 &lt;em&gt;K&lt;/em&gt; 라는 개수의 가장 가까운 점들을 이웃한 점으로 선택하는 방식입니다. 논문에서는 ball query 방식을 선택했는데, kNN과 비교했을 때 정해진 크기의 지역을 확실하게 표현할 수 있기 때문입니다. 이는 일정하게 sampling되지 않은 point cloud 데이터에서 영역 별로 범위가 달라지는 문제를 방지할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;마지막으로 pointnet 단계에서는 각 local한 영역에 대해 특징 정보를 encoding한 하나의 feature vector를 추출할 수 있습니다. 각 중심점(총 &lt;em&gt;N’&lt;/em&gt; 개)별로 생성된 &lt;em&gt;K&lt;/em&gt; 개의 이웃한 점들에 대한 &lt;em&gt;(d+C)&lt;/em&gt; 크기의 feature vector를 행렬로 표현하면 &lt;em&gt;N’&lt;/em&gt; x &lt;em&gt;K&lt;/em&gt; x &lt;em&gt;(d+C’)&lt;/em&gt; 크기의 행렬이 되는데, 이 행렬이 pointnet 단계를 거치면 &lt;em&gt;N’&lt;/em&gt;x&lt;em&gt;(d+C’)&lt;/em&gt; 크기의 feature vector로 변환됩니다. 이 때, 점들의 좌표값은 중심점과의 상대적인 거리의 차이에 대한 값이 들어가게 되는데, 이를 통해 점들 사이의 위치적 관계에 대한 정보를 잡아낼 수 있기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;density-adaptive-feature-learning&quot;&gt;Density Adaptive Feature Learning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210121-pointnet++/2-nonuniform-pointcloud.png&quot; alt=&quot;&quot; /&gt;
Point cloud 데이터는 위의 그림처럼 non-uniform한 밀도 분포를 가지는 것이 대부분입니다. 이렇게 일정하지 않은 점들의 분포는 point cloud의 특징에 대한 학습을 어렵게 만드는데, density가 높은 point cloud에 대해 학습한 네트워크는 sparse한 점들의 point cloud를 만났을 때 정보가 부족하다고 느끼고 특징을 제대로 표현하지 못하기 때문입니다. (그 반대의 경우도 마찬가지입니다.) 논문에서는 이를 해결하기 위해 point cloud의 sampling density를 변화시키며 학습하였습니다. 또한, 다양한 scale의 point cloud에서 feature vector를 추출하는 multi-scale feature extraction 구조를 활용하였는데, 저자는 이를 &lt;em&gt;density adaptive layer&lt;/em&gt; 이라고 표현했습니다. 다양한 크기의 density를 가지는 point cloud를 학습하는 두 가지 방법에 대해 소개하겠습니다.
&lt;img src=&quot;assets/images/210121-pointnet++/3-grouping.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-scale grouping (MSG)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우선 grouping 단계를 다양한 scale로 여러 번 적용하여 하나의 중심점에 대해 여러 scale의 point group을 얻는 방법이 있습니다. 각 point group에서 각각 추출한 feature vector를 이어붙이면(concatenate), multi-scale feature vector를 얻을 수 있게 됩니다. 이 때, 각 point group은 임의의 dropout ratio를 선택하여 그 비율에 맞게 random하게 down-sampling(dropout) 하여 각 point group마다 서로 다른 scale로 균일하지 않은 density를 가지게끔 변환해주었습니다. 이러한 과정을 거치면 다양한 sparsity와 서로 다른 uniformity를 가지는 점들을 얻을 수 있습니다. 하지만 MSG는 각 중심점들과 그 이웃한 점들이 모두 pointnet을 거쳐야 하므로 게산량 차원에서 아주 비효율적이고 time-consuming하다는 단점이 있습니다. 논문에서는 이를 보완하기 위해 multi-resolution grouping을 제안하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-resolution grouping (MRG)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MRG는 MSG의 단점을 보완한 grouping 방식입니다. MRG는 서로 다른 scale로 얻은 두 feature vector를 이어붙여서(concatenate) multi-scale feature vector를 얻습니다. 이 때, 첫 번째 vector는 local group에 해당하는 점들 전체에 대해 pointnet 단계를 거쳐서 얻고, 두 번째 vector는 local group에 대해 그보다 한 단계 아래의 sub-region에서 얻은 feature를 종합하여 얻습니다. 저자는, 만약 input으로 들어오는 point cloud의 density가 낮다면, 첫 번째 vector에 의해 전반적인 특징에 대한 정보를 추출할 수 있고, density가 높다면, sub-region에 대한 feature가 높은 resolution 더 디테일한 특징 정보를 제공할 수 있다고 이야기합니다. 따라서 두 vector를 모두 이용하게 되면 여러 density의 point cloud에 대해서 모두 대응할 수 있으며, 계산량 측면에서도 효율적이라고 말합니다.&lt;/p&gt;

&lt;h2 id=&quot;point-feature-propagation&quot;&gt;Point Feature Propagation&lt;/h2&gt;
&lt;p&gt;Set abstracion layer를 거치게 되면, sampling 단계에 의해 point cloud의 크기가 줄어들게 됩니다. 이렇게 얻은 feature vector를 segmentation task에 활용하려면 다시 원래의 크기로 복원해주어야 합니다. 복원해주지 않고 set abstraction을 하기 위해 모든 점들을 중심점으로 지정해서 feature aggregation을 해주는 방법도 있지만, 이는 computation cost가 너무 많이 들기 때문에 논문에서는 point cloud를 down-sampling하고 다시 interpolation 기반의 방법을 통해 up-sampling하는 방식을 제안하고 있습니다.&lt;/p&gt;

&lt;p&gt;구체적으로, 이전 점들에 대한 feature vector로부터 (&lt;em&gt;1/거리값)&lt;/em&gt; 으로 weighting을 가해서 interpolation하는 방법을 이용하였습니다. 또한 down-sampling 되기 전의 feature vector를 skip-connection을 통해 concatenate하여 부족할 수도 있는 정보량을 보충해주었습니다. Interpolation 과정은 원래 point의 개수로 맞춰질 때까지 반복해주었고, 결과로 얻은 feature vector를 통해서 segmentation task를 수행해주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;PointNet++는 MNIST(2D Object), ModelNet40(3D Object), ScanNet(3D Scene) 등의 다양한 데이터셋에 대한 evaluation 과정을 통해 classifiaction 및 segmentation task에서 성능을 증명하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST
&lt;img src=&quot;assets/images/210121-pointnet++/4-mnist-result.png&quot; alt=&quot;&quot; /&gt;
MNIST 데이터셋은 손글씨 숫자에 대한 60,000개 이상의 image입니다. PointNet++는 2D image의 좌표를 2D point cloud 형태로 변환하여 input으로 사용했는데, 기존 PointNet과 비교했을 때 error rate이 30% 이상 감소하는 등의 성능 향상을 보여주었습니다. 또한 기존 CNN 기반의 모델들과 비교했을 때도 더 좋은 성능을 보이기도 했습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ModelNet40
&lt;img src=&quot;assets/images/210121-pointnet++/5-modelnet-result.png&quot; alt=&quot;&quot; /&gt;
ModelNet40 데이터셋은 40개의 클래스에 대한 CAD 모델입니다. CAD 모델은 3D Mesh 형태를 띄고 있기 때문에, 논문에서는 mesh의 표면을 sampling하여 3D point cloud 형태로 변환한 후에 PointNet++의 input으로 사용하였습니다. 또한 모델 구조는 3단계의 계층 단계에 3개의 Fully connected layer를 이어붙여서 구성했으며, 모든 point들의 좌표는 반지름 1의 구 안에 들어오게끔 하여 normalization을 해주었습니다. PointNet++는 3D classification task에서도 기존의 SOTA 모델로 알려진 MVCNN의 성능을 크게 뛰어넘었습니다.
&lt;img src=&quot;assets/images/210121-pointnet++/6-grouping-result.png&quot; alt=&quot;&quot; /&gt;
또한 ModelNet 데이터셋을 활용하여 Density adaptive layer의 성능을 측정하는 ablation study를 진행했습니다. 위의 그림을 보면, 1024개의 점에 대한 point cloud로부터 random하게 점을 지워서 512, 256, 128개로 down-sampling하였습니다. 점의 개수가 다른 point cloud를 PointNet 및 PointNet++의 input으로 넣어주었을 때, 앞서 설명드린 density aadaptive layer (MSG 또는 MRG)를 적용시켜 multi-scale로 학습한 모델들은 점의 개수가 달라져도 robust한 결과를 보여주었습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ScanNet&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ScanNet 데이터셋은 실내 환경에 대한 1500개 이상의 3D point cloud 데이터로 구성되어 있습니다. 각각의 점들에는 해당 점이 어떤 물체에 속해 있는지에 대한 segmentation label이 달려 있습니다. PointNet++는 ScanNet에 대한 segmentation task에서도 아주 뛰어난 결과를 보여주었습니다. 계층적 구조를 통한 local한 feature 학습이 다양한 scale의 scene을 이해하는데 중요하다고 해석할 수 있을 것 같습니다. 
&lt;img src=&quot;assets/images/210121-pointnet++/8-scannet-result.png&quot; alt=&quot;&quot; /&gt;
ScanNet에서도 sampling density가 달라졌을 때 robust한 결과를 도출할 수 있는지 실험하였습니다. ScanNet 데이터를 non-uniform한 sampling density로 줄여서 학습한 뒤에 결과를 확인해보았습니다. 앞선 ModelNet40을 이용한 실험 결과와 마찬가지로, MRG 네트워크가 Single-scale grouping을 통해 학습한 네트워크보다 다양한 density의 데이터들에 대해 훨씬 더 좋은 결과를 보여주었습니다. 
&lt;img src=&quot;assets/images/210121-pointnet++/7-grouping-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;PointNet과 더불어 PointNet++는 이전에 classification이나 detection 분야에서 활용되었던 multi-scale feature learning 기법을 point cloud 데이터에 적용시켜 급격한 성능 향상을 만들었습니다. 또한 Graph의 크기를 줄이는 graph coarsening이나 다시 graph의 크기를 키우는 point feature propagation 등 다양한 개념이 제시된 논문이라 의미가 크다고 생각합니다. 다음 번에는 이를 기반으로 attention 메커니즘을 적용시킨 논문을 한번 리뷰해보겠습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://stanford.edu/~rqi/pointnet2/&quot;&gt;http://stanford.edu/~rqi/pointnet2/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/charlesq34/pointnet2&quot;&gt;https://github.com/charlesq34/pointnet2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3d" />
      

      
        <category term="3d" />
      

      
        <summary type="html">원문 : Qi, Charles Ruizhongtai, et al. “Pointnet++: Deep hierarchical feature learning on point sets in a metric space.” Advances in neural information processing systems. 2017.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs 리뷰</title>
      <link href="https://rauleun.github.io/Edge-Conditioned-Convolution" rel="alternate" type="text/html" title="Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs 리뷰" />
      <published>2021-01-20T09:00:00+09:00</published>
      <updated>2021-01-20T09:00:00+09:00</updated>
      <id>https://rauleun.github.io/Edge-Conditioned-Convolution</id>
      <content type="html" xml:base="https://rauleun.github.io/Edge-Conditioned-Convolution">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/abs/1704.02901&quot;&gt;Simonovsky, Martin, and Nikos Komodakis. “Dynamic edge-conditioned filters in convolutional neural networks on graphs.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 ENPC에서 2017년 CVPR에 발표한 &lt;strong&gt;&lt;em&gt;Dynamic edge-conditioned filters in convolutional neural networks on graphs&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Point cloud 등의 graph 구조로 표현 가능한 데이터에서 convolution 형태의 연산을 통해 feature vector를 추출하는 하나의 방법에 대해 제시하고 있습니다.&lt;/p&gt;

&lt;p&gt;그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;graph-convolution&quot;&gt;Graph Convolution&lt;/h2&gt;
&lt;p&gt;Convolution(합성곱) 연산은 2D image에 대한 내재된 feature 벡터를 잘 추출해줌으로써 classification, segmentation 등의 여러 task들에 대한 성능을 폭발적으로 끌어올렸습니다. 학계에서는 2D image 등의 Uclidian 공간에 대한 데이터 뿐만 아니라 graph 등 non-Uclidian 공간 상의 데이터도 convolution 연산을 통해 처리할 수 있지 않을까에 대해 많이 고민했습니다.&lt;/p&gt;

&lt;p&gt;이러한 고민의 결과로 인접한 node 간의 특징 벡터를 모아주는 graph convolution 연산을 발견하였고, 이는 graph 형태로 표현 가능한 3D modeling, biological molecule, social network 등의 데이터에 활용되었습니다. 하지만 기존의 방법대로는 한 점과 인접한 점들의 feature 벡터를 균일하게(homogeneous) 더해줄 수 밖에 없었고, 이는 점들 간의 연관성의 정도를 전혀 고려하지 않은 연산이었습니다. (일반적인 discrete convolution으로 생각해보면, uniform function만을 filter로 사용하는 형태였습니다.)&lt;/p&gt;

&lt;p&gt;이를 보완하기 위해 논문에서는 점과 점을 연결한 edge에 점들 간의 관계를 나타내는 label을 부여해서 weight parameter처럼 활용하였습니다. 이렇게 edge label을 이용하여 graph convolution을 하게 되면, 단순히 점들을 homogeneous하게 더하는 것이 아니라 점들 간의 연관성을 고려하여 더해주기 때문에 주변 점들과의 관계가 반영된 feature 벡터를 추출할 수 있다고 합니다. 또한 이 feature vector를 이용해서 graph 또는 vertex 단위로 classification을 했을 때 점들의 관계 및 분포에 대한 정보가 녹아있어 정확도가 향상되었다고 합니다. 본문에서는 이렇게 edge label을 이용한 convolution 연산을 edge-conditioned convolution(ECC)라고 부릅니다.&lt;/p&gt;

&lt;h2 id=&quot;point-cloud&quot;&gt;Point cloud&lt;/h2&gt;
&lt;p&gt;Point cloud는 3D 물체를 표현하기 위한 형식 중의 하나입니다. 원래 point cloud는 일정한 형식이 없는 집합 형태의 데이터이기 때문에 neural network의 input으로 활용하기 어려웠습니다. 하지만 특정 점에 대해 정의된 거리(r) 내에 있는 점들 중 K-nearest neighbors 알고리즘을 적용해서 인접한 점들을 찾은 뒤, 서로 연결해서 graph 형태로 변환하게 되면 graph convolution을 적용할 수 있습니다. 따라서 논문에서는 여러 종류의 graph 데이터들 중 주로 point cloud를 graph로 변환한 데이터를 활용했습니다. Point cloud는 물체나 풍경을 3D로 표현했다는 특성상 graph 또는 vertex 단위로 label이 존재하기 때문에 graph convolution의 성능을 측정하기에 적절하기도 합니다. 또한 최근에는 RGB-D 카메라나 LiDaR의 출력 형식으로 사용되기 때문에 범용적이기도 합니다.&lt;/p&gt;

&lt;h2 id=&quot;edge-conditioned-convolution-ecc&quot;&gt;Edge-conditioned Convolution (ECC)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210120-ECC/1-edge-convolution.png&quot; alt=&quot;&quot; /&gt;
Edge-conditioned convolution 연산 과정은 그리 어렵지 않습니다. Point cloud 데이터를 graph 형태로 변환하여 &lt;em&gt;n&lt;/em&gt;개의 node와 &lt;em&gt;m&lt;/em&gt;개의 edge를 얻었다고 가정하겠습니다. 이 때 node의 feature vector가 &lt;em&gt;l&lt;/em&gt; 차원, edge의 feature vector가 &lt;em&gt;s&lt;/em&gt; 차원이라고 하면 우리는 edge의 feature vector로부터 node feature vector의 transformation matrix (&lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt; 차원)를 얻는 미분 가능한 함수를 정의하고자 합니다. 논문에서는 가장 간단한 multi-layer perceptron을 활용하였는데, input이 &lt;em&gt;s&lt;/em&gt;차원 &amp;amp; output이 &lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt; 차원인 linear layer입니다. 이렇게 각 node마다 &lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt; 차원의 “edge-specific weights”를 얻을 수 있고, 이는 edge feature vector에서 추출한 행렬이기 때문에 점들 사이의 관계를 담고 있습니다. 이렇게 얻은 weight을 이웃한 각 node feature vector에 곱해주고 normalize하면 edge-conditioned convolution을 거친 해당 점의 feature vector를 얻을 수 있습니다.
&lt;img src=&quot;assets/images/210120-ECC/2-one-hot-edge.png&quot; alt=&quot;&quot; /&gt;
각 edge feature vector를 순서에 맞는 one-hot vector로 설정하면 edge-conditioned convolution 연산은 uniform function에 대한 convolution과 같아집니다. 위의 그림에서 ECC는 filter size가 3인 1차원의 discrete convolution과 정확히 같아지는 것을 확인할 수 있습니다.
ECC의 특징 중의 하나가 multi-hop convolution이 불가능하다는 점입니다. 다른 graph convolution 연산의 경우에는 인접 행렬을 여러번 곱해서 multi-hop의 범위를 가지는 node에 대해서도 feature aggregation을 할 수가 있지만, ECC는 인접 행렬을 이용하지 않기 때문에 multi-hop 연산이 불가능합니다. 하지만 저자는 일반적인 2D convolution에서도 크기가 큰 filter를 이용하는 것보다 3x3 크기의 filter를 여러개 이어붙이는 것이 더 좋은 feature를 추출하기 때문에 ECC layer도 여러번 반복하면 multi-hop convolution보다 좋은 성능을 낼 것이라고 이야기합니다.
ECC layer의 끝에는 다른 convolution layer와 마찬가지로 batch normalization layer를 붙여주어 학습을 효율적으로 진행할 수 있게 하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;deep-network-with-ecc&quot;&gt;Deep network with ECC&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210120-ECC/3-network-architecture.png&quot; alt=&quot;&quot; /&gt;
ECC 연산을 통해 2D image data에 대한 neural network와 비슷한 구조를 graph 구조에 대해서도 구현할 수 있습니다. 위 그림은 ECC를 이용한 convolution layer와 graph-pooling 및 coarsening 과정을 이용한 pooling layer를 이어붙여 deep neural network를 구현한 그림입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Graph pooling &amp;amp; coarsening
ECC는 nearest neighborhood를 통해 정의된 그래프의 구조를 그대로 유지한 결과를 출력합니다. 하지만 classification task의 출력 형식인 C(number of class)차원의 출력 벡터를 얻기 위해서는 graph의 크기를 줄여서 node feature을 합쳐주어야 합니다. Graph의 크기를 줄이는 과정을 graph coarsening, node feature를 합치는 과정을 graph pooling이라고 부릅니다. Coarsening을 반복하게 되면 결국에는 서로 연결되지 않은 몇 개의 node들만 남게 됩니다. 이 때 각 node들은 self-loop을 가지고 있기 때문에 여전히 graph의 형태를 띈다고 할 수 있으며 global max pooling 등의 과정을 통해 마지막 남은 feature vector를 합쳐줄 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Graph coarsening에는 다양한 알고리즘들이 활용될 수 있는데, 논문에서는 특정한 resolution을 가지는 3차원의 grid를 생성해서 해당 grid 안의 점들을 하나로 모아주는 VoxelGrid 알고리즘을 이용했습니다. 합쳐진 점들에 대해서는 다시 nearest neighbor을 통해 graph로 변환해줍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Edge label
Edge label은 해당 edge와 연결된 두 vertex 좌표값의 차이로 설정했습니다. Cartesian 및 Spherical coordinate으로 변환하여 총 6차원의 edge feature vector를 활용하였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Augmentation
학습할 때에는 dataset의 규모가 크지 않기 때문에 여러 방법으로 augmentation을 하여 overfitting을 방지하였습니다. 논문에서는 point cloud를 임의로 회전시키거나, 크기를 변화시키거나, 특정 축으로 대칭시키거나, 몇개의 점을 지우는 등의 방법을 통해 데이터 다양성을 확보했다고 주장합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;ECC는 다양한 형태의 point cloud 및 graph classification 실험을 통해 성능을 보여주었습니다. 오늘은 대표적으로 ModelNet 데이터셋과 MNIST 데이터셋을 활용한 실험을 소개하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ModelNet
&lt;img src=&quot;assets/images/210120-ECC/4-modelnet-result.png&quot; alt=&quot;&quot; /&gt;
ModelNet 데이터셋은 물체에 대한 3D Mesh 형태의 데이터입니다. 카테고리 개수에 따라 ModelNet10과 ModelNet40으로 구분되며 각각 4000개, 10000개 정도의 학습 데이터가 존재합니다. 논문에서는 Mesh 형태의 데이터의 표면에서 uniform하게 1000개씩의 point를 sampling하여 point cloud 형태로 변환해주었습니다. 신경망 구조는 &lt;strong&gt;&lt;em&gt;ECC(16)-ECC(32)-MP(2.5/32,7.5/32)-ECC(32)-ECC(32)-MP(7.5/32,22.5/32)-ECC(64)-GMP-FC(64)-Dropout(0.2)-FC(10 or 40)&lt;/em&gt;&lt;/strong&gt; 의 형태를 이용하였고 100 epoch의 학습을 진행하였습니다. Classification에 대한 전반적인 accuracy는 SOTA만큼은 아니지만 경쟁력 있는 수준을 보여주었습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST
논문에서는 숫자에 대한 28x28 크기의 이미지 데이터셋인 MNIST 데이터셋을 이용한 성능도 측정하였습니다. 우선 ECC를 적용할 수 있게 2D image를 &lt;em&gt;(x, y, 0)&lt;/em&gt;의 좌표를 가지는 point cloud 형태로 변환하고 nearest neighbor 알고리즘을 통해 다시 graph 형태로 변환하였습니다. 여기서는 &lt;strong&gt;&lt;em&gt;C(16)-MP(2,3.4)-C(32)-MP(4,6.8)-C(64)-MP(8,30)-C(128)-D(0.5)-FC(10)&lt;/em&gt;&lt;/strong&gt; 형태의 네트워크 구조를 활용하여 20 epoch만큼 학습을 진행하였습니다.
&lt;img src=&quot;assets/images/210120-ECC/5-mnist-result.png&quot; alt=&quot;&quot; /&gt;
학습 결과는 위와 같습니다. 기존의 baseline 모델들과 비교해도 괜찮은 결과를 도출하였습니다. 또한 2D image에서 어두운 부분을 제외하고 graph 형태로 변환하여 학습을 시켜도 비슷한 결과를 도출하였습니다. 이는 graph의 형태가 바뀌어도 neural network가 학습 및 추론을 안정적으로 할 수 있다는 것을 의미합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;논문에서는 edge label을 처음으로 사용한 Edge-conditioned Convolution 연산을 제시하였고, 이를 활용하여 graph 형태의 데이터를 위한 feed-forward network를 구성하였습니다. 추후에 진행되는 attention 기반의 graph convolution 등의 연구의 기반이 되는 중요한 연구라고 생각합니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.html&quot;&gt;https://openaccess.thecvf.com/content_cvpr_2017/html/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3d" />
      

      
        <category term="3d" />
      

      
        <summary type="html">원문 : Simonovsky, Martin, and Nikos Komodakis. “Dynamic edge-conditioned filters in convolutional neural networks on graphs.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</summary>
      

      
      
    </entry>
  
</feed>
