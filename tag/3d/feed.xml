<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/tag/3d/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2021-01-22T21:48:21+09:00</updated>
  <id>http://localhost:4000/tag/3d/feed.xml</id>

  
  
  

  
    <title type="html">RE Tech Archive | </title>
  

  
    <subtitle>machine learning research notes</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">PointNet++ - Deep Hierarchical Feature Learning on Point Sets in a Metric Space 리뷰</title>
      <link href="http://localhost:4000/PointNet++" rel="alternate" type="text/html" title="PointNet++ - Deep Hierarchical Feature Learning on Point Sets in a Metric Space 리뷰" />
      <published>2021-01-21T09:00:00+09:00</published>
      <updated>2021-01-21T09:00:00+09:00</updated>
      <id>http://localhost:4000/PointNet++</id>
      <content type="html" xml:base="http://localhost:4000/PointNet++">&lt;p&gt;원문 : &lt;a href=&quot;https://papers.nips.cc/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf&quot;&gt;Qi, Charles Ruizhongtai, et al. “Pointnet++: Deep hierarchical feature learning on point sets in a metric space.” Advances in neural information processing systems. 2017.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 Stanford에서 2017년 NIPS에 발표한 &lt;strong&gt;&lt;em&gt;Pointnet++: Deep hierarchical feature learning on point sets in a metric space&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Point cloud 형식의 데이터를 Deep learning 분야에 적용시킨 선구적인 논문인 PointNet의 후속편으로, local한 특징을 잡아내지 못하는 기존의 PointNet을 보완하여 classification 및 segmentation 성능을 크게 끌어올렸습니다.&lt;/p&gt;

&lt;p&gt;그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;pointnet&quot;&gt;PointNet&lt;/h2&gt;
&lt;p&gt;PointNet++에 대하여 설명드리기 전에, 전편인 PointNet에 관해서 간단히 짚고 넘어가겠습니다. (자세한 설명은 PointNet 논문 리뷰를 참고해주세요.) Point cloud는 3차원 공간 내의 물체를 표현하기 위한 데이터의 한 형식으로 각 점들에 대한 좌표값으로 구성되어 있습니다. Point cloud는 sparse한 점들의 집합이기 때문에 계산량 및 메모리 차원에서 효율적이지만, 순서가 없는 집합 형태의 데이터이기 때문에 neural network가 이를 처리하려면 permutation-invariant한 특성을 가진 layer를 포함하고 있어야 했습니다. PointNet에서는 symmetric function 중의 하나인 max-pooling 함수를 활용하여 point cloud 데이터에 대한 global한 feature vector를 추출하였습니다. 하지만 max-pooling 함수의 특성상 최대값을 제외한 local한 정보들을 소실될 수밖에 없고 이는 정교한 경계선 설정이 중요한 segmentation task 등에서 성능 저하의 요인이 되었습니다. PointNet++에서는 몇 가지 구조를 제안하여 local한 feature vector를 추출하였습니다.&lt;/p&gt;

&lt;p&gt;Local한 특징을 추출하는 것은 중요합니다. CNN에서도 다양한 receptive field를 가지는 convolution kernel을 활용함으로써 local한 특징을 추출하였고, 이는 2D image들에 대한 폭발적인 성능 향상을 이뤄냈습니다. PointNet++는 계층적 구조의 neural network를 활용하여 다양한 scale의 local feature를 추출하였고, 이를 통합하여 SOTA의 classification 및 segmentation 성능을 기록할 수 있었습니다. 그럼 PointNet++에 대해 좀 더 자세히 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;pointnet-1&quot;&gt;PointNet++&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210121-pointnet++/1-pointnet++.png&quot; alt=&quot;&quot; /&gt;
PointNet++는 &lt;strong&gt;Set abstraction layer&lt;/strong&gt;과 &lt;strong&gt;Density adaptive layer&lt;/strong&gt;로 구성되어 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;set-abstraction&quot;&gt;Set Abstraction&lt;/h2&gt;
&lt;p&gt;PointNet++에서는 set abstraction이라고 부르는 과정을 통해 point cloud의 local feature를 추출하였습니다. Set abstraction을 거치면, point 들의 집합은 이전보다 적은 수의 점들로 구성된 point cloud로 추상화되며, semantic한 정보를 담게 됩니다.&lt;/p&gt;

&lt;p&gt;Set abstraction은 세 단계로 구분할 수 있습니다. 우선 sampling 단계에서는 point cloud의 local한 부분집합의 중심에 해당하는 중요한 몇 개의 점들을 선택합니다. 이후 grouping 단계에서는 sampling 단계에서 찾은 중요한 몇 개의 점들과 이웃한 점들을 찾고 하나의 집합으로 구성합니다. 마지막으로 pointnet 단계에서는 local한 점들의 집합에 대한 패턴을 encoding하여 feature vector를 추출합니다.&lt;/p&gt;

&lt;p&gt;예를 들어 Point cloud가 &lt;em&gt;N&lt;/em&gt; 개의 점들로 구성되어 있고 각 점은 &lt;em&gt;d&lt;/em&gt; 차원의 좌표를 가지며 &lt;em&gt;C&lt;/em&gt; 차원의 feature vector를 가진다고 가정하겠습니다. Set abstraction 과정을 거치면 &lt;em&gt;N&lt;/em&gt; x &lt;em&gt;(d+C)&lt;/em&gt; 크기의 input 행렬이 &lt;em&gt;N’&lt;/em&gt; x &lt;em&gt;(d+C’)&lt;/em&gt; 크기의 행렬로 변환되어 출력됩니다. 이 때 &lt;em&gt;N’&lt;/em&gt; 는 subsampling된 점들의 개수이고, &lt;em&gt;C’&lt;/em&gt; 는 중심점 행렬의 feature vector의 차원입니다.&lt;/p&gt;

&lt;p&gt;Sampling 단계에서는 &lt;em&gt;N&lt;/em&gt; 개의 점들 중 &lt;em&gt;N’&lt;/em&gt; 개의 점을 중심점으로 선택합니다. 이 &lt;em&gt;N’&lt;/em&gt; 개의 점들은 전체 점들 중 나머지 점들과의 거리가 가장 먼 점들로 구성됩니다. 이때 이 거리라는 개념은 Uclidian distance 일 수도 있고 그 외 다른 거리 관련 metric일 수도 있습니다. 논문에서는 이 거리가 가장 먼 점들의 집합을 Farthest point sampling (FPS) 알고리즘을 반복적으로 적용해 얻었다고 설명하였습니다. 또한, 이렇게 서로 최대한 떨어져 있는 점들을 선택하게 되면 random sampling을 통해서 점들을 선택하는 것보다 더 일반적이고 전체적인 범위의 point들을 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;Grouping 단계에서는 sampling 단계에서 추출한 &lt;em&gt;N’&lt;/em&gt; 개 중심점들의 주변 점들을 선택하여 local한 영역으로 묶어줍니다. 각 중심점과 묶이는 주변 점들의 개수는 영역마다 다를 수 있는데, 후에 설명드릴 pointnet 단계에서 점들의 개수와 상관 없이 같은 크기의 feature vector로 변환해주기 때문입니다. 또한 이웃한 점들을 선택하는 방식도 두 가지로 나뉠 수 있습니다. 우선, ball query 방식은 반지름 &lt;em&gt;r&lt;/em&gt; 을 정해 이 반지름 내에 있는 점들을 모두 이웃한 점으로 선택하는 방식입니다. 또한, kNN 방식은 정해진 &lt;em&gt;K&lt;/em&gt; 라는 개수의 가장 가까운 점들을 이웃한 점으로 선택하는 방식입니다. 논문에서는 ball query 방식을 선택했는데, kNN과 비교했을 때 정해진 크기의 지역을 확실하게 표현할 수 있기 때문입니다. 이는 일정하게 sampling되지 않은 point cloud 데이터에서 영역 별로 범위가 달라지는 문제를 방지할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;마지막으로 pointnet 단계에서는 각 local한 영역에 대해 특징 정보를 encoding한 하나의 feature vector를 추출할 수 있습니다. 각 중심점(총 &lt;em&gt;N’&lt;/em&gt; 개)별로 생성된 &lt;em&gt;K&lt;/em&gt; 개의 이웃한 점들에 대한 &lt;em&gt;(d+C)&lt;/em&gt; 크기의 feature vector를 행렬로 표현하면 &lt;em&gt;N’&lt;/em&gt; x &lt;em&gt;K&lt;/em&gt; x &lt;em&gt;(d+C’)&lt;/em&gt; 크기의 행렬이 되는데, 이 행렬이 pointnet 단계를 거치면 &lt;em&gt;N’&lt;/em&gt;x&lt;em&gt;(d+C’)&lt;/em&gt; 크기의 feature vector로 변환됩니다. 이 때, 점들의 좌표값은 중심점과의 상대적인 거리의 차이에 대한 값이 들어가게 되는데, 이를 통해 점들 사이의 위치적 관계에 대한 정보를 잡아낼 수 있기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;density-adaptive-feature-learning&quot;&gt;Density Adaptive Feature Learning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/210121-pointnet++/2-nonuniform-pointcloud.png&quot; alt=&quot;&quot; /&gt;
Point cloud 데이터는 위의 그림처럼 non-uniform한 밀도 분포를 가지는 것이 대부분입니다. 이렇게 일정하지 않은 점들의 분포는 point cloud의 특징에 대한 학습을 어렵게 만드는데, density가 높은 point cloud에 대해 학습한 네트워크는 sparse한 점들의 point cloud를 만났을 때 정보가 부족하다고 느끼고 특징을 제대로 표현하지 못하기 때문입니다. (그 반대의 경우도 마찬가지입니다.) 논문에서는 이를 해결하기 위해 point cloud의 sampling density를 변화시키며 학습하였습니다. 또한, 다양한 scale의 point cloud에서 feature vector를 추출하는 multi-scale feature extraction 구조를 활용하였는데, 저자는 이를 &lt;em&gt;density adaptive layer&lt;/em&gt; 이라고 표현했습니다. 다양한 크기의 density를 가지는 point cloud를 학습하는 두 가지 방법에 대해 소개하겠습니다.
&lt;img src=&quot;assets/images/210121-pointnet++/3-grouping.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-scale grouping (MSG)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우선 grouping 단계를 다양한 scale로 여러 번 적용하여 하나의 중심점에 대해 여러 scale의 point group을 얻는 방법이 있습니다. 각 point group에서 각각 추출한 feature vector를 이어붙이면(concatenate), multi-scale feature vector를 얻을 수 있게 됩니다. 이 때, 각 point group은 임의의 dropout ratio를 선택하여 그 비율에 맞게 random하게 down-sampling(dropout) 하여 각 point group마다 서로 다른 scale로 균일하지 않은 density를 가지게끔 변환해주었습니다. 이러한 과정을 거치면 다양한 sparsity와 서로 다른 uniformity를 가지는 점들을 얻을 수 있습니다. 하지만 MSG는 각 중심점들과 그 이웃한 점들이 모두 pointnet을 거쳐야 하므로 게산량 차원에서 아주 비효율적이고 time-consuming하다는 단점이 있습니다. 논문에서는 이를 보완하기 위해 multi-resolution grouping을 제안하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-resolution grouping (MRG)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MRG는 MSG의 단점을 보완한 grouping 방식입니다. MRG는 서로 다른 scale로 얻은 두 feature vector를 이어붙여서(concatenate) multi-scale feature vector를 얻습니다. 이 때, 첫 번째 vector는 local group에 해당하는 점들 전체에 대해 pointnet 단계를 거쳐서 얻고, 두 번째 vector는 local group에 대해 그보다 한 단계 아래의 sub-region에서 얻은 feature를 종합하여 얻습니다. 저자는, 만약 input으로 들어오는 point cloud의 density가 낮다면, 첫 번째 vector에 의해 전반적인 특징에 대한 정보를 추출할 수 있고, density가 높다면, sub-region에 대한 feature가 높은 resolution 더 디테일한 특징 정보를 제공할 수 있다고 이야기합니다. 따라서 두 vector를 모두 이용하게 되면 여러 density의 point cloud에 대해서 모두 대응할 수 있으며, 계산량 측면에서도 효율적이라고 말합니다.&lt;/p&gt;

&lt;h2 id=&quot;point-feature-propagation&quot;&gt;Point Feature Propagation&lt;/h2&gt;
&lt;p&gt;Set abstracion layer를 거치게 되면, sampling 단계에 의해 point cloud의 크기가 줄어들게 됩니다. 이렇게 얻은 feature vector를 segmentation task에 활용하려면 다시 원래의 크기로 복원해주어야 합니다. 복원해주지 않고 set abstraction을 하기 위해 모든 점들을 중심점으로 지정해서 feature aggregation을 해주는 방법도 있지만, 이는 computation cost가 너무 많이 들기 때문에 논문에서는 point cloud를 down-sampling하고 다시 interpolation 기반의 방법을 통해 up-sampling하는 방식을 제안하고 있습니다.&lt;/p&gt;

&lt;p&gt;구체적으로, 이전 점들에 대한 feature vector로부터 (&lt;em&gt;1/거리값)&lt;/em&gt; 으로 weighting을 가해서 interpolation하는 방법을 이용하였습니다. 또한 down-sampling 되기 전의 feature vector를 skip-connection을 통해 concatenate하여 부족할 수도 있는 정보량을 보충해주었습니다. Interpolation 과정은 원래 point의 개수로 맞춰질 때까지 반복해주었고, 결과로 얻은 feature vector를 통해서 segmentation task를 수행해주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;PointNet++는 MNIST(2D Object), ModelNet40(3D Object), ScanNet(3D Scene) 등의 다양한 데이터셋에 대한 evaluation 과정을 통해 classifiaction 및 segmentation task에서 성능을 증명하였습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST
&lt;img src=&quot;assets/images/210121-pointnet++/4-mnist-result.png&quot; alt=&quot;&quot; /&gt;
MNIST 데이터셋은 손글씨 숫자에 대한 60,000개 이상의 image입니다. PointNet++는 2D image의 좌표를 2D point cloud 형태로 변환하여 input으로 사용했는데, 기존 PointNet과 비교했을 때 error rate이 30% 이상 감소하는 등의 성능 향상을 보여주었습니다. 또한 기존 CNN 기반의 모델들과 비교했을 때도 더 좋은 성능을 보이기도 했습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ModelNet40
&lt;img src=&quot;assets/images/210121-pointnet++/5-modelnet-result.png&quot; alt=&quot;&quot; /&gt;
ModelNet40 데이터셋은 40개의 클래스에 대한 CAD 모델입니다. CAD 모델은 3D Mesh 형태를 띄고 있기 때문에, 논문에서는 mesh의 표면을 sampling하여 3D point cloud 형태로 변환한 후에 PointNet++의 input으로 사용하였습니다. 또한 모델 구조는 3단계의 계층 단계에 3개의 Fully connected layer를 이어붙여서 구성했으며, 모든 point들의 좌표는 반지름 1의 구 안에 들어오게끔 하여 normalization을 해주었습니다. PointNet++는 3D classification task에서도 기존의 SOTA 모델로 알려진 MVCNN의 성능을 크게 뛰어넘었습니다.
&lt;img src=&quot;assets/images/210121-pointnet++/6-grouping-result.png&quot; alt=&quot;&quot; /&gt;
또한 ModelNet 데이터셋을 활용하여 Density adaptive layer의 성능을 측정하는 ablation study를 진행했습니다. 위의 그림을 보면, 1024개의 점에 대한 point cloud로부터 random하게 점을 지워서 512, 256, 128개로 down-sampling하였습니다. 점의 개수가 다른 point cloud를 PointNet 및 PointNet++의 input으로 넣어주었을 때, 앞서 설명드린 density aadaptive layer (MSG 또는 MRG)를 적용시켜 multi-scale로 학습한 모델들은 점의 개수가 달라져도 robust한 결과를 보여주었습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ScanNet&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ScanNet 데이터셋은 실내 환경에 대한 1500개 이상의 3D point cloud 데이터로 구성되어 있습니다. 각각의 점들에는 해당 점이 어떤 물체에 속해 있는지에 대한 segmentation label이 달려 있습니다. PointNet++는 ScanNet에 대한 segmentation task에서도 아주 뛰어난 결과를 보여주었습니다. 계층적 구조를 통한 local한 feature 학습이 다양한 scale의 scene을 이해하는데 중요하다고 해석할 수 있을 것 같습니다. 
&lt;img src=&quot;assets/images/210121-pointnet++/8-scannet-result.png&quot; alt=&quot;&quot; /&gt;
ScanNet에서도 sampling density가 달라졌을 때 robust한 결과를 도출할 수 있는지 실험하였습니다. ScanNet 데이터를 non-uniform한 sampling density로 줄여서 학습한 뒤에 결과를 확인해보았습니다. 앞선 ModelNet40을 이용한 실험 결과와 마찬가지로, MRG 네트워크가 Single-scale grouping을 통해 학습한 네트워크보다 다양한 density의 데이터들에 대해 훨씬 더 좋은 결과를 보여주었습니다. 
&lt;img src=&quot;assets/images/210121-pointnet++/7-grouping-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;PointNet과 더불어 PointNet++는 이전에 classification이나 detection 분야에서 활용되었던 multi-scale feature learning 기법을 point cloud 데이터에 적용시켜 급격한 성능 향상을 만들었습니다. 또한 Graph의 크기를 줄이는 graph coarsening이나 다시 graph의 크기를 키우는 point feature propagation 등 다양한 개념이 제시된 논문이라 의미가 크다고 생각합니다. 다음 번에는 이를 기반으로 attention 메커니즘을 적용시킨 논문을 한번 리뷰해보겠습니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://stanford.edu/~rqi/pointnet2/&quot;&gt;http://stanford.edu/~rqi/pointnet2/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/charlesq34/pointnet2&quot;&gt;https://github.com/charlesq34/pointnet2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="3D" />
      

      
        <category term="3D" />
      

      
        <summary type="html">원문 : Qi, Charles Ruizhongtai, et al. “Pointnet++: Deep hierarchical feature learning on point sets in a metric space.” Advances in neural information processing systems. 2017.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs 리뷰</title>
      <link href="http://localhost:4000/Edge-Conditioned-Convolution" rel="alternate" type="text/html" title="Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs 리뷰" />
      <published>2021-01-20T09:00:00+09:00</published>
      <updated>2021-01-20T09:00:00+09:00</updated>
      <id>http://localhost:4000/Edge-Conditioned-Convolution</id>
      <content type="html" xml:base="http://localhost:4000/Edge-Conditioned-Convolution">&lt;p&gt;원문 : &lt;a href=&quot;https://arxiv.org/abs/1704.02901&quot;&gt;Simonovsky, Martin, and Nikos Komodakis. “Dynamic edge-conditioned filters in convolutional neural networks on graphs.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;오늘 소개드릴 논문은 ENPC에서 2017년 CVPR에 발표한 &lt;strong&gt;&lt;em&gt;Dynamic edge-conditioned filters in convolutional neural networks on graphs&lt;/em&gt;&lt;/strong&gt; 논문에 대한 리뷰입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Point cloud 등의 graph 구조로 표현 가능한 데이터에서 convolution 형태의 연산을 통해 feature vector를 추출하는 하나의 방법에 대해 제시하고 있습니다.&lt;/p&gt;

&lt;p&gt;그럼 시작하겠습니다!&lt;/p&gt;
&lt;h2 id=&quot;graph-convolution&quot;&gt;Graph Convolution&lt;/h2&gt;
&lt;p&gt;Convolution(합성곱) 연산은 2D image에 대한 내재된 feature 벡터를 잘 추출해줌으로써 classification, segmentation 등의 여러 task들에 대한 성능을 폭발적으로 끌어올렸습니다. 학계에서는 2D image 등의 Uclidian 공간에 대한 데이터 뿐만 아니라 graph 등 non-Uclidian 공간 상의 데이터도 convolution 연산을 통해 처리할 수 있지 않을까에 대해 많이 고민했습니다.&lt;/p&gt;

&lt;p&gt;이러한 고민의 결과로 인접한 node 간의 특징 벡터를 모아주는 graph convolution 연산을 발견하였고, 이는 graph 형태로 표현 가능한 3D modeling, biological molecule, social network 등의 데이터에 활용되었습니다. 하지만 기존의 방법대로는 한 점과 인접한 점들의 feature 벡터를 균일하게(homogeneous) 더해줄 수 밖에 없었고, 이는 점들 간의 연관성의 정도를 전혀 고려하지 않은 연산이었습니다. (일반적인 discrete convolution으로 생각해보면, uniform function만을 filter로 사용하는 형태였습니다.)&lt;/p&gt;

&lt;p&gt;이를 보완하기 위해 논문에서는 점과 점을 연결한 edge에 점들 간의 관계를 나타내는 label을 부여해서 weight parameter처럼 활용하였습니다. 이렇게 edge label을 이용하여 graph convolution을 하게 되면, 단순히 점들을 homogeneous하게 더하는 것이 아니라 점들 간의 연관성을 고려하여 더해주기 때문에 주변 점들과의 관계가 반영된 feature 벡터를 추출할 수 있다고 합니다. 또한 이 feature vector를 이용해서 graph 또는 vertex 단위로 classification을 했을 때 점들의 관계 및 분포에 대한 정보가 녹아있어 정확도가 향상되었다고 합니다. 본문에서는 이렇게 edge label을 이용한 convolution 연산을 edge-conditioned convolution(ECC)라고 부릅니다.&lt;/p&gt;

&lt;h2 id=&quot;point-cloud&quot;&gt;Point cloud&lt;/h2&gt;
&lt;p&gt;Point cloud는 3D 물체를 표현하기 위한 형식 중의 하나입니다. 원래 point cloud는 일정한 형식이 없는 집합 형태의 데이터이기 때문에 neural network의 input으로 활용하기 어려웠습니다. 하지만 특정 점에 대해 정의된 거리(r) 내에 있는 점들 중 K-nearest neighbors 알고리즘을 적용해서 인접한 점들을 찾은 뒤, 서로 연결해서 graph 형태로 변환하게 되면 graph convolution을 적용할 수 있습니다. 따라서 논문에서는 여러 종류의 graph 데이터들 중 주로 point cloud를 graph로 변환한 데이터를 활용했습니다. Point cloud는 물체나 풍경을 3D로 표현했다는 특성상 graph 또는 vertex 단위로 label이 존재하기 때문에 graph convolution의 성능을 측정하기에 적절하기도 합니다. 또한 최근에는 RGB-D 카메라나 LiDaR의 출력 형식으로 사용되기 때문에 범용적이기도 합니다.&lt;/p&gt;

&lt;h2 id=&quot;edge-conditioned-convolution-ecc&quot;&gt;Edge-conditioned Convolution (ECC)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/edge-convolution/1-edge-convolution.png&quot; alt=&quot;&quot; /&gt;
Edge-conditioned convolution 연산 과정은 그리 어렵지 않습니다. Point cloud 데이터를 graph 형태로 변환하여 &lt;em&gt;n&lt;/em&gt;개의 node와 &lt;em&gt;m&lt;/em&gt;개의 edge를 얻었다고 가정하겠습니다. 이 때 node의 feature vector가 &lt;em&gt;l&lt;/em&gt;차원, edge의 feature vector가 &lt;em&gt;s&lt;/em&gt;차원이라고 하면 우리는 edge의 feature vector로부터 node feature vector의 transformation matrix(&lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt;차원)를 얻는 미분 가능한 함수를 정의하고자 합니다. 논문에서는 가장 간단한 multi-layer perceptron을 활용하였는데, input이 &lt;em&gt;s&lt;/em&gt;차원/ output이 &lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt;차원인 linear layer입니다. 이렇게 각 node마다 &lt;em&gt;l&lt;/em&gt;x&lt;em&gt;l&lt;/em&gt; 차원의 “edge-specific weights”를 얻을 수 있고, 이는 edge feature vector에서 추출한 행렬이기 때문에 점들 사이의 관계를 담고 있습니다. 이렇게 얻은 weight을 이웃한 각 node feature vector에 곱해주고 normalize하면 edge-conditioned convolution을 거친 해당 점의 feature vector를 얻을 수 있습니다.
&lt;img src=&quot;assets/images/edge-convolution/2-one-hot-edge.png&quot; alt=&quot;&quot; /&gt;
각 edge feature vector를 순서에 맞는 one-hot vector로 설정하면 edge-conditioned convolution 연산은 uniform function에 대한 convolution과 같아집니다. 위의 그림에서 ECC는 filter size가 3인 1차원의 discrete convolution과 정확히 같아지는 것을 확인할 수 있습니다.
ECC의 특징 중의 하나가 multi-hop convolution이 불가능하다는 점입니다. 다른 graph convolution 연산의 경우에는 인접 행렬을 여러번 곱해서 multi-hop의 범위를 가지는 node에 대해서도 feature aggregation을 할 수가 있지만, ECC는 인접 행렬을 이용하지 않기 때문에 multi-hop 연산이 불가능합니다. 하지만 저자는 일반적인 2D convolution에서도 크기가 큰 filter를 이용하는 것보다 3x3 크기의 filter를 여러개 이어붙이는 것이 더 좋은 feature를 추출하기 때문에 ECC layer도 여러번 반복하면 multi-hop convolution보다 좋은 성능을 낼 것이라고 이야기합니다.
ECC layer의 끝에는 다른 convolution layer와 마찬가지로 batch normalization layer를 붙여주어 학습을 효율적으로 진행할 수 있게 하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;deep-network-with-ecc&quot;&gt;Deep network with ECC&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;assets/images/edge-convolution/3-network-architecture.png&quot; alt=&quot;&quot; /&gt;
ECC 연산을 통해 2D image data에 대한 neural network와 비슷한 구조를 graph 구조에 대해서도 구현할 수 있습니다. 위 그림은 ECC를 이용한 convolution layer와 graph-pooling 및 coarsening 과정을 이용한 pooling layer를 이어붙여 deep neural network를 구현한 그림입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Graph pooling &amp;amp; coarsening
ECC는 nearest neighborhood를 통해 정의된 그래프의 구조를 그대로 유지한 결과를 출력합니다. 하지만 classification task의 출력 형식인 C(number of class)차원의 출력 벡터를 얻기 위해서는 graph의 크기를 줄여서 node feature을 합쳐주어야 합니다. Graph의 크기를 줄이는 과정을 graph coarsening, node feature를 합치는 과정을 graph pooling이라고 부릅니다. Coarsening을 반복하게 되면 결국에는 서로 연결되지 않은 몇 개의 node들만 남게 됩니다. 이 때 각 node들은 self-loop을 가지고 있기 때문에 여전히 graph의 형태를 띈다고 할 수 있으며 global max pooling 등의 과정을 통해 마지막 남은 feature vector를 합쳐줄 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Graph coarsening에는 다양한 알고리즘들이 활용될 수 있는데, 논문에서는 특정한 resolution을 가지는 3차원의 grid를 생성해서 해당 grid 안의 점들을 하나로 모아주는 VoxelGrid 알고리즘을 이용했습니다. 합쳐진 점들에 대해서는 다시 nearest neighbor을 통해 graph로 변환해줍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Edge label
Edge label은 해당 edge와 연결된 두 vertex 좌표값의 차이로 설정했습니다. Cartesian 및 Spherical coordinate으로 변환하여 총 6차원의 edge feature vector를 활용하였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Augmentation
학습할 때에는 dataset의 규모가 크지 않기 때문에 여러 방법으로 augmentation을 하여 overfitting을 방지하였습니다. 논문에서는 point cloud를 임의로 회전시키거나, 크기를 변화시키거나, 특정 축으로 대칭시키거나, 몇개의 점을 지우는 등의 방법을 통해 데이터 다양성을 확보했다고 주장합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;ECC는 다양한 형태의 point cloud 및 graph classification 실험을 통해 성능을 보여주었습니다. 오늘은 대표적으로 ModelNet 데이터셋과 MNIST 데이터셋을 활용한 실험을 소개하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ModelNet
&lt;img src=&quot;assets/images/edge-convolution/4-modelnet-result.png&quot; alt=&quot;&quot; /&gt;
ModelNet 데이터셋은 물체에 대한 3D Mesh 형태의 데이터입니다. 카테고리 개수에 따라 ModelNet10과 ModelNet40으로 구분되며 각각 4000개, 10000개 정도의 학습 데이터가 존재합니다. 논문에서는 Mesh 형태의 데이터의 표면에서 uniform하게 1000개씩의 point를 sampling하여 point cloud 형태로 변환해주었습니다. 신경망 구조는 &lt;strong&gt;&lt;em&gt;ECC(16)-ECC(32)-MP(2.5/32,7.5/32)-ECC(32)-ECC(32)-MP(7.5/32,22.5/32)-ECC(64)-GMP-FC(64)-Dropout(0.2)-FC(10 or 40)&lt;/em&gt;&lt;/strong&gt; 의 형태를 이용하였고 100 epoch의 학습을 진행하였습니다. Classification에 대한 전반적인 accuracy는 SOTA만큼은 아니지만 경쟁력 있는 수준을 보여주었습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST
논문에서는 숫자에 대한 28x28 크기의 이미지 데이터셋인 MNIST 데이터셋을 이용한 성능도 측정하였습니다. 우선 ECC를 적용할 수 있게 2D image를 &lt;em&gt;(x, y, 0)&lt;/em&gt;의 좌표를 가지는 point cloud 형태로 변환하고 nearest neighbor 알고리즘을 통해 다시 graph 형태로 변환하였습니다. 여기서는 &lt;strong&gt;&lt;em&gt;C(16)-MP(2,3.4)-C(32)-MP(4,6.8)-C(64)-MP(8,30)-C(128)-D(0.5)-FC(10)&lt;/em&gt;&lt;/strong&gt; 형태의 네트워크 구조를 활용하여 20 epoch만큼 학습을 진행하였습니다.
&lt;img src=&quot;assets/images/edge-convolution/5-mnist-result.png&quot; alt=&quot;&quot; /&gt;
학습 결과는 위와 같습니다. 기존의 baseline 모델들과 비교해도 괜찮은 결과를 도출하였습니다. 또한 2D image에서 어두운 부분을 제외하고 graph 형태로 변환하여 학습을 시켜도 비슷한 결과를 도출하였습니다. 이는 graph의 형태가 바뀌어도 neural network가 학습 및 추론을 안정적으로 할 수 있다는 것을 의미합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;논문에서는 edge label을 처음으로 사용한 Edge-conditioned Convolution 연산을 제시하였고, 이를 활용하여 graph 형태의 데이터를 위한 feed-forward network를 구성하였습니다. 추후에 진행되는 attention 기반의 graph convolution 등의 연구의 기반이 되는 중요한 연구라고 생각합니다. 읽어주셔서 감사합니다 :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고 문헌 및 출처&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.html&quot;&gt;https://openaccess.thecvf.com/content_cvpr_2017/html/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyunsung Eun</name>
        
        
      </author>

      
        <category term="NLP" />
      

      
        <category term="3D" />
      

      
        <summary type="html">원문 : Simonovsky, Martin, and Nikos Komodakis. “Dynamic edge-conditioned filters in convolutional neural networks on graphs.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</summary>
      

      
      
    </entry>
  
</feed>
